{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/images.jpg","path":"images/images.jpg","modified":1,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon.ico","path":"images/favicon.ico","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/images.jpg","path":"images/images.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1}],"Cache":[{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1494683576000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1494683576000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1494683576000},{"_id":"themes/next/.gitignore","hash":"32ea93f21d8693d5d8fa4eef1c51a21ad0670047","modified":1494683576000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1494683576000},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1494683576000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1494683576000},{"_id":"themes/next/.travis.yml","hash":"c42d9608c8c7fe90de7b1581a8dc3886e90c179e","modified":1494683576000},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1494683576000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1494683576000},{"_id":"themes/next/README.en.md","hash":"4ece25ee5f64447cd522e54cb0fffd9a375f0bd4","modified":1494683576000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1494683576000},{"_id":"themes/next/bower.json","hash":"be0a430362cb73a7e3cf9ecf51a67edf8214b637","modified":1494683576000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1494683576000},{"_id":"themes/next/package.json","hash":"7e87b2621104b39a30488654c2a8a0c6a563574b","modified":1494683576000},{"_id":"source/about/index.md","hash":"b1c0ea16dab27b36c6961bc037d7cfeff409978e","modified":1496233446892},{"_id":"themes/next/_config.yml","hash":"74c0ba0f9ab6cdd6e15aeefc02fdbe5023fc3993","modified":1496390508132},{"_id":"source/_posts/FaceNet论文笔记.md","hash":"518fe5e6b37680f057229e878c1c65df071e17e3","modified":1498617247173},{"_id":"source/_posts/Caffe调参.md","hash":"2558d905413e1fe9cd6883e0a881498316c9c089","modified":1497666289318},{"_id":"source/_posts/待办及进度.md","hash":"0a07443805580a2206fe0371802b8e9f8accc540","modified":1497580439782},{"_id":"source/_posts/Hexo相关.md","hash":"079fcd7fdd1ee6e2e8584047a64470a322f431e4","modified":1496391937278},{"_id":"source/images/images.jpg","hash":"a83aa0596ff8d7ab87cfc47d4a57dd01d6c4ccee","modified":1496232703693},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"fdd63b77472612337309eb93ec415a059b90756b","modified":1494683576000},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1494683576000},{"_id":"source/tags/index.md","hash":"27a029fbb025ae4aa8e5e3f65ab9a299f6f58c55","modified":1496233234309},{"_id":"source/_posts/Python.md","hash":"837b8a369bc289f42332110a09f6047f15661eb7","modified":1497405084227},{"_id":"source/_posts/Triplet loss.md","hash":"d294f3a168a1d618e5844614d69d888cc29e4668","modified":1496408982119},{"_id":"themes/next/languages/de.yml","hash":"306db8c865630f32c6b6260ade9d3209fbec8011","modified":1494683576000},{"_id":"themes/next/languages/default.yml","hash":"4cc6aeb1ac09a58330e494c8771773758ab354af","modified":1494683576000},{"_id":"themes/next/languages/en.yml","hash":"e7def07a709ef55684490b700a06998c67f35f39","modified":1494683576000},{"_id":"source/_posts/杂知识点.md","hash":"6f0dd4a3279b4b0fcf7e962a2f11bf22fff808c6","modified":1496887130356},{"_id":"themes/next/languages/fr-FR.yml","hash":"24180322c83587a153cea110e74e96eacc3355ad","modified":1494683576000},{"_id":"themes/next/languages/id.yml","hash":"2835ea80dadf093fcf47edd957680973f1fb6b85","modified":1494683576000},{"_id":"themes/next/languages/ja.yml","hash":"1c3a05ab80a6f8be63268b66da6f19da7aa2c638","modified":1494683576000},{"_id":"themes/next/languages/ko.yml","hash":"be150543379150f78329815af427bf152c0e9431","modified":1494683576000},{"_id":"themes/next/languages/pt.yml","hash":"36c8f60dacbe5d27d84d0e0d6974d7679f928da0","modified":1494683576000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3c0c7dfd0256457ee24df9e9879226c58cb084b5","modified":1494683576000},{"_id":"themes/next/languages/ru.yml","hash":"1549a7c2fe23caa7cbedcd0aa2b77c46e57caf27","modified":1494683576000},{"_id":"themes/next/languages/pt-BR.yml","hash":"958e49571818a34fdf4af3232a07a024050f8f4e","modified":1494683576000},{"_id":"source/categories/index.md","hash":"4041694b9457789fceb11df6a2576543a2e18495","modified":1496233317600},{"_id":"themes/next/languages/zh-hk.yml","hash":"1c917997413bf566cb79e0975789f3c9c9128ccd","modified":1494683576000},{"_id":"themes/next/layout/_layout.swig","hash":"9d1a23a6add6f3d0f88c2d17979956f14aaa37a4","modified":1494683576000},{"_id":"themes/next/languages/zh-tw.yml","hash":"0b2c18aa76570364003c8d1cd429fa158ae89022","modified":1494683576000},{"_id":"themes/next/layout/archive.swig","hash":"5de4dca06b05d99e4f6bad617a4b8f4f3592fb01","modified":1494683576000},{"_id":"themes/next/layout/category.swig","hash":"82e7bc278559b4335ad974659104eaaf04863032","modified":1494683576000},{"_id":"themes/next/layout/schedule.swig","hash":"f93c53f6fd5c712584f6efba6f770c30fa8a3e80","modified":1494683576000},{"_id":"themes/next/layout/index.swig","hash":"03e8a2cda03bad42ac0cb827025eb81f95d496a2","modified":1494683576000},{"_id":"themes/next/layout/page.swig","hash":"2c6a78999133b991d9221f484aee2eacae894251","modified":1494683576000},{"_id":"themes/next/layout/post.swig","hash":"2d5f8d7f0a96b611e2d5a5e4d111fc17726a990f","modified":1494683576000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1494683576000},{"_id":"themes/next/layout/tag.swig","hash":"2e73ee478e981092ea9a5d10dd472a9461db395b","modified":1494683576000},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1494683576000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1494683576000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1494683576000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1494683576000},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1494683576000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1494683576000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1494683576000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"b16fcbf0efd20c018d7545257a8533c497ea7647","modified":1494683576000},{"_id":"themes/next/layout/_macro/post.swig","hash":"c00261ee0dca8ef7d3f7753e8f8cd444f51118c4","modified":1494683576000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1494683576000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1494683576000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"911b99ba0445b2c07373128d87a4ef2eb7de341a","modified":1494683576000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"1c7d3c975e499b9aa3119d6724b030b7b00fc87e","modified":1494683576000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1494683576000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1494683576000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1494683576000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1494683576000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1494683576000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1494683576000},{"_id":"themes/next/layout/_partials/head.swig","hash":"d4a05c51aac02f1f6248baccf2ddb8ee12b9122f","modified":1494683576000},{"_id":"themes/next/layout/_partials/header.swig","hash":"a1ffbb691dfad3eaf2832a11766e58a179003b8b","modified":1494683576000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9baf90f7c40b3b10f288e9268c3191e895890cea","modified":1494683576000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1494683576000},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1494683576000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1494683576000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1494683576000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1494683576000},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1494683576000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1494683576000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1494683576000},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1494683576000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1494683576000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1494683576000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1494683576000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1494683576000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1494683576000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1494683576000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1494683576000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1494683576000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1494683576000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1494683576000},{"_id":"themes/next/source/images/favicon.ico","hash":"3354f46359b13ee21fede9a2f64a81875daae6a5","modified":1496235727301},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1494683576000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1494683576000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/images/images.jpg","hash":"a83aa0596ff8d7ab87cfc47d4a57dd01d6c4ccee","modified":1496232703693},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1494683576000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1494683576000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1494683576000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1494683576000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"3358d11b9a26185a2d36c96049e4340e701646e4","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1494683576000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1494683576000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"abb92620197a16ed2c0775edf18a0f044a82256e","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"1d0d01aaeb7bcde3671263d736718f8837c20182","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"af9dd8a4aed7d06cf47b363eebff48850888566c","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"1f349aa30dd1f7022f7d07a1f085eea5ace3f26d","modified":1494683576000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1494683576000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1494683576000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1494683576000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"d6a793bcada68d4b6c58392546bc48a482e4a7d3","modified":1494683576000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"06f432f328a5b8a9ef0dbd5301b002aba600b4ce","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1494683576000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1494683576000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1494683576000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1494683576000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1494683576000},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1494683576000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1494683576000},{"_id":"themes/next/source/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1494683576000},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1494683576000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1494683576000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1494683576000},{"_id":"themes/next/source/js/src/utils.js","hash":"803f684fa7d0e729115a48851023a31f6fb6d0a7","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1494683576000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1494683576000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1494683576000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"3587602ad777b031628bb5944864d1a4fcfea4ac","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1494683576000},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1494683576000},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1494683576000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1494683576000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1494683576000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1494683576000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"7804e31c44717c9a9ddf0f8482b9b9c1a0f74538","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e3e23751d4ad24e8714b425d768cf68e37de7ded","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"59ad08bcc6fe9793594869ac2b4c525021453e78","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"ef089a407c90e58eca10c49bc47ec978f96e03ba","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1494683576000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"d9c0b3dc9158e717fde36f554709e6c3a22b5f85","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1494683576000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1494683576000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"38e48f275ad00daa9dcdcb8d9b44e576acda4707","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"740d37f428b8f4574a76fc95cc25e50e0565f45e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"c089419916988d0f51d89b225460fe11b631e0a3","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"dbc07ec641a537df5918b41ce40a6466712a44f6","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"88c7d75646b66b168213190ee4cd874609afd5e3","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"5f6ea57aabfa30a437059bf8352f1ad829dbd4ff","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8c0276883398651336853d5ec0e9da267a00dd86","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a2ec22ef4a6817bbb2abe8660fcd99fe4ca0cc5e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"dd310c2d999185e881db007360176ee2f811df10","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"bb3be8374c31c372ed0995bd8030d2b920d581de","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1494683576000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1494683576000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1494683576000},{"_id":"public/atom.xml","hash":"5249364c869e8c40c99fe537d4aa1668ef366ad8","modified":1498618547881},{"_id":"public/search.xml","hash":"43270cdc0398c019cf23a0efc0bc55ed0423e8d5","modified":1498618547920},{"_id":"public/sitemap.xml","hash":"58ae6c769758b763a9c75bb9f2bdb4ffead56120","modified":1498618547921},{"_id":"public/about/index.html","hash":"f348a7d7a2b5321ec68380464ee3b5739600da3d","modified":1498618547941},{"_id":"public/tags/index.html","hash":"655d7e301f31803c0e8dc2339064462513b01cd4","modified":1498618547942},{"_id":"public/categories/index.html","hash":"3c11828f9e58fc6e454ca162a11541ae60135dc5","modified":1498618547942},{"_id":"public/categories/Face/index.html","hash":"d6b8019e16b0ba092da43b6fe3e9fb51ff760567","modified":1498618547942},{"_id":"public/categories/深度学习/index.html","hash":"515657c5f2e87afa0bb5617105a4f39f57f06ba8","modified":1498618547942},{"_id":"public/categories/Hexo/index.html","hash":"01f7dd324e5d8d3b0949747a423db2f1e7455196","modified":1498618547943},{"_id":"public/tags/笔记，人脸识别/index.html","hash":"1280d1cf380ede83fbbd684088130be5045544a9","modified":1498618547943},{"_id":"public/tags/caffe/index.html","hash":"47a3704a0b9e41596511125eae742ee872b0046c","modified":1498618547943},{"_id":"public/tags/调参/index.html","hash":"d9314e76e639396edf8a3a1431b0d1697ee9c221","modified":1498618547943},{"_id":"public/tags/Hexo/index.html","hash":"363daf86b1f681d4c37ac5e27c62d1665ef40fe9","modified":1498618547943},{"_id":"public/tags/Python/index.html","hash":"fae1ba4a6f71bbd08d43b3d1b1bb9802dfa251d1","modified":1498618547943},{"_id":"public/tags/深度学习/index.html","hash":"3824b4348d2d555095439ba7fd77bb96d322ce09","modified":1498618547944},{"_id":"public/tags/神经网络/index.html","hash":"24015ba8aad0c13f4179e949f8bd4202bc8afdbb","modified":1498618547944},{"_id":"public/tags/深度学习，人脸识别/index.html","hash":"fde30eb7d42ecf81b084026fcb75221421c41fc2","modified":1498618547944},{"_id":"public/2017/06/28/FaceNet论文笔记/index.html","hash":"4a8692f0ae4c0ac5d90a20e99abb26360b0ed866","modified":1498618547944},{"_id":"public/2017/06/17/Caffe调参/index.html","hash":"47c7f3f2415c72e5eca8a32885893e4fcec889ab","modified":1498618547944},{"_id":"public/2017/06/09/Python/index.html","hash":"25f6e393019ef28179e7a7b7840917a890c8ab7d","modified":1498618547944},{"_id":"public/2017/06/02/Triplet loss/index.html","hash":"c6508daf237975ab2bdc0e2ca471284d79ef0238","modified":1498618547945},{"_id":"public/2017/06/02/Hexo相关/index.html","hash":"299186bff2c6bc09dfa76d6924989ded63cb90e4","modified":1498618547945},{"_id":"public/2017/06/02/待办及进度/index.html","hash":"da688cd45669d45c821641b79f62d6da8f814d06","modified":1498618547945},{"_id":"public/2017/06/01/杂知识点/index.html","hash":"de30630389807db7276a1dbcc9dcb1bd019532dd","modified":1498618547945},{"_id":"public/archives/index.html","hash":"ad3d7b9c49f74a90bf70ca7febf068d74fa88e8c","modified":1498618547945},{"_id":"public/archives/2017/index.html","hash":"095e8e834252bdf416472ecc24bf9e7cbfbad85b","modified":1498618547950},{"_id":"public/archives/2017/06/index.html","hash":"1981646843beedecb218ceb3de51b884fa15f0fd","modified":1498618547950},{"_id":"public/index.html","hash":"f0e8a030361b5ce684ceb744ed63f29c90ecf422","modified":1498618547951},{"_id":"public/images/images.jpg","hash":"a83aa0596ff8d7ab87cfc47d4a57dd01d6c4ccee","modified":1498618548055},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1498618548055},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1498618548056},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1498618548056},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1498618548056},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1498618548056},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1498618548056},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1498618548057},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1498618548057},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1498618548057},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1498618548058},{"_id":"public/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1498618548058},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1498618548058},{"_id":"public/images/favicon.ico","hash":"3354f46359b13ee21fede9a2f64a81875daae6a5","modified":1498618548058},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1498618548058},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1498618548058},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1498618548058},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1498618548059},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1498618548059},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1498618548059},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1498618548060},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1498618548060},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1498618548060},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1498618548060},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1498618548060},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1498618548060},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1498618548061},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1498618548061},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1498618548061},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1498618548061},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1498618553153},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1498618553222},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1498618553266},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1498618553266},{"_id":"public/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1498618553266},{"_id":"public/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1498618553266},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1498618553267},{"_id":"public/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1498618553267},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1498618553267},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1498618553267},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1498618553268},{"_id":"public/js/src/utils.js","hash":"803f684fa7d0e729115a48851023a31f6fb6d0a7","modified":1498618553268},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1498618553268},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1498618553269},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1498618553273},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1498618553275},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"3587602ad777b031628bb5944864d1a4fcfea4ac","modified":1498618553275},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1498618553275},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1498618553276},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1498618553276},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1498618553276},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1498618553276},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1498618553276},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1498618553276},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1498618553276},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1498618553276},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1498618553277},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1498618553277},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1498618553277},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1498618553277},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1498618553277},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1498618553277},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1498618553277},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1498618553277},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1498618553277},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1498618553278},{"_id":"public/css/main.css","hash":"450775cdb6348d49e5f7cf84d92149b311da3da2","modified":1498618553278},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1498618553278},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1498618553278},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1498618553278},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1498618553278},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1498618553278},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1498618553278},{"_id":"public/lib/Han/dist/han.min.css","hash":"d9c0b3dc9158e717fde36f554709e6c3a22b5f85","modified":1498618553279},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1498618553279},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1498618553279},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1498618553279},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1498618553279},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1498618553279},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1498618553279},{"_id":"public/lib/Han/dist/han.css","hash":"38e48f275ad00daa9dcdcb8d9b44e576acda4707","modified":1498618553279},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1498618553280},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1498618553280},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1498618553280},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1498618553280},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1498618553280},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1498618553281},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1498618553282},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1498618553375}],"Category":[{"name":"Face","_id":"cj4gesbfc0004k4c899mwvqo9"},{"name":"深度学习","_id":"cj4gesbgm000ak4c83vmou1fy"},{"name":"Hexo","_id":"cj4gesbh1000gk4c8h7unjq99"}],"Data":[],"Page":[{"title":"about","date":"2017-05-31T12:22:54.000Z","_content":"\n本来无一物\n\nFrom Calyp\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-05-31 20:22:54\n---\n\n本来无一物\n\nFrom Calyp\n","updated":"2017-05-31T12:24:06.892Z","path":"about/index.html","comments":1,"layout":"page","_id":"cj4gesbe60000k4c81n709oal","content":"<p>本来无一物</p>\n<p>From Calyp</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本来无一物</p>\n<p>From Calyp</p>\n"},{"title":"tags","date":"2017-05-31T12:19:32.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2017-05-31 20:19:32\ntype: \"tags\"\n---\n","updated":"2017-05-31T12:20:34.309Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cj4gesbew0002k4c8cmx2fisl","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2017-05-31T12:21:40.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-05-31 20:21:40\ntype: \"categories\"\n---\n","updated":"2017-05-31T12:21:57.600Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cj4gesbfr0006k4c85iinmpl6","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"FaceNet论文笔记","date":"2017-06-28T02:11:42.000Z","description":null,"mathjax":null,"_content":"\n*原文链接*:[FaceNet:A unified embedding for face recognition and clustering](https://arxiv.org/abs/1503.03832)\n\n## 简介\n\nFaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。\n\n## 前言\n\nFaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。\n\n当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。","source":"_posts/FaceNet论文笔记.md","raw":"---\ntitle: FaceNet论文笔记\ndate: 2017-06-28 10:11:42\ncategories: Face\ntags: [笔记，人脸识别]\ndescription:\nmathjax:\n---\n\n*原文链接*:[FaceNet:A unified embedding for face recognition and clustering](https://arxiv.org/abs/1503.03832)\n\n## 简介\n\nFaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。\n\n## 前言\n\nFaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。\n\n当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。","slug":"FaceNet论文笔记","published":1,"updated":"2017-06-28T02:34:07.173Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj4gesbeh0001k4c8vzn3gc70","content":"<p><em>原文链接</em>:<a href=\"https://arxiv.org/abs/1503.03832\" target=\"_blank\" rel=\"external\">FaceNet:A unified embedding for face recognition and clustering</a></p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>FaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。</p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>FaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。</p>\n<p>当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。</p>\n","site":{"data":{}},"excerpt":"","more":"<p><em>原文链接</em>:<a href=\"https://arxiv.org/abs/1503.03832\" target=\"_blank\" rel=\"external\">FaceNet:A unified embedding for face recognition and clustering</a></p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>FaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。</p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>FaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。</p>\n<p>当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。</p>\n"},{"title":"Caffe调参","date":"2017-06-17T01:26:12.000Z","description":null,"mathjax":null,"_content":"\n# loss为nan\n\n**梯度爆炸**\n\n**原因**：梯度变得非常大，使得学习过程难以继续\n\n**现象：**观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。\n\n**措施**： \n\n1. 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 \n2. 设置clip gradient，用于限制过大的diff\n\n## \n\n**不当的损失函数**\n\n**原因**：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。\n\n**现象**：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。\n\n**措施**：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。\n\n示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。\n\n## \n\n**不当的输入**\n\n**原因**：输入中就含有NaN。\n\n**现象**：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。\n\n**措施**：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。\n\n**案例**：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。\n\n**池化层中步长比核的尺寸大**\n\n如下例所示，当池化层中stride > kernel的时候会在y中产生NaN\n\n```\n    layer {\n      name: \"faulty_pooling\"\n      type: \"Pooling\"\n      bottom: \"x\"\n      top: \"y\"\n      pooling_param {\n      pool: AVE\n      stride: 5\n      kernel: 3\n      }\n    }\n```\n\n## 致谢\n\n*http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training*\n\n\n\n# Accuracy一直为0\n\n考虑标签是否从0开始递增\n\n","source":"_posts/Caffe调参.md","raw":"---\ntitle: Caffe调参\ndate: 2017-06-17 09:26:12\ncategories: 深度学习\ntags: [caffe,调参]\ndescription:\nmathjax:\n---\n\n# loss为nan\n\n**梯度爆炸**\n\n**原因**：梯度变得非常大，使得学习过程难以继续\n\n**现象：**观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。\n\n**措施**： \n\n1. 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 \n2. 设置clip gradient，用于限制过大的diff\n\n## \n\n**不当的损失函数**\n\n**原因**：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。\n\n**现象**：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。\n\n**措施**：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。\n\n示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。\n\n## \n\n**不当的输入**\n\n**原因**：输入中就含有NaN。\n\n**现象**：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。\n\n**措施**：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。\n\n**案例**：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。\n\n**池化层中步长比核的尺寸大**\n\n如下例所示，当池化层中stride > kernel的时候会在y中产生NaN\n\n```\n    layer {\n      name: \"faulty_pooling\"\n      type: \"Pooling\"\n      bottom: \"x\"\n      top: \"y\"\n      pooling_param {\n      pool: AVE\n      stride: 5\n      kernel: 3\n      }\n    }\n```\n\n## 致谢\n\n*http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training*\n\n\n\n# Accuracy一直为0\n\n考虑标签是否从0开始递增\n\n","slug":"Caffe调参","published":1,"updated":"2017-06-17T02:24:49.318Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj4gesbf10003k4c89hqu88il","content":"<h1 id=\"loss为nan\"><a href=\"#loss为nan\" class=\"headerlink\" title=\"loss为nan\"></a>loss为nan</h1><p><strong>梯度爆炸</strong></p>\n<p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续</p>\n<p><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。</p>\n<p><strong>措施</strong>： </p>\n<ol>\n<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>\n<li>设置clip gradient，用于限制过大的diff</li>\n</ol>\n<p>## </p>\n<p><strong>不当的损失函数</strong></p>\n<p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p>\n<p><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>\n<p><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。</p>\n<p>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>\n<p>## </p>\n<p><strong>不当的输入</strong></p>\n<p><strong>原因</strong>：输入中就含有NaN。</p>\n<p><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>\n<p><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。</p>\n<p><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>\n<p><strong>池化层中步长比核的尺寸大</strong></p>\n<p>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">  name: &quot;faulty_pooling&quot;</div><div class=\"line\">  type: &quot;Pooling&quot;</div><div class=\"line\">  bottom: &quot;x&quot;</div><div class=\"line\">  top: &quot;y&quot;</div><div class=\"line\">  pooling_param &#123;</div><div class=\"line\">  pool: AVE</div><div class=\"line\">  stride: 5</div><div class=\"line\">  kernel: 3</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"致谢\"><a href=\"#致谢\" class=\"headerlink\" title=\"致谢\"></a>致谢</h2><p><em><a href=\"http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training</a></em></p>\n<h1 id=\"Accuracy一直为0\"><a href=\"#Accuracy一直为0\" class=\"headerlink\" title=\"Accuracy一直为0\"></a>Accuracy一直为0</h1><p>考虑标签是否从0开始递增</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"loss为nan\"><a href=\"#loss为nan\" class=\"headerlink\" title=\"loss为nan\"></a>loss为nan</h1><p><strong>梯度爆炸</strong></p>\n<p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续</p>\n<p><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。</p>\n<p><strong>措施</strong>： </p>\n<ol>\n<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>\n<li>设置clip gradient，用于限制过大的diff</li>\n</ol>\n<p>## </p>\n<p><strong>不当的损失函数</strong></p>\n<p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p>\n<p><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>\n<p><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。</p>\n<p>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>\n<p>## </p>\n<p><strong>不当的输入</strong></p>\n<p><strong>原因</strong>：输入中就含有NaN。</p>\n<p><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>\n<p><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。</p>\n<p><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>\n<p><strong>池化层中步长比核的尺寸大</strong></p>\n<p>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">  name: &quot;faulty_pooling&quot;</div><div class=\"line\">  type: &quot;Pooling&quot;</div><div class=\"line\">  bottom: &quot;x&quot;</div><div class=\"line\">  top: &quot;y&quot;</div><div class=\"line\">  pooling_param &#123;</div><div class=\"line\">  pool: AVE</div><div class=\"line\">  stride: 5</div><div class=\"line\">  kernel: 3</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<h2 id=\"致谢\"><a href=\"#致谢\" class=\"headerlink\" title=\"致谢\"></a>致谢</h2><p><em><a href=\"http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training</a></em></p>\n<h1 id=\"Accuracy一直为0\"><a href=\"#Accuracy一直为0\" class=\"headerlink\" title=\"Accuracy一直为0\"></a>Accuracy一直为0</h1><p>考虑标签是否从0开始递增</p>\n"},{"title":"待办及进度","date":"2017-06-02T07:15:46.000Z","description":null,"_content":"\n| Model              | PCA Size | Threshold | Score  |\n| ------------------ | -------- | --------- | ------ |\n| Mirror             | 192      | 0.64      | 99.42% |\n| Mirror Concat      | 192      | 0.65      | 99.42% |\n| Mirror Add/Average | 184      | 0.64      | 99.47% |\n| Mirror Max         | 144      | 0.65      | 99.43% |\n| Mirror Min         | 168      | 0.65      | 99.48% |\n| Mirror Avg+min     | 168      | 0.65      | 99.45% |\n\n\n\n\n\n| eye_width | num      | eye_width | num    |\n| --------- | -------- | --------- | ------ |\n| 0~10      | 14       | 90~100    | 8,5674 |\n| 10~20     | 3329     | 100~110   | 2,9481 |\n| 20~30     | 21,3675  | 110~120   | 9051   |\n| 30~40     | 45,1416  | 120~130   | 2894   |\n| 40~50     | 49,1913  | 130~140   | 932    |\n| 50~60     | 72,8911  | 140~150   | 279    |\n| 60~70     | 137,9232 | 150~160   | 86     |\n| 70~80     | 128,7442 | 160~170   | 14     |\n| 80~90     | 30,9026  | 170~180   | 6      |\n\n![figure](C:\\Users\\lc\\Desktop\\test_crop\\all\\figure.png)\n\n\n# 问题\n\n1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y...],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)*0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5*eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句\n```\nif eye_width*0.5>eye_left_x:\n            eye_width=eye_left_x*2\n```\n导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张\n\n重新生成眼睛宽度估算文件，其分布如下\n\n| eye_width | num      | eye_width | num  |\n| --------- | -------- | --------- | ---- |\n| 0~10      | 1,8975   | 100~110   | 98   |\n| 10~20     | 71,3760  | 110~120   | 17   |\n| 20~30     | 126,4102 | 120~130   | 3    |\n| 30~40     | 226,4348 | 130~140   | 0    |\n| 40~50     | 59,3351  | 140~150   | 0    |\n| 50~60     | 10,6502  | 150~160   | 0    |\n| 60~70     | 2,4588   | 160~170   | 1    |\n| 70~80     | 5592     | 170~180   | 0    |\n| 80~90     | 1588     | 180~190   | 8    |\n| 90~100    | 421      | 190~200   | 21   |\n\n![figure2](C:\\Users\\lc\\Desktop\\test_crop\\all\\figure2.png)\n\n\n\n\n2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可\n3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。\n最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526\n","source":"_posts/待办及进度.md","raw":"---\ntitle: 待办及进度\ndate: 2017-06-02 15:15:46\ncategories:\ntags:\ndescription:\n---\n\n| Model              | PCA Size | Threshold | Score  |\n| ------------------ | -------- | --------- | ------ |\n| Mirror             | 192      | 0.64      | 99.42% |\n| Mirror Concat      | 192      | 0.65      | 99.42% |\n| Mirror Add/Average | 184      | 0.64      | 99.47% |\n| Mirror Max         | 144      | 0.65      | 99.43% |\n| Mirror Min         | 168      | 0.65      | 99.48% |\n| Mirror Avg+min     | 168      | 0.65      | 99.45% |\n\n\n\n\n\n| eye_width | num      | eye_width | num    |\n| --------- | -------- | --------- | ------ |\n| 0~10      | 14       | 90~100    | 8,5674 |\n| 10~20     | 3329     | 100~110   | 2,9481 |\n| 20~30     | 21,3675  | 110~120   | 9051   |\n| 30~40     | 45,1416  | 120~130   | 2894   |\n| 40~50     | 49,1913  | 130~140   | 932    |\n| 50~60     | 72,8911  | 140~150   | 279    |\n| 60~70     | 137,9232 | 150~160   | 86     |\n| 70~80     | 128,7442 | 160~170   | 14     |\n| 80~90     | 30,9026  | 170~180   | 6      |\n\n![figure](C:\\Users\\lc\\Desktop\\test_crop\\all\\figure.png)\n\n\n# 问题\n\n1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y...],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)*0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5*eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句\n```\nif eye_width*0.5>eye_left_x:\n            eye_width=eye_left_x*2\n```\n导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张\n\n重新生成眼睛宽度估算文件，其分布如下\n\n| eye_width | num      | eye_width | num  |\n| --------- | -------- | --------- | ---- |\n| 0~10      | 1,8975   | 100~110   | 98   |\n| 10~20     | 71,3760  | 110~120   | 17   |\n| 20~30     | 126,4102 | 120~130   | 3    |\n| 30~40     | 226,4348 | 130~140   | 0    |\n| 40~50     | 59,3351  | 140~150   | 0    |\n| 50~60     | 10,6502  | 150~160   | 0    |\n| 60~70     | 2,4588   | 160~170   | 1    |\n| 70~80     | 5592     | 170~180   | 0    |\n| 80~90     | 1588     | 180~190   | 8    |\n| 90~100    | 421      | 190~200   | 21   |\n\n![figure2](C:\\Users\\lc\\Desktop\\test_crop\\all\\figure2.png)\n\n\n\n\n2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可\n3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。\n最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526\n","slug":"待办及进度","published":1,"updated":"2017-06-16T02:33:59.782Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj4gesbfw0007k4c8yozua3r9","content":"<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>PCA Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Mirror</td>\n<td>192</td>\n<td>0.64</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Concat</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Add/Average</td>\n<td>184</td>\n<td>0.64</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>Mirror Max</td>\n<td>144</td>\n<td>0.65</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>Mirror Min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.48%</td>\n</tr>\n<tr>\n<td>Mirror Avg+min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.45%</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>14</td>\n<td>90~100</td>\n<td>8,5674</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>3329</td>\n<td>100~110</td>\n<td>2,9481</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>21,3675</td>\n<td>110~120</td>\n<td>9051</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>45,1416</td>\n<td>120~130</td>\n<td>2894</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>49,1913</td>\n<td>130~140</td>\n<td>932</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>72,8911</td>\n<td>140~150</td>\n<td>279</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>137,9232</td>\n<td>150~160</td>\n<td>86</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>128,7442</td>\n<td>160~170</td>\n<td>14</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>30,9026</td>\n<td>170~180</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"C:\\Users\\lc\\Desktop\\test_crop\\all\\figure.png\" alt=\"figure\"></p>\n<h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><p>1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y…],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)<em>0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5</em>eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">if eye_width*0.5&gt;eye_left_x:</div><div class=\"line\">            eye_width=eye_left_x*2</div></pre></td></tr></table></figure></p>\n<p>导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张</p>\n<p>重新生成眼睛宽度估算文件，其分布如下</p>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>1,8975</td>\n<td>100~110</td>\n<td>98</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>71,3760</td>\n<td>110~120</td>\n<td>17</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>126,4102</td>\n<td>120~130</td>\n<td>3</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>226,4348</td>\n<td>130~140</td>\n<td>0</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>59,3351</td>\n<td>140~150</td>\n<td>0</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>10,6502</td>\n<td>150~160</td>\n<td>0</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>2,4588</td>\n<td>160~170</td>\n<td>1</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>5592</td>\n<td>170~180</td>\n<td>0</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>1588</td>\n<td>180~190</td>\n<td>8</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>421</td>\n<td>190~200</td>\n<td>21</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"C:\\Users\\lc\\Desktop\\test_crop\\all\\figure2.png\" alt=\"figure2\"></p>\n<p>2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可<br>3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。<br>最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526</p>\n","site":{"data":{}},"excerpt":"","more":"<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>PCA Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Mirror</td>\n<td>192</td>\n<td>0.64</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Concat</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Add/Average</td>\n<td>184</td>\n<td>0.64</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>Mirror Max</td>\n<td>144</td>\n<td>0.65</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>Mirror Min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.48%</td>\n</tr>\n<tr>\n<td>Mirror Avg+min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.45%</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>14</td>\n<td>90~100</td>\n<td>8,5674</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>3329</td>\n<td>100~110</td>\n<td>2,9481</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>21,3675</td>\n<td>110~120</td>\n<td>9051</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>45,1416</td>\n<td>120~130</td>\n<td>2894</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>49,1913</td>\n<td>130~140</td>\n<td>932</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>72,8911</td>\n<td>140~150</td>\n<td>279</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>137,9232</td>\n<td>150~160</td>\n<td>86</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>128,7442</td>\n<td>160~170</td>\n<td>14</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>30,9026</td>\n<td>170~180</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"C:\\Users\\lc\\Desktop\\test_crop\\all\\figure.png\" alt=\"figure\"></p>\n<h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><p>1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y…],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)<em>0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5</em>eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">if eye_width*0.5&gt;eye_left_x:</div><div class=\"line\">            eye_width=eye_left_x*2</div></pre></td></tr></table></figure></p>\n<p>导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张</p>\n<p>重新生成眼睛宽度估算文件，其分布如下</p>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>1,8975</td>\n<td>100~110</td>\n<td>98</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>71,3760</td>\n<td>110~120</td>\n<td>17</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>126,4102</td>\n<td>120~130</td>\n<td>3</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>226,4348</td>\n<td>130~140</td>\n<td>0</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>59,3351</td>\n<td>140~150</td>\n<td>0</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>10,6502</td>\n<td>150~160</td>\n<td>0</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>2,4588</td>\n<td>160~170</td>\n<td>1</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>5592</td>\n<td>170~180</td>\n<td>0</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>1588</td>\n<td>180~190</td>\n<td>8</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>421</td>\n<td>190~200</td>\n<td>21</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"C:\\Users\\lc\\Desktop\\test_crop\\all\\figure2.png\" alt=\"figure2\"></p>\n<p>2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可<br>3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。<br>最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526</p>\n"},{"title":"Hexo相关","mathjax":true,"date":"2017-06-02T08:05:45.000Z","description":null,"_content":"\n# 问题\n\n## Hexo无法正常显示公式\n\n善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：\n\n```\n# MathJax Support\nmathjax:\n  enable: true\n  per_page: true\n```\n\n另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：\n\n```\n---\ntitle: index.html\ndate: 2016-12-28 21:01:30\ntags:\nmathjax: true\n--\n```\n\n题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：\n\n```\ntitle: {{ title }}\ndate: {{ date }}\ncategories: \ntags:\ndescription: \nmathjax: \n```\n\n\n\n# 优化\n\n","source":"_posts/Hexo相关.md","raw":"---\ntitle: Hexo相关\nmathjax: true\ndate: 2017-06-02 16:05:45\ncategories: Hexo\ntags: [Hexo]\ndescription:\n---\n\n# 问题\n\n## Hexo无法正常显示公式\n\n善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：\n\n```\n# MathJax Support\nmathjax:\n  enable: true\n  per_page: true\n```\n\n另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：\n\n```\n---\ntitle: index.html\ndate: 2016-12-28 21:01:30\ntags:\nmathjax: true\n--\n```\n\n题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：\n\n```\ntitle: {{ title }}\ndate: {{ date }}\ncategories: \ntags:\ndescription: \nmathjax: \n```\n\n\n\n# 优化\n\n","slug":"Hexo相关","published":1,"updated":"2017-06-02T08:25:37.278Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj4gesbg50008k4c8nupvetgb","content":"<h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><h2 id=\"Hexo无法正常显示公式\"><a href=\"#Hexo无法正常显示公式\" class=\"headerlink\" title=\"Hexo无法正常显示公式\"></a>Hexo无法正常显示公式</h2><p>善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"># MathJax Support</div><div class=\"line\">mathjax:</div><div class=\"line\">  enable: true</div><div class=\"line\">  per_page: true</div></pre></td></tr></table></figure>\n<p>另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">---</div><div class=\"line\">title: index.html</div><div class=\"line\">date: 2016-12-28 21:01:30</div><div class=\"line\">tags:</div><div class=\"line\">mathjax: true</div><div class=\"line\">--</div></pre></td></tr></table></figure>\n<p>题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">title: &#123;&#123; title &#125;&#125;</div><div class=\"line\">date: &#123;&#123; date &#125;&#125;</div><div class=\"line\">categories: </div><div class=\"line\">tags:</div><div class=\"line\">description: </div><div class=\"line\">mathjax:</div></pre></td></tr></table></figure>\n<h1 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h1>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><h2 id=\"Hexo无法正常显示公式\"><a href=\"#Hexo无法正常显示公式\" class=\"headerlink\" title=\"Hexo无法正常显示公式\"></a>Hexo无法正常显示公式</h2><p>善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"># MathJax Support</div><div class=\"line\">mathjax:</div><div class=\"line\">  enable: true</div><div class=\"line\">  per_page: true</div></pre></td></tr></table></figure>\n<p>另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">---</div><div class=\"line\">title: index.html</div><div class=\"line\">date: 2016-12-28 21:01:30</div><div class=\"line\">tags:</div><div class=\"line\">mathjax: true</div><div class=\"line\">--</div></pre></td></tr></table></figure>\n<p>题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">title: &#123;&#123; title &#125;&#125;</div><div class=\"line\">date: &#123;&#123; date &#125;&#125;</div><div class=\"line\">categories: </div><div class=\"line\">tags:</div><div class=\"line\">description: </div><div class=\"line\">mathjax:</div></pre></td></tr></table></figure>\n<h1 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h1>"},{"title":"Python","date":"2017-06-09T12:52:56.000Z","description":"Python","mathjax":false,"_content":"\n# 图像操作\n\n## 关于PIL image和skimage的图像处理\n\n### 对skimage图像\n\n镜像处理：\n\n```\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\nimg=io.imread(\"test.jpg\")\n#img=transform.rotate(img,180)\nimg=cv2.flip(img,1)\nplt.figure('skimage')\nplt.imshow(img)\nplt.show()\nprint img.shape\nprint(img.dtype)\n```\n\n# 文本处理\n\n## 提取两个文件中相同的部分\n\na.txt内容：\n\n```\naaa 0\nbbb 0\nccc 0\n```\n\nb.txt内容：\n\n```\naaa 0\nbbb 1\nccc 0\n```\n\n### 提取相同的部分\n\n写入到c.txt\n\n```\nfa=open('a.txt','r')\na=fa.readlines()\nfa.close()\nfb=open('b.txt','r')\nb=fb.readlines()\nfb.close()\nc= [i for i in a if i in b]\nfc=open('c.txt','w')\nfc.writelines(c)\nfc.close()\nprint 'Done'\n```\n\n最后c.txt内容\n\n```\naaa 0\nccc 0\n```\n\n\n\n# 读取文件并绘制图片\n\n## 散点图\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    X = []\n    y = []\n    for line in inFile:\n        trainingSet = line.split()  \n        X.append(trainingSet[0])\n        y.append(trainingSet[1])\n    return (X, y)\n\n\ndef plotData(X, y):\n    length = len(y)\n    plt.figure(1)\n    #plt.plot(X, y, 'rx')\n    plt.scatter(X,y,c='r',marker='.')\n    plt.xlabel('eye_width')\n    plt.ylabel('eye_height')\n    #plt.show()\n    plt.savefig('dis.png')\n\nif __name__ == '__main__':\n    (X, y) = loadData('dis.txt')\n\n    plotData(X, y)\n```\n\n## 折线图\n\n```\nimport matplotlib.pyplot as plt \n\ny1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] \nx1=range(0,200,10) \n\nnum = 0\nfor i in range(20):\n    num+=y1[i]\nprint num\nplt.plot(x1,y1,label='Frist line',linewidth=1,color='r',marker='o', \nmarkerfacecolor='blue',markersize=6) \nplt.xlabel('eye_width distribute') \nplt.ylabel('Num') \nplt.title('Eye\\nCheck it out') \nplt.legend()\nplt.savefig('figure.png') \nplt.show() \n```\n\n\n\n# 统计文件中的数据分布\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    x_lines=inFile.readlines()#x_lines为str的list\n    x_distribute=[0]*20 #对列表元素进行重复复制\n    for x_line in x_lines:\n        x_point=x_line.split()[0]\n        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型\n        x_distribute[i]+=1\n    print x_distribute\n\nif __name__ == '__main__':\n    loadData('dis.txt')\n```\n\n# windows下安装OpenCV for Python\n\n1. Download Python, Numpy, OpenCV from their official sites.\n\n2. Extract OpenCV (will be extracted to a folder opencv)\n\n3. Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd\n\n4. Paste it in C:\\Python27\\Lib\\site-packages\n\n5. Open Python IDLE or terminal, and type\n\n   ```\n   >>> import cv2\n   ```\n\n# 读取文件，进行批量创建目录\n\n存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：\n\n```\nxxx/0.jpg\nyyy/0.jpg\nzzz/0.jpg\n```\n\n创建xxx目录，yyy目录，zzz目录\n\n```\nimport sys\nimport os\nimport numpy as np\nimport skimage\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef read_file(path):\n    with open(path) as f:\n        return list(f)\n\n\ndef make_dir(image_path):   \n    image_lines = read_file(image_path)\n    if not image_lines:\n        print 'empty file'\n        return\n    i = 0\n    for image_line in image_lines:\n        image_line = image_line.strip('\\n')\n        subdir_name = image_line.split('/')[0]\n        print subdir_name\n        isExists=os.path.exists(subdir_name)\n        if not isExists:\n            os.mkdir(subdir_name)\n            print subdir_name+\"created successfully!\"\n       \n        i = i+1\n        sys.stdout.write('\\riter %d\\n' %(i))\n        sys.stdout.flush()\n\n    print 'Done'\n\nif __name__=='__main__': \n    image_path='./image.txt'\n    make_dir(image_path)\n\n```\n\n","source":"_posts/Python.md","raw":"---\ntitle: Python\ndate: 2017-06-09 20:52:56\ncategories:\ntags: Python\ndescription: Python\nmathjax: False\n---\n\n# 图像操作\n\n## 关于PIL image和skimage的图像处理\n\n### 对skimage图像\n\n镜像处理：\n\n```\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\nimg=io.imread(\"test.jpg\")\n#img=transform.rotate(img,180)\nimg=cv2.flip(img,1)\nplt.figure('skimage')\nplt.imshow(img)\nplt.show()\nprint img.shape\nprint(img.dtype)\n```\n\n# 文本处理\n\n## 提取两个文件中相同的部分\n\na.txt内容：\n\n```\naaa 0\nbbb 0\nccc 0\n```\n\nb.txt内容：\n\n```\naaa 0\nbbb 1\nccc 0\n```\n\n### 提取相同的部分\n\n写入到c.txt\n\n```\nfa=open('a.txt','r')\na=fa.readlines()\nfa.close()\nfb=open('b.txt','r')\nb=fb.readlines()\nfb.close()\nc= [i for i in a if i in b]\nfc=open('c.txt','w')\nfc.writelines(c)\nfc.close()\nprint 'Done'\n```\n\n最后c.txt内容\n\n```\naaa 0\nccc 0\n```\n\n\n\n# 读取文件并绘制图片\n\n## 散点图\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    X = []\n    y = []\n    for line in inFile:\n        trainingSet = line.split()  \n        X.append(trainingSet[0])\n        y.append(trainingSet[1])\n    return (X, y)\n\n\ndef plotData(X, y):\n    length = len(y)\n    plt.figure(1)\n    #plt.plot(X, y, 'rx')\n    plt.scatter(X,y,c='r',marker='.')\n    plt.xlabel('eye_width')\n    plt.ylabel('eye_height')\n    #plt.show()\n    plt.savefig('dis.png')\n\nif __name__ == '__main__':\n    (X, y) = loadData('dis.txt')\n\n    plotData(X, y)\n```\n\n## 折线图\n\n```\nimport matplotlib.pyplot as plt \n\ny1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] \nx1=range(0,200,10) \n\nnum = 0\nfor i in range(20):\n    num+=y1[i]\nprint num\nplt.plot(x1,y1,label='Frist line',linewidth=1,color='r',marker='o', \nmarkerfacecolor='blue',markersize=6) \nplt.xlabel('eye_width distribute') \nplt.ylabel('Num') \nplt.title('Eye\\nCheck it out') \nplt.legend()\nplt.savefig('figure.png') \nplt.show() \n```\n\n\n\n# 统计文件中的数据分布\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    x_lines=inFile.readlines()#x_lines为str的list\n    x_distribute=[0]*20 #对列表元素进行重复复制\n    for x_line in x_lines:\n        x_point=x_line.split()[0]\n        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型\n        x_distribute[i]+=1\n    print x_distribute\n\nif __name__ == '__main__':\n    loadData('dis.txt')\n```\n\n# windows下安装OpenCV for Python\n\n1. Download Python, Numpy, OpenCV from their official sites.\n\n2. Extract OpenCV (will be extracted to a folder opencv)\n\n3. Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd\n\n4. Paste it in C:\\Python27\\Lib\\site-packages\n\n5. Open Python IDLE or terminal, and type\n\n   ```\n   >>> import cv2\n   ```\n\n# 读取文件，进行批量创建目录\n\n存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：\n\n```\nxxx/0.jpg\nyyy/0.jpg\nzzz/0.jpg\n```\n\n创建xxx目录，yyy目录，zzz目录\n\n```\nimport sys\nimport os\nimport numpy as np\nimport skimage\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef read_file(path):\n    with open(path) as f:\n        return list(f)\n\n\ndef make_dir(image_path):   \n    image_lines = read_file(image_path)\n    if not image_lines:\n        print 'empty file'\n        return\n    i = 0\n    for image_line in image_lines:\n        image_line = image_line.strip('\\n')\n        subdir_name = image_line.split('/')[0]\n        print subdir_name\n        isExists=os.path.exists(subdir_name)\n        if not isExists:\n            os.mkdir(subdir_name)\n            print subdir_name+\"created successfully!\"\n       \n        i = i+1\n        sys.stdout.write('\\riter %d\\n' %(i))\n        sys.stdout.flush()\n\n    print 'Done'\n\nif __name__=='__main__': \n    image_path='./image.txt'\n    make_dir(image_path)\n\n```\n\n","slug":"Python","published":1,"updated":"2017-06-14T01:51:24.227Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj4gesbgj0009k4c8uops6yoy","content":"<h1 id=\"图像操作\"><a href=\"#图像操作\" class=\"headerlink\" title=\"图像操作\"></a>图像操作</h1><h2 id=\"关于PIL-image和skimage的图像处理\"><a href=\"#关于PIL-image和skimage的图像处理\" class=\"headerlink\" title=\"关于PIL image和skimage的图像处理\"></a>关于PIL image和skimage的图像处理</h2><h3 id=\"对skimage图像\"><a href=\"#对skimage图像\" class=\"headerlink\" title=\"对skimage图像\"></a>对skimage图像</h3><p>镜像处理：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">img=io.imread(&quot;test.jpg&quot;)</div><div class=\"line\">#img=transform.rotate(img,180)</div><div class=\"line\">img=cv2.flip(img,1)</div><div class=\"line\">plt.figure(&apos;skimage&apos;)</div><div class=\"line\">plt.imshow(img)</div><div class=\"line\">plt.show()</div><div class=\"line\">print img.shape</div><div class=\"line\">print(img.dtype)</div></pre></td></tr></table></figure>\n<h1 id=\"文本处理\"><a href=\"#文本处理\" class=\"headerlink\" title=\"文本处理\"></a>文本处理</h1><h2 id=\"提取两个文件中相同的部分\"><a href=\"#提取两个文件中相同的部分\" class=\"headerlink\" title=\"提取两个文件中相同的部分\"></a>提取两个文件中相同的部分</h2><p>a.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<p>b.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 1</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h3 id=\"提取相同的部分\"><a href=\"#提取相同的部分\" class=\"headerlink\" title=\"提取相同的部分\"></a>提取相同的部分</h3><p>写入到c.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">fa=open(&apos;a.txt&apos;,&apos;r&apos;)</div><div class=\"line\">a=fa.readlines()</div><div class=\"line\">fa.close()</div><div class=\"line\">fb=open(&apos;b.txt&apos;,&apos;r&apos;)</div><div class=\"line\">b=fb.readlines()</div><div class=\"line\">fb.close()</div><div class=\"line\">c= [i for i in a if i in b]</div><div class=\"line\">fc=open(&apos;c.txt&apos;,&apos;w&apos;)</div><div class=\"line\">fc.writelines(c)</div><div class=\"line\">fc.close()</div><div class=\"line\">print &apos;Done&apos;</div></pre></td></tr></table></figure>\n<p>最后c.txt内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h1 id=\"读取文件并绘制图片\"><a href=\"#读取文件并绘制图片\" class=\"headerlink\" title=\"读取文件并绘制图片\"></a>读取文件并绘制图片</h1><h2 id=\"散点图\"><a href=\"#散点图\" class=\"headerlink\" title=\"散点图\"></a>散点图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    X = []</div><div class=\"line\">    y = []</div><div class=\"line\">    for line in inFile:</div><div class=\"line\">        trainingSet = line.split()  </div><div class=\"line\">        X.append(trainingSet[0])</div><div class=\"line\">        y.append(trainingSet[1])</div><div class=\"line\">    return (X, y)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def plotData(X, y):</div><div class=\"line\">    length = len(y)</div><div class=\"line\">    plt.figure(1)</div><div class=\"line\">    #plt.plot(X, y, &apos;rx&apos;)</div><div class=\"line\">    plt.scatter(X,y,c=&apos;r&apos;,marker=&apos;.&apos;)</div><div class=\"line\">    plt.xlabel(&apos;eye_width&apos;)</div><div class=\"line\">    plt.ylabel(&apos;eye_height&apos;)</div><div class=\"line\">    #plt.show()</div><div class=\"line\">    plt.savefig(&apos;dis.png&apos;)</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    (X, y) = loadData(&apos;dis.txt&apos;)</div><div class=\"line\"></div><div class=\"line\">    plotData(X, y)</div></pre></td></tr></table></figure>\n<h2 id=\"折线图\"><a href=\"#折线图\" class=\"headerlink\" title=\"折线图\"></a>折线图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib.pyplot as plt </div><div class=\"line\"></div><div class=\"line\">y1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] </div><div class=\"line\">x1=range(0,200,10) </div><div class=\"line\"></div><div class=\"line\">num = 0</div><div class=\"line\">for i in range(20):</div><div class=\"line\">    num+=y1[i]</div><div class=\"line\">print num</div><div class=\"line\">plt.plot(x1,y1,label=&apos;Frist line&apos;,linewidth=1,color=&apos;r&apos;,marker=&apos;o&apos;, </div><div class=\"line\">markerfacecolor=&apos;blue&apos;,markersize=6) </div><div class=\"line\">plt.xlabel(&apos;eye_width distribute&apos;) </div><div class=\"line\">plt.ylabel(&apos;Num&apos;) </div><div class=\"line\">plt.title(&apos;Eye\\nCheck it out&apos;) </div><div class=\"line\">plt.legend()</div><div class=\"line\">plt.savefig(&apos;figure.png&apos;) </div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure>\n<h1 id=\"统计文件中的数据分布\"><a href=\"#统计文件中的数据分布\" class=\"headerlink\" title=\"统计文件中的数据分布\"></a>统计文件中的数据分布</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import numpy as np</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    x_lines=inFile.readlines()#x_lines为str的list</div><div class=\"line\">    x_distribute=[0]*20 #对列表元素进行重复复制</div><div class=\"line\">    for x_line in x_lines:</div><div class=\"line\">        x_point=x_line.split()[0]</div><div class=\"line\">        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型</div><div class=\"line\">        x_distribute[i]+=1</div><div class=\"line\">    print x_distribute</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    loadData(&apos;dis.txt&apos;)</div></pre></td></tr></table></figure>\n<h1 id=\"windows下安装OpenCV-for-Python\"><a href=\"#windows下安装OpenCV-for-Python\" class=\"headerlink\" title=\"windows下安装OpenCV for Python\"></a>windows下安装OpenCV for Python</h1><ol>\n<li><p>Download Python, Numpy, OpenCV from their official sites.</p>\n</li>\n<li><p>Extract OpenCV (will be extracted to a folder opencv)</p>\n</li>\n<li><p>Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd</p>\n</li>\n<li><p>Paste it in C:\\Python27\\Lib\\site-packages</p>\n</li>\n<li><p>Open Python IDLE or terminal, and type</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&gt;&gt;&gt; import cv2</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"读取文件，进行批量创建目录\"><a href=\"#读取文件，进行批量创建目录\" class=\"headerlink\" title=\"读取文件，进行批量创建目录\"></a>读取文件，进行批量创建目录</h1><p>存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">xxx/0.jpg</div><div class=\"line\">yyy/0.jpg</div><div class=\"line\">zzz/0.jpg</div></pre></td></tr></table></figure>\n<p>创建xxx目录，yyy目录，zzz目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">import sys</div><div class=\"line\">import os</div><div class=\"line\">import numpy as np</div><div class=\"line\">import skimage</div><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">def read_file(path):</div><div class=\"line\">    with open(path) as f:</div><div class=\"line\">        return list(f)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def make_dir(image_path):   </div><div class=\"line\">    image_lines = read_file(image_path)</div><div class=\"line\">    if not image_lines:</div><div class=\"line\">        print &apos;empty file&apos;</div><div class=\"line\">        return</div><div class=\"line\">    i = 0</div><div class=\"line\">    for image_line in image_lines:</div><div class=\"line\">        image_line = image_line.strip(&apos;\\n&apos;)</div><div class=\"line\">        subdir_name = image_line.split(&apos;/&apos;)[0]</div><div class=\"line\">        print subdir_name</div><div class=\"line\">        isExists=os.path.exists(subdir_name)</div><div class=\"line\">        if not isExists:</div><div class=\"line\">            os.mkdir(subdir_name)</div><div class=\"line\">            print subdir_name+&quot;created successfully!&quot;</div><div class=\"line\">       </div><div class=\"line\">        i = i+1</div><div class=\"line\">        sys.stdout.write(&apos;\\riter %d\\n&apos; %(i))</div><div class=\"line\">        sys.stdout.flush()</div><div class=\"line\"></div><div class=\"line\">    print &apos;Done&apos;</div><div class=\"line\"></div><div class=\"line\">if __name__==&apos;__main__&apos;: </div><div class=\"line\">    image_path=&apos;./image.txt&apos;</div><div class=\"line\">    make_dir(image_path)</div></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"图像操作\"><a href=\"#图像操作\" class=\"headerlink\" title=\"图像操作\"></a>图像操作</h1><h2 id=\"关于PIL-image和skimage的图像处理\"><a href=\"#关于PIL-image和skimage的图像处理\" class=\"headerlink\" title=\"关于PIL image和skimage的图像处理\"></a>关于PIL image和skimage的图像处理</h2><h3 id=\"对skimage图像\"><a href=\"#对skimage图像\" class=\"headerlink\" title=\"对skimage图像\"></a>对skimage图像</h3><p>镜像处理：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">img=io.imread(&quot;test.jpg&quot;)</div><div class=\"line\">#img=transform.rotate(img,180)</div><div class=\"line\">img=cv2.flip(img,1)</div><div class=\"line\">plt.figure(&apos;skimage&apos;)</div><div class=\"line\">plt.imshow(img)</div><div class=\"line\">plt.show()</div><div class=\"line\">print img.shape</div><div class=\"line\">print(img.dtype)</div></pre></td></tr></table></figure>\n<h1 id=\"文本处理\"><a href=\"#文本处理\" class=\"headerlink\" title=\"文本处理\"></a>文本处理</h1><h2 id=\"提取两个文件中相同的部分\"><a href=\"#提取两个文件中相同的部分\" class=\"headerlink\" title=\"提取两个文件中相同的部分\"></a>提取两个文件中相同的部分</h2><p>a.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<p>b.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 1</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h3 id=\"提取相同的部分\"><a href=\"#提取相同的部分\" class=\"headerlink\" title=\"提取相同的部分\"></a>提取相同的部分</h3><p>写入到c.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">fa=open(&apos;a.txt&apos;,&apos;r&apos;)</div><div class=\"line\">a=fa.readlines()</div><div class=\"line\">fa.close()</div><div class=\"line\">fb=open(&apos;b.txt&apos;,&apos;r&apos;)</div><div class=\"line\">b=fb.readlines()</div><div class=\"line\">fb.close()</div><div class=\"line\">c= [i for i in a if i in b]</div><div class=\"line\">fc=open(&apos;c.txt&apos;,&apos;w&apos;)</div><div class=\"line\">fc.writelines(c)</div><div class=\"line\">fc.close()</div><div class=\"line\">print &apos;Done&apos;</div></pre></td></tr></table></figure>\n<p>最后c.txt内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h1 id=\"读取文件并绘制图片\"><a href=\"#读取文件并绘制图片\" class=\"headerlink\" title=\"读取文件并绘制图片\"></a>读取文件并绘制图片</h1><h2 id=\"散点图\"><a href=\"#散点图\" class=\"headerlink\" title=\"散点图\"></a>散点图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    X = []</div><div class=\"line\">    y = []</div><div class=\"line\">    for line in inFile:</div><div class=\"line\">        trainingSet = line.split()  </div><div class=\"line\">        X.append(trainingSet[0])</div><div class=\"line\">        y.append(trainingSet[1])</div><div class=\"line\">    return (X, y)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def plotData(X, y):</div><div class=\"line\">    length = len(y)</div><div class=\"line\">    plt.figure(1)</div><div class=\"line\">    #plt.plot(X, y, &apos;rx&apos;)</div><div class=\"line\">    plt.scatter(X,y,c=&apos;r&apos;,marker=&apos;.&apos;)</div><div class=\"line\">    plt.xlabel(&apos;eye_width&apos;)</div><div class=\"line\">    plt.ylabel(&apos;eye_height&apos;)</div><div class=\"line\">    #plt.show()</div><div class=\"line\">    plt.savefig(&apos;dis.png&apos;)</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    (X, y) = loadData(&apos;dis.txt&apos;)</div><div class=\"line\"></div><div class=\"line\">    plotData(X, y)</div></pre></td></tr></table></figure>\n<h2 id=\"折线图\"><a href=\"#折线图\" class=\"headerlink\" title=\"折线图\"></a>折线图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib.pyplot as plt </div><div class=\"line\"></div><div class=\"line\">y1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] </div><div class=\"line\">x1=range(0,200,10) </div><div class=\"line\"></div><div class=\"line\">num = 0</div><div class=\"line\">for i in range(20):</div><div class=\"line\">    num+=y1[i]</div><div class=\"line\">print num</div><div class=\"line\">plt.plot(x1,y1,label=&apos;Frist line&apos;,linewidth=1,color=&apos;r&apos;,marker=&apos;o&apos;, </div><div class=\"line\">markerfacecolor=&apos;blue&apos;,markersize=6) </div><div class=\"line\">plt.xlabel(&apos;eye_width distribute&apos;) </div><div class=\"line\">plt.ylabel(&apos;Num&apos;) </div><div class=\"line\">plt.title(&apos;Eye\\nCheck it out&apos;) </div><div class=\"line\">plt.legend()</div><div class=\"line\">plt.savefig(&apos;figure.png&apos;) </div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure>\n<h1 id=\"统计文件中的数据分布\"><a href=\"#统计文件中的数据分布\" class=\"headerlink\" title=\"统计文件中的数据分布\"></a>统计文件中的数据分布</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import numpy as np</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    x_lines=inFile.readlines()#x_lines为str的list</div><div class=\"line\">    x_distribute=[0]*20 #对列表元素进行重复复制</div><div class=\"line\">    for x_line in x_lines:</div><div class=\"line\">        x_point=x_line.split()[0]</div><div class=\"line\">        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型</div><div class=\"line\">        x_distribute[i]+=1</div><div class=\"line\">    print x_distribute</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    loadData(&apos;dis.txt&apos;)</div></pre></td></tr></table></figure>\n<h1 id=\"windows下安装OpenCV-for-Python\"><a href=\"#windows下安装OpenCV-for-Python\" class=\"headerlink\" title=\"windows下安装OpenCV for Python\"></a>windows下安装OpenCV for Python</h1><ol>\n<li><p>Download Python, Numpy, OpenCV from their official sites.</p>\n</li>\n<li><p>Extract OpenCV (will be extracted to a folder opencv)</p>\n</li>\n<li><p>Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd</p>\n</li>\n<li><p>Paste it in C:\\Python27\\Lib\\site-packages</p>\n</li>\n<li><p>Open Python IDLE or terminal, and type</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&gt;&gt;&gt; import cv2</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"读取文件，进行批量创建目录\"><a href=\"#读取文件，进行批量创建目录\" class=\"headerlink\" title=\"读取文件，进行批量创建目录\"></a>读取文件，进行批量创建目录</h1><p>存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">xxx/0.jpg</div><div class=\"line\">yyy/0.jpg</div><div class=\"line\">zzz/0.jpg</div></pre></td></tr></table></figure>\n<p>创建xxx目录，yyy目录，zzz目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">import sys</div><div class=\"line\">import os</div><div class=\"line\">import numpy as np</div><div class=\"line\">import skimage</div><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">def read_file(path):</div><div class=\"line\">    with open(path) as f:</div><div class=\"line\">        return list(f)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def make_dir(image_path):   </div><div class=\"line\">    image_lines = read_file(image_path)</div><div class=\"line\">    if not image_lines:</div><div class=\"line\">        print &apos;empty file&apos;</div><div class=\"line\">        return</div><div class=\"line\">    i = 0</div><div class=\"line\">    for image_line in image_lines:</div><div class=\"line\">        image_line = image_line.strip(&apos;\\n&apos;)</div><div class=\"line\">        subdir_name = image_line.split(&apos;/&apos;)[0]</div><div class=\"line\">        print subdir_name</div><div class=\"line\">        isExists=os.path.exists(subdir_name)</div><div class=\"line\">        if not isExists:</div><div class=\"line\">            os.mkdir(subdir_name)</div><div class=\"line\">            print subdir_name+&quot;created successfully!&quot;</div><div class=\"line\">       </div><div class=\"line\">        i = i+1</div><div class=\"line\">        sys.stdout.write(&apos;\\riter %d\\n&apos; %(i))</div><div class=\"line\">        sys.stdout.flush()</div><div class=\"line\"></div><div class=\"line\">    print &apos;Done&apos;</div><div class=\"line\"></div><div class=\"line\">if __name__==&apos;__main__&apos;: </div><div class=\"line\">    image_path=&apos;./image.txt&apos;</div><div class=\"line\">    make_dir(image_path)</div></pre></td></tr></table></figure>\n"},{"title":"杂知识点","date":"2017-06-01T01:42:05.000Z","mathjax":true,"_content":"\n# 分类与回归\n\n**本节部分转载于穆文发表于知乎的[分类与回归区别是什么](https://www.zhihu.com/question/21329754/answer/151216012)下面的回答，获得原作者授权**\n\n*分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。*\n\n1. Logistic Regression&Linear Regression:\n\n   + Linear Regression:输出一个标量**wx+b**，这个值是连续值，用以回归问题\n   + Logistic Regression:将上面的**wx+b**通过**sigmoid**函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题\n   + 对于N分类问题，可以先计算N组w值不同的**wx+b** ，然后归一化，比如**softmax**函数变成N个类上的概率，用以多分类\n\n2. SVR &SVM\n\n   + SVR:输出**wx+b**，即某个样本点到分类面的距离，是连续值，属于回归问题\n   + SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类\n\n3. Naive Bayes用来分类和回归\n\n4. 前馈神经网络（CNN系列）用于分类和回归\n\n   + 回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的三个月抽根烟截图看做向量v，现全部连接到一个神经元上，这个神经元的输出**wv+b**，是一个连续值，处理回归问题，和Linear Regression的思想一样\n   + 分类：将m个神经元最后连接到N个神经元，有N组不同的**wv+b**，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题\n\n5. 循环神经网络（RNN系列）用于分类和回归\n\n   + 回归和分类与CNN类似，输出层的值**y=wx+b**，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列\n\n6. Logistic回归&SVM\n\n    两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。[^1]\n\n    线性模型的表达式为\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\n$$\n\n​\t将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。\n\n​\t用n表示Feature数量,m表示训练集个数。下面分情况讨论[^2]：\n\n- n很大，m很小\n  n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择`线性kernel的SVM`，也可以选择`Logistic回归`。\n- n很小，m中等 \n  n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择`非线性kernel的SVM`，比如`高斯核kernel的SVM`。\n- n很小，m很大\n  n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，`带核函数的SVM`计算也非常慢。所以此时应该选`线性kernel的SVM`，也可以选择`Logistic回归`。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。\n\n\n\n\n\n\n\n# 一些概念\n\n## 迁移学习[^3]：\n\n有监督预训练(*supervised pre-training*)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。\n\n+ **NMS(非极大值抑制)：**\n\n在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列[算法](http://lib.csdn.net/base/datastructure)中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。\n\n定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。\n(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;\n(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。\n(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。\n就这样一直重复，找到所有被保留下来的矩形框。\n\n+ **IoU(交并比)：**\n\n物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们[算法](http://lib.csdn.net/base/datastructure)不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式\n$$\nIoU=\\frac{A\\cap B}{A\\cup B}\n$$\n\n## 准确率&精确率&召回率:\n\n​\t_准确率是正确预测的样本占总的预测样本比例_\n​\t*精确率是预测为正的样本中有多少是真的正类*\n​\t*召回率是样本中有多少正例被正确的预测*\n​\t_F值=准确率\\*召回率\\*2/(准确率+召回率)，是准确率和召回率的调和平均值_\n\n​*TP*：正类被预测为正类\n​*FN*：正类被预测为负类\n​*FP*：负类被预测为正类\n​*TN*：负类被预测为负类\n\n$$\n准确率=\\frac{TP+TN}{TP+TF+FN+FP}\n$$\n\n$$\n精确率=\\frac{TP}{TP+FP}\n$$\n\n$$\n召回率=\\frac{TP}{TP+FN}\n$$\n\n## 卷积计算后的图片尺寸：\n\n$$\noutputsize=\\frac{imagesize+2*padding-kernelsize}{stride}\n$$\n\n## RankBoost:\n\n​\tRankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类\t器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1>r2>r3>r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题\n\n\n\n# 参考文献\n\n[^1]: [SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n[^2]: [SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n[^3]: [[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n\n[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n\n[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n","source":"_posts/杂知识点.md","raw":"---\ntitle: 杂知识点\ndate: 2017-06-01 09:42:05\ncategories: 深度学习\ntags: [深度学习,神经网络]\nmathjax: true\n---\n\n# 分类与回归\n\n**本节部分转载于穆文发表于知乎的[分类与回归区别是什么](https://www.zhihu.com/question/21329754/answer/151216012)下面的回答，获得原作者授权**\n\n*分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。*\n\n1. Logistic Regression&Linear Regression:\n\n   + Linear Regression:输出一个标量**wx+b**，这个值是连续值，用以回归问题\n   + Logistic Regression:将上面的**wx+b**通过**sigmoid**函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题\n   + 对于N分类问题，可以先计算N组w值不同的**wx+b** ，然后归一化，比如**softmax**函数变成N个类上的概率，用以多分类\n\n2. SVR &SVM\n\n   + SVR:输出**wx+b**，即某个样本点到分类面的距离，是连续值，属于回归问题\n   + SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类\n\n3. Naive Bayes用来分类和回归\n\n4. 前馈神经网络（CNN系列）用于分类和回归\n\n   + 回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的三个月抽根烟截图看做向量v，现全部连接到一个神经元上，这个神经元的输出**wv+b**，是一个连续值，处理回归问题，和Linear Regression的思想一样\n   + 分类：将m个神经元最后连接到N个神经元，有N组不同的**wv+b**，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题\n\n5. 循环神经网络（RNN系列）用于分类和回归\n\n   + 回归和分类与CNN类似，输出层的值**y=wx+b**，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列\n\n6. Logistic回归&SVM\n\n    两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。[^1]\n\n    线性模型的表达式为\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\n$$\n\n​\t将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。\n\n​\t用n表示Feature数量,m表示训练集个数。下面分情况讨论[^2]：\n\n- n很大，m很小\n  n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择`线性kernel的SVM`，也可以选择`Logistic回归`。\n- n很小，m中等 \n  n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择`非线性kernel的SVM`，比如`高斯核kernel的SVM`。\n- n很小，m很大\n  n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，`带核函数的SVM`计算也非常慢。所以此时应该选`线性kernel的SVM`，也可以选择`Logistic回归`。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。\n\n\n\n\n\n\n\n# 一些概念\n\n## 迁移学习[^3]：\n\n有监督预训练(*supervised pre-training*)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。\n\n+ **NMS(非极大值抑制)：**\n\n在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列[算法](http://lib.csdn.net/base/datastructure)中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。\n\n定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。\n(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;\n(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。\n(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。\n就这样一直重复，找到所有被保留下来的矩形框。\n\n+ **IoU(交并比)：**\n\n物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们[算法](http://lib.csdn.net/base/datastructure)不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式\n$$\nIoU=\\frac{A\\cap B}{A\\cup B}\n$$\n\n## 准确率&精确率&召回率:\n\n​\t_准确率是正确预测的样本占总的预测样本比例_\n​\t*精确率是预测为正的样本中有多少是真的正类*\n​\t*召回率是样本中有多少正例被正确的预测*\n​\t_F值=准确率\\*召回率\\*2/(准确率+召回率)，是准确率和召回率的调和平均值_\n\n​*TP*：正类被预测为正类\n​*FN*：正类被预测为负类\n​*FP*：负类被预测为正类\n​*TN*：负类被预测为负类\n\n$$\n准确率=\\frac{TP+TN}{TP+TF+FN+FP}\n$$\n\n$$\n精确率=\\frac{TP}{TP+FP}\n$$\n\n$$\n召回率=\\frac{TP}{TP+FN}\n$$\n\n## 卷积计算后的图片尺寸：\n\n$$\noutputsize=\\frac{imagesize+2*padding-kernelsize}{stride}\n$$\n\n## RankBoost:\n\n​\tRankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类\t器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1>r2>r3>r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题\n\n\n\n# 参考文献\n\n[^1]: [SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n[^2]: [SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n[^3]: [[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n\n[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n\n[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n","slug":"杂知识点","published":1,"updated":"2017-06-08T01:58:50.356Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj4gesbgr000dk4c8yabsi07o","content":"<h1 id=\"分类与回归\"><a href=\"#分类与回归\" class=\"headerlink\" title=\"分类与回归\"></a>分类与回归</h1><p><strong>本节部分转载于穆文发表于知乎的<a href=\"https://www.zhihu.com/question/21329754/answer/151216012\" target=\"_blank\" rel=\"external\">分类与回归区别是什么</a>下面的回答，获得原作者授权</strong></p>\n<p><em>分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。</em></p>\n<ol>\n<li><p>Logistic Regression&amp;Linear Regression:</p>\n<ul>\n<li>Linear Regression:输出一个标量<strong>wx+b</strong>，这个值是连续值，用以回归问题</li>\n<li>Logistic Regression:将上面的<strong>wx+b</strong>通过<strong>sigmoid</strong>函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题</li>\n<li>对于N分类问题，可以先计算N组w值不同的<strong>wx+b</strong> ，然后归一化，比如<strong>softmax</strong>函数变成N个类上的概率，用以多分类</li>\n</ul>\n</li>\n<li><p>SVR &amp;SVM</p>\n<ul>\n<li>SVR:输出<strong>wx+b</strong>，即某个样本点到分类面的距离，是连续值，属于回归问题</li>\n<li>SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类</li>\n</ul>\n</li>\n<li><p>Naive Bayes用来分类和回归</p>\n</li>\n<li><p>前馈神经网络（CNN系列）用于分类和回归</p>\n<ul>\n<li>回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的三个月抽根烟截图看做向量v，现全部连接到一个神经元上，这个神经元的输出<strong>wv+b</strong>，是一个连续值，处理回归问题，和Linear Regression的思想一样</li>\n<li>分类：将m个神经元最后连接到N个神经元，有N组不同的<strong>wv+b</strong>，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题</li>\n</ul>\n</li>\n<li><p>循环神经网络（RNN系列）用于分类和回归</p>\n<ul>\n<li>回归和分类与CNN类似，输出层的值<strong>y=wx+b</strong>，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列</li>\n</ul>\n</li>\n<li><p>Logistic回归&amp;SVM</p>\n<p> 两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。<a href=\"[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\">^1</a></p>\n<p> 线性模型的表达式为</p>\n</li>\n</ol>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n<br>$$</p>\n<p>​    将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。</p>\n<p>​    用n表示Feature数量,m表示训练集个数。下面分情况讨论<a href=\"[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\">^2</a>：</p>\n<ul>\n<li>n很大，m很小<br>n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。</li>\n<li>n很小，m中等<br>n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择<code>非线性kernel的SVM</code>，比如<code>高斯核kernel的SVM</code>。</li>\n<li>n很小，m很大<br>n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，<code>带核函数的SVM</code>计算也非常慢。所以此时应该选<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。</li>\n</ul>\n<h1 id=\"一些概念\"><a href=\"#一些概念\" class=\"headerlink\" title=\"一些概念\"></a>一些概念</h1><h2 id=\"迁移学习-3：\"><a href=\"#迁移学习-3：\" class=\"headerlink\" title=\"迁移学习^3：\"></a>迁移学习<a href=\"[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\">^3</a>：</h2><p>有监督预训练(<em>supervised pre-training</em>)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。</p>\n<ul>\n<li><strong>NMS(非极大值抑制)：</strong></li>\n</ul>\n<p>在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。</p>\n<p>定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。<br>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;<br>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。<br>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。<br>就这样一直重复，找到所有被保留下来的矩形框。</p>\n<ul>\n<li><strong>IoU(交并比)：</strong></li>\n</ul>\n<p>物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式<br>$$<br>IoU=\\frac{A\\cap B}{A\\cup B}<br>$$</p>\n<h2 id=\"准确率-amp-精确率-amp-召回率\"><a href=\"#准确率-amp-精确率-amp-召回率\" class=\"headerlink\" title=\"准确率&amp;精确率&amp;召回率:\"></a>准确率&amp;精确率&amp;召回率:</h2><p>​    <em>准确率是正确预测的样本占总的预测样本比例</em><br>​    <em>精确率是预测为正的样本中有多少是真的正类</em><br>​    <em>召回率是样本中有多少正例被正确的预测</em><br>​    <em>F值=准确率*召回率*2/(准确率+召回率)，是准确率和召回率的调和平均值</em></p>\n<p>​<em>TP</em>：正类被预测为正类<br>​<em>FN</em>：正类被预测为负类<br>​<em>FP</em>：负类被预测为正类<br>​<em>TN</em>：负类被预测为负类</p>\n<p>$$<br>准确率=\\frac{TP+TN}{TP+TF+FN+FP}<br>$$</p>\n<p>$$<br>精确率=\\frac{TP}{TP+FP}<br>$$</p>\n<p>$$<br>召回率=\\frac{TP}{TP+FN}<br>$$</p>\n<h2 id=\"卷积计算后的图片尺寸：\"><a href=\"#卷积计算后的图片尺寸：\" class=\"headerlink\" title=\"卷积计算后的图片尺寸：\"></a>卷积计算后的图片尺寸：</h2><p>$$<br>outputsize=\\frac{imagesize+2*padding-kernelsize}{stride}<br>$$</p>\n<h2 id=\"RankBoost\"><a href=\"#RankBoost\" class=\"headerlink\" title=\"RankBoost:\"></a>RankBoost:</h2><p>​    RankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类    器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1&gt;r2&gt;r3&gt;r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p><a href=\"https://www.zhihu.com/question/21704547/answer/20293255\" target=\"_blank\" rel=\"external\">SVM和logistic回归分别在什么情况下使用</a></p>\n<p><a href=\"http://blog.csdn.net/ybdesire/article/details/54143481\" target=\"_blank\" rel=\"external\">SVM和Logistic的区别</a></p>\n<p>[<a href=\"http://blog.csdn.net/zhang_shuai12/article/details/52716952\" target=\"_blank\" rel=\"external\">物体检测中常用的几个概念迁移学习、IOU、NMS理解</a>]</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"分类与回归\"><a href=\"#分类与回归\" class=\"headerlink\" title=\"分类与回归\"></a>分类与回归</h1><p><strong>本节部分转载于穆文发表于知乎的<a href=\"https://www.zhihu.com/question/21329754/answer/151216012\" target=\"_blank\" rel=\"external\">分类与回归区别是什么</a>下面的回答，获得原作者授权</strong></p>\n<p><em>分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。</em></p>\n<ol>\n<li><p>Logistic Regression&amp;Linear Regression:</p>\n<ul>\n<li>Linear Regression:输出一个标量<strong>wx+b</strong>，这个值是连续值，用以回归问题</li>\n<li>Logistic Regression:将上面的<strong>wx+b</strong>通过<strong>sigmoid</strong>函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题</li>\n<li>对于N分类问题，可以先计算N组w值不同的<strong>wx+b</strong> ，然后归一化，比如<strong>softmax</strong>函数变成N个类上的概率，用以多分类</li>\n</ul>\n</li>\n<li><p>SVR &amp;SVM</p>\n<ul>\n<li>SVR:输出<strong>wx+b</strong>，即某个样本点到分类面的距离，是连续值，属于回归问题</li>\n<li>SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类</li>\n</ul>\n</li>\n<li><p>Naive Bayes用来分类和回归</p>\n</li>\n<li><p>前馈神经网络（CNN系列）用于分类和回归</p>\n<ul>\n<li>回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的三个月抽根烟截图看做向量v，现全部连接到一个神经元上，这个神经元的输出<strong>wv+b</strong>，是一个连续值，处理回归问题，和Linear Regression的思想一样</li>\n<li>分类：将m个神经元最后连接到N个神经元，有N组不同的<strong>wv+b</strong>，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题</li>\n</ul>\n</li>\n<li><p>循环神经网络（RNN系列）用于分类和回归</p>\n<ul>\n<li>回归和分类与CNN类似，输出层的值<strong>y=wx+b</strong>，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列</li>\n</ul>\n</li>\n<li><p>Logistic回归&amp;SVM</p>\n<p> 两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。<a href=\"[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\">^1</a></p>\n<p> 线性模型的表达式为</p>\n</li>\n</ol>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n<br>$$</p>\n<p>​    将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。</p>\n<p>​    用n表示Feature数量,m表示训练集个数。下面分情况讨论<a href=\"[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\">^2</a>：</p>\n<ul>\n<li>n很大，m很小<br>n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。</li>\n<li>n很小，m中等<br>n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择<code>非线性kernel的SVM</code>，比如<code>高斯核kernel的SVM</code>。</li>\n<li>n很小，m很大<br>n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，<code>带核函数的SVM</code>计算也非常慢。所以此时应该选<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。</li>\n</ul>\n<h1 id=\"一些概念\"><a href=\"#一些概念\" class=\"headerlink\" title=\"一些概念\"></a>一些概念</h1><h2 id=\"迁移学习-3：\"><a href=\"#迁移学习-3：\" class=\"headerlink\" title=\"迁移学习^3：\"></a>迁移学习<a href=\"[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\">^3</a>：</h2><p>有监督预训练(<em>supervised pre-training</em>)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。</p>\n<ul>\n<li><strong>NMS(非极大值抑制)：</strong></li>\n</ul>\n<p>在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。</p>\n<p>定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。<br>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;<br>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。<br>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。<br>就这样一直重复，找到所有被保留下来的矩形框。</p>\n<ul>\n<li><strong>IoU(交并比)：</strong></li>\n</ul>\n<p>物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式<br>$$<br>IoU=\\frac{A\\cap B}{A\\cup B}<br>$$</p>\n<h2 id=\"准确率-amp-精确率-amp-召回率\"><a href=\"#准确率-amp-精确率-amp-召回率\" class=\"headerlink\" title=\"准确率&amp;精确率&amp;召回率:\"></a>准确率&amp;精确率&amp;召回率:</h2><p>​    <em>准确率是正确预测的样本占总的预测样本比例</em><br>​    <em>精确率是预测为正的样本中有多少是真的正类</em><br>​    <em>召回率是样本中有多少正例被正确的预测</em><br>​    <em>F值=准确率*召回率*2/(准确率+召回率)，是准确率和召回率的调和平均值</em></p>\n<p>​<em>TP</em>：正类被预测为正类<br>​<em>FN</em>：正类被预测为负类<br>​<em>FP</em>：负类被预测为正类<br>​<em>TN</em>：负类被预测为负类</p>\n<p>$$<br>准确率=\\frac{TP+TN}{TP+TF+FN+FP}<br>$$</p>\n<p>$$<br>精确率=\\frac{TP}{TP+FP}<br>$$</p>\n<p>$$<br>召回率=\\frac{TP}{TP+FN}<br>$$</p>\n<h2 id=\"卷积计算后的图片尺寸：\"><a href=\"#卷积计算后的图片尺寸：\" class=\"headerlink\" title=\"卷积计算后的图片尺寸：\"></a>卷积计算后的图片尺寸：</h2><p>$$<br>outputsize=\\frac{imagesize+2*padding-kernelsize}{stride}<br>$$</p>\n<h2 id=\"RankBoost\"><a href=\"#RankBoost\" class=\"headerlink\" title=\"RankBoost:\"></a>RankBoost:</h2><p>​    RankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类    器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1&gt;r2&gt;r3&gt;r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p><a href=\"https://www.zhihu.com/question/21704547/answer/20293255\" target=\"_blank\" rel=\"external\">SVM和logistic回归分别在什么情况下使用</a></p>\n<p><a href=\"http://blog.csdn.net/ybdesire/article/details/54143481\" target=\"_blank\" rel=\"external\">SVM和Logistic的区别</a></p>\n<p>[<a href=\"http://blog.csdn.net/zhang_shuai12/article/details/52716952\" target=\"_blank\" rel=\"external\">物体检测中常用的几个概念迁移学习、IOU、NMS理解</a>]</p>\n"},{"title":"Triplet loss","date":"2017-06-02T12:26:35.000Z","description":null,"mathjax":true,"_content":"\n# 原理\n\nTriplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。\n\n\n\n\n\n![Triplet Loss 示意图](http://img.blog.csdn.net/20160727090101355)\n\n\n\n### Triplet loss中的margin取值分析\n\n我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。\n\n```\n当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。\n\n当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。\n\n因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。\n```\n\n\n\n## 相关\n\n区分相似图形，除了triplet loss，还有一篇CVPR：[《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》](http://blog.csdn.net/u010167269/article/details/51783446)提出的Coupled Cluster Loss.\n\n\n\n**本文参考**：\n\n[triplet loss 原理以及梯度推导](http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html)\n\n[如何在Caffe中增加layer以及Caffe中triplet loss layer的实现](http://www.voidcn.com/blog/mao_kun/article/p-6246924.html)\n\n\n\n\n\n\n\n\n\n","source":"_posts/Triplet loss.md","raw":"---\ntitle: Triplet loss\ndate: 2017-06-02 20:26:35\ncategories: Face\ntags: [深度学习，人脸识别]\ndescription:\nmathjax: true\n---\n\n# 原理\n\nTriplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。\n\n\n\n\n\n![Triplet Loss 示意图](http://img.blog.csdn.net/20160727090101355)\n\n\n\n### Triplet loss中的margin取值分析\n\n我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。\n\n```\n当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。\n\n当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。\n\n因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。\n```\n\n\n\n## 相关\n\n区分相似图形，除了triplet loss，还有一篇CVPR：[《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》](http://blog.csdn.net/u010167269/article/details/51783446)提出的Coupled Cluster Loss.\n\n\n\n**本文参考**：\n\n[triplet loss 原理以及梯度推导](http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html)\n\n[如何在Caffe中增加layer以及Caffe中triplet loss layer的实现](http://www.voidcn.com/blog/mao_kun/article/p-6246924.html)\n\n\n\n\n\n\n\n\n\n","slug":"Triplet loss","published":1,"updated":"2017-06-02T13:09:42.119Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj4gesbgy000fk4c8bsis9m1z","content":"<h1 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h1><p>Triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。</p>\n<p><img src=\"http://img.blog.csdn.net/20160727090101355\" alt=\"Triplet Loss 示意图\"></p>\n<h3 id=\"Triplet-loss中的margin取值分析\"><a href=\"#Triplet-loss中的margin取值分析\" class=\"headerlink\" title=\"Triplet loss中的margin取值分析\"></a>Triplet loss中的margin取值分析</h3><p>我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。</div><div class=\"line\"></div><div class=\"line\">当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。</div><div class=\"line\"></div><div class=\"line\">因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。</div></pre></td></tr></table></figure>\n<h2 id=\"相关\"><a href=\"#相关\" class=\"headerlink\" title=\"相关\"></a>相关</h2><p>区分相似图形，除了triplet loss，还有一篇CVPR：<a href=\"http://blog.csdn.net/u010167269/article/details/51783446\" target=\"_blank\" rel=\"external\">《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》</a>提出的Coupled Cluster Loss.</p>\n<p><strong>本文参考</strong>：</p>\n<p><a href=\"http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html\" target=\"_blank\" rel=\"external\">triplet loss 原理以及梯度推导</a></p>\n<p><a href=\"http://www.voidcn.com/blog/mao_kun/article/p-6246924.html\" target=\"_blank\" rel=\"external\">如何在Caffe中增加layer以及Caffe中triplet loss layer的实现</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h1><p>Triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。</p>\n<p><img src=\"http://img.blog.csdn.net/20160727090101355\" alt=\"Triplet Loss 示意图\"></p>\n<h3 id=\"Triplet-loss中的margin取值分析\"><a href=\"#Triplet-loss中的margin取值分析\" class=\"headerlink\" title=\"Triplet loss中的margin取值分析\"></a>Triplet loss中的margin取值分析</h3><p>我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。</div><div class=\"line\"></div><div class=\"line\">当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。</div><div class=\"line\"></div><div class=\"line\">因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。</div></pre></td></tr></table></figure>\n<h2 id=\"相关\"><a href=\"#相关\" class=\"headerlink\" title=\"相关\"></a>相关</h2><p>区分相似图形，除了triplet loss，还有一篇CVPR：<a href=\"http://blog.csdn.net/u010167269/article/details/51783446\" target=\"_blank\" rel=\"external\">《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》</a>提出的Coupled Cluster Loss.</p>\n<p><strong>本文参考</strong>：</p>\n<p><a href=\"http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html\" target=\"_blank\" rel=\"external\">triplet loss 原理以及梯度推导</a></p>\n<p><a href=\"http://www.voidcn.com/blog/mao_kun/article/p-6246924.html\" target=\"_blank\" rel=\"external\">如何在Caffe中增加layer以及Caffe中triplet loss layer的实现</a></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cj4gesbeh0001k4c8vzn3gc70","category_id":"cj4gesbfc0004k4c899mwvqo9","_id":"cj4gesbgx000ek4c8sslfb53v"},{"post_id":"cj4gesbgr000dk4c8yabsi07o","category_id":"cj4gesbgm000ak4c83vmou1fy","_id":"cj4gesbh6000ik4c8g4ambr5q"},{"post_id":"cj4gesbf10003k4c89hqu88il","category_id":"cj4gesbgm000ak4c83vmou1fy","_id":"cj4gesbh9000kk4c8y6pqx9f2"},{"post_id":"cj4gesbgy000fk4c8bsis9m1z","category_id":"cj4gesbfc0004k4c899mwvqo9","_id":"cj4gesbha000mk4c8h557vdbm"},{"post_id":"cj4gesbg50008k4c8nupvetgb","category_id":"cj4gesbh1000gk4c8h7unjq99","_id":"cj4gesbhc000ok4c854i5dgdu"}],"PostTag":[{"post_id":"cj4gesbeh0001k4c8vzn3gc70","tag_id":"cj4gesbfp0005k4c8xnfpgmcn","_id":"cj4gesbgp000ck4c8i3sqpvt1"},{"post_id":"cj4gesbf10003k4c89hqu88il","tag_id":"cj4gesbgn000bk4c8u7zgpbdv","_id":"cj4gesbha000lk4c8q1jj65k7"},{"post_id":"cj4gesbf10003k4c89hqu88il","tag_id":"cj4gesbh2000hk4c87fav53fg","_id":"cj4gesbhb000nk4c8er4262h8"},{"post_id":"cj4gesbg50008k4c8nupvetgb","tag_id":"cj4gesbh7000jk4c86grbzkq3","_id":"cj4gesbhg000qk4c827q0kizs"},{"post_id":"cj4gesbgj0009k4c8uops6yoy","tag_id":"cj4gesbhe000pk4c83r5zyex4","_id":"cj4gesbhk000sk4c8i6l11lcs"},{"post_id":"cj4gesbgr000dk4c8yabsi07o","tag_id":"cj4gesbhj000rk4c8hh3fdycc","_id":"cj4gesbhu000vk4c8mheptlkm"},{"post_id":"cj4gesbgr000dk4c8yabsi07o","tag_id":"cj4gesbhm000tk4c8mqanhtaq","_id":"cj4gesbhw000wk4c8mubz4hkg"},{"post_id":"cj4gesbgy000fk4c8bsis9m1z","tag_id":"cj4gesbht000uk4c8ilmgn99z","_id":"cj4gesbhz000xk4c8aytrrb2x"}],"Tag":[{"name":"笔记，人脸识别","_id":"cj4gesbfp0005k4c8xnfpgmcn"},{"name":"caffe","_id":"cj4gesbgn000bk4c8u7zgpbdv"},{"name":"调参","_id":"cj4gesbh2000hk4c87fav53fg"},{"name":"Hexo","_id":"cj4gesbh7000jk4c86grbzkq3"},{"name":"Python","_id":"cj4gesbhe000pk4c83r5zyex4"},{"name":"深度学习","_id":"cj4gesbhj000rk4c8hh3fdycc"},{"name":"神经网络","_id":"cj4gesbhm000tk4c8mqanhtaq"},{"name":"深度学习，人脸识别","_id":"cj4gesbht000uk4c8ilmgn99z"}]}}