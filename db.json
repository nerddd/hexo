{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/4]M8LF]VF14SLOBE)L2KZFE.png","path":"images/4]M8LF]VF14SLOBE)L2KZFE.png","modified":0,"renderable":0},{"_id":"source/images/QQ图片20170724143700.png","path":"images/QQ图片20170724143700.png","modified":0,"renderable":0},{"_id":"source/images/Q83KC{3A7Y}PVX~YU`383RN.png","path":"images/Q83KC{3A7Y}PVX~YU`383RN.png","modified":0,"renderable":0},{"_id":"source/images/QQ图片20181202121223.png","path":"images/QQ图片20181202121223.png","modified":0,"renderable":0},{"_id":"source/images/Q`4VLL2A~D`}00V$`([L4T6.png","path":"images/Q`4VLL2A~D`}00V$`([L4T6.png","modified":0,"renderable":0},{"_id":"source/images/images.jpg","path":"images/images.jpg","modified":0,"renderable":0},{"_id":"source/images/154392682.jpg","path":"images/154392682.jpg","modified":0,"renderable":0},{"_id":"source/images/1543927335.jpg","path":"images/1543927335.jpg","modified":0,"renderable":0},{"_id":"source/images/QQ图片20181202121159.png","path":"images/QQ图片20181202121159.png","modified":0,"renderable":0},{"_id":"source/images/唐小蔓.png","path":"images/唐小蔓.png","modified":0,"renderable":0},{"_id":"source/images/v2-affecbef53e66d2554440778fa8fdefb_hd.jpg","path":"images/v2-affecbef53e66d2554440778fa8fdefb_hd.jpg","modified":0,"renderable":0},{"_id":"source/images/QQ图片20181202120825.png","path":"images/QQ图片20181202120825.png","modified":0,"renderable":0},{"_id":"source/images/QQ图片20181202125948.png","path":"images/QQ图片20181202125948.png","modified":0,"renderable":0},{"_id":"source/images/QQ图片20181204204421.png","path":"images/QQ图片20181204204421.png","modified":0,"renderable":0},{"_id":"source/images/QQ图片20181202130028.png","path":"images/QQ图片20181202130028.png","modified":0,"renderable":0},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"source/images/facenet_triplet1.png","path":"images/facenet_triplet1.png","modified":0,"renderable":0},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":1},{"_id":"themes/next/source/images/images.jpg","path":"images/images.jpg","modified":0,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":0,"renderable":1},{"_id":"source/images/QQ图片20181204203113.png","path":"images/QQ图片20181204203113.png","modified":0,"renderable":0},{"_id":"source/images/QQ图片20181202120956.png","path":"images/QQ图片20181202120956.png","modified":0,"renderable":0},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1494683576000},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1494683576000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1494683576000},{"_id":"themes/next/.gitignore","hash":"32ea93f21d8693d5d8fa4eef1c51a21ad0670047","modified":1494683576000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1494683576000},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1494683576000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1494683576000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1494683576000},{"_id":"themes/next/.travis.yml","hash":"c42d9608c8c7fe90de7b1581a8dc3886e90c179e","modified":1494683576000},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1494683576000},{"_id":"themes/next/README.en.md","hash":"4ece25ee5f64447cd522e54cb0fffd9a375f0bd4","modified":1494683576000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1494683576000},{"_id":"themes/next/_config.yml","hash":"ae78493d14cfa4d9c5ba6f284070df1bcfc54c21","modified":1544000781446},{"_id":"themes/next/bower.json","hash":"be0a430362cb73a7e3cf9ecf51a67edf8214b637","modified":1494683576000},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1494683576000},{"_id":"themes/next/package.json","hash":"7e87b2621104b39a30488654c2a8a0c6a563574b","modified":1494683576000},{"_id":"source/_posts/Caffe调参.md","hash":"1dd11d02b112b53b3d8782f32c7a7e5d55474144","modified":1544001587823},{"_id":"source/_posts/Center-loss笔记.md","hash":"3d3a8243c805d1f2ff5fe35349b254ce3efe25df","modified":1544002154096},{"_id":"source/_posts/DeepID3论文笔记.md","hash":"0e9f9714f22b4ca3f5d4326e04df16f039d02146","modified":1544001635318},{"_id":"source/_posts/FaceNet论文笔记.md","hash":"15574dd66d282357df03519e736fd8f5d25a9244","modified":1544002149723},{"_id":"source/_posts/Hexo相关.md","hash":"ee06a856b3c02f2bcd7517a36e6647cd876605fe","modified":1544001667498},{"_id":"source/_posts/Python.md","hash":"cc85c171f6e5649d0073361d9c7a719f838e5fc9","modified":1544001701670},{"_id":"source/_posts/Triplet loss.md","hash":"a93f1ddafccb730f983299a70ed71a90f8b53e42","modified":1544002158170},{"_id":"source/_posts/人脸识别回顾.md","hash":"56c11137e09f002ec725ff1589072a7e532f7793","modified":1544001862556},{"_id":"source/_posts/待办及进度.md","hash":"5bb2e601f20affd696a44dba883f888e4df0a7e7","modified":1544001828239},{"_id":"source/_posts/手册&指南.md","hash":"cedb1c9fd8fb603eee4261947581d14775979d8f","modified":1544001916952},{"_id":"source/_posts/杂知识点.md","hash":"53bc28ae8e8299155ef3398ba36a9715165aeee9","modified":1544001947318},{"_id":"source/_posts/每日阅读.md","hash":"7d94989481068083955976d4b12afbcb3bb7b9e3","modified":1544000645513},{"_id":"source/about/index.md","hash":"b1c0ea16dab27b36c6961bc037d7cfeff409978e","modified":1496233446892},{"_id":"source/categories/index.md","hash":"4041694b9457789fceb11df6a2576543a2e18495","modified":1496233317600},{"_id":"source/_posts/年龄估计.md","hash":"357879ebf39505f7a426572bbdb2fb4ef73fd6ae","modified":1544000313030},{"_id":"source/images/4]M8LF]VF14SLOBE)L2KZFE.png","hash":"501e9168bb7bd53b7e0583bbc563a0667bb8fa2c","modified":1500878036824},{"_id":"source/images/QQ图片20170724143700.png","hash":"1d2b883010480b3cff8226fefffc4b3635d9bd01","modified":1500723203705},{"_id":"source/images/Q83KC{3A7Y}PVX~YU`383RN.png","hash":"d0c1523dd9f54920454c3d3edfeb6f8496652e86","modified":1543725953847},{"_id":"source/images/QQ图片20181202121223.png","hash":"9daf4efb4e70b6d6363d695bb88e9daf5324e143","modified":1543723942098},{"_id":"source/images/Q`4VLL2A~D`}00V$`([L4T6.png","hash":"9094446ffe38a5b71219615c605ae59323b7f5d0","modified":1500104208619},{"_id":"source/images/images.jpg","hash":"a83aa0596ff8d7ab87cfc47d4a57dd01d6c4ccee","modified":1496232703694},{"_id":"source/tags/index.md","hash":"27a029fbb025ae4aa8e5e3f65ab9a299f6f58c55","modified":1496233234309},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1494683576000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"fdd63b77472612337309eb93ec415a059b90756b","modified":1494683576000},{"_id":"themes/next/languages/de.yml","hash":"306db8c865630f32c6b6260ade9d3209fbec8011","modified":1494683576000},{"_id":"themes/next/languages/default.yml","hash":"4cc6aeb1ac09a58330e494c8771773758ab354af","modified":1494683576000},{"_id":"themes/next/languages/en.yml","hash":"e7def07a709ef55684490b700a06998c67f35f39","modified":1494683576000},{"_id":"themes/next/languages/fr-FR.yml","hash":"24180322c83587a153cea110e74e96eacc3355ad","modified":1494683576000},{"_id":"themes/next/languages/id.yml","hash":"2835ea80dadf093fcf47edd957680973f1fb6b85","modified":1494683576000},{"_id":"themes/next/languages/ja.yml","hash":"1c3a05ab80a6f8be63268b66da6f19da7aa2c638","modified":1494683576000},{"_id":"themes/next/languages/ko.yml","hash":"be150543379150f78329815af427bf152c0e9431","modified":1494683576000},{"_id":"themes/next/languages/pt-BR.yml","hash":"958e49571818a34fdf4af3232a07a024050f8f4e","modified":1494683576000},{"_id":"themes/next/languages/pt.yml","hash":"36c8f60dacbe5d27d84d0e0d6974d7679f928da0","modified":1494683576000},{"_id":"themes/next/languages/ru.yml","hash":"1549a7c2fe23caa7cbedcd0aa2b77c46e57caf27","modified":1494683576000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3c0c7dfd0256457ee24df9e9879226c58cb084b5","modified":1494683576000},{"_id":"themes/next/languages/zh-hk.yml","hash":"1c917997413bf566cb79e0975789f3c9c9128ccd","modified":1494683576000},{"_id":"themes/next/languages/zh-tw.yml","hash":"0b2c18aa76570364003c8d1cd429fa158ae89022","modified":1494683576000},{"_id":"themes/next/layout/_layout.swig","hash":"9d1a23a6add6f3d0f88c2d17979956f14aaa37a4","modified":1494683576000},{"_id":"themes/next/layout/archive.swig","hash":"5de4dca06b05d99e4f6bad617a4b8f4f3592fb01","modified":1494683576000},{"_id":"themes/next/layout/category.swig","hash":"82e7bc278559b4335ad974659104eaaf04863032","modified":1494683576000},{"_id":"themes/next/layout/index.swig","hash":"03e8a2cda03bad42ac0cb827025eb81f95d496a2","modified":1494683576000},{"_id":"themes/next/layout/page.swig","hash":"2c6a78999133b991d9221f484aee2eacae894251","modified":1494683576000},{"_id":"themes/next/layout/post.swig","hash":"2d5f8d7f0a96b611e2d5a5e4d111fc17726a990f","modified":1494683576000},{"_id":"themes/next/layout/schedule.swig","hash":"f93c53f6fd5c712584f6efba6f770c30fa8a3e80","modified":1494683576000},{"_id":"themes/next/layout/tag.swig","hash":"2e73ee478e981092ea9a5d10dd472a9461db395b","modified":1494683576000},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1494683576000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1494683576000},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1494683576000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1494683576000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1494683576000},{"_id":"source/images/154392682.jpg","hash":"13f6dd9075eeff7ecaa90b288d2e4fadfc85d058","modified":1543926824570},{"_id":"source/images/1543927335.jpg","hash":"fa9a292bf25931438e5e9b912c4803c9341bdd49","modified":1543927335378},{"_id":"source/images/QQ图片20181202121159.png","hash":"184c96876b2730869d81bea5e6542cdab5bde1f9","modified":1543723916904},{"_id":"source/images/唐小蔓.png","hash":"09b378d1237988b9575046e3b31446323aea5437","modified":1500723316448},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"source/_posts/pictures/4]M8LF]VF14SLOBE)L2KZFE.png","hash":"501e9168bb7bd53b7e0583bbc563a0667bb8fa2c","modified":1500878036824},{"_id":"source/_posts/pictures/Q83KC{3A7Y}PVX~YU`383RN.png","hash":"d0c1523dd9f54920454c3d3edfeb6f8496652e86","modified":1543725953847},{"_id":"source/_posts/pictures/QQ图片20170724143700.png","hash":"1d2b883010480b3cff8226fefffc4b3635d9bd01","modified":1500723203705},{"_id":"source/_posts/pictures/QQ图片20181202121223.png","hash":"9daf4efb4e70b6d6363d695bb88e9daf5324e143","modified":1543723942098},{"_id":"source/images/v2-affecbef53e66d2554440778fa8fdefb_hd.jpg","hash":"edcf7b255240fa242743ee01054f4d968c59e6b7","modified":1543926772095},{"_id":"source/_posts/pictures/Q`4VLL2A~D`}00V$`([L4T6.png","hash":"9094446ffe38a5b71219615c605ae59323b7f5d0","modified":1500104208619},{"_id":"source/_posts/pictures/v2-affecbef53e66d2554440778fa8fdefb_hd.jpg","hash":"edcf7b255240fa242743ee01054f4d968c59e6b7","modified":1543926772095},{"_id":"source/_posts/pictures/唐小蔓.png","hash":"09b378d1237988b9575046e3b31446323aea5437","modified":1500723316448},{"_id":"source/images/QQ图片20181202120825.png","hash":"fa417f9e266bdff95d8ca38fdee2646c4b669e59","modified":1543723703270},{"_id":"source/images/QQ图片20181202125948.png","hash":"2509eea11c138a43152b0c0da3178796cb71ec9e","modified":1543726766545},{"_id":"source/images/QQ图片20181204204421.png","hash":"2484b5c880bb07e7e1b875ac5e50e8d3ed5f197a","modified":1543927457286},{"_id":"source/images/QQ图片20181202130028.png","hash":"2509eea11c138a43152b0c0da3178796cb71ec9e","modified":1543726766545},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1494683576000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1494683576000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1494683576000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"b16fcbf0efd20c018d7545257a8533c497ea7647","modified":1494683576000},{"_id":"themes/next/layout/_macro/post.swig","hash":"74ea1a468f1f215480896f111f64d9f087e04261","modified":1543996348434},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1494683576000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"911b99ba0445b2c07373128d87a4ef2eb7de341a","modified":1494683576000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1494683576000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"1c7d3c975e499b9aa3119d6724b030b7b00fc87e","modified":1494683576000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1494683576000},{"_id":"themes/next/layout/_partials/head.swig","hash":"d4a05c51aac02f1f6248baccf2ddb8ee12b9122f","modified":1494683576000},{"_id":"themes/next/layout/_partials/header.swig","hash":"a1ffbb691dfad3eaf2832a11766e58a179003b8b","modified":1494683576000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1494683576000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1494683576000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1494683576000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1494683576000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1494683576000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9baf90f7c40b3b10f288e9268c3191e895890cea","modified":1494683576000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1494683576000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1494683576000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1494683576000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1494683576000},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1494683576000},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1494683576000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1494683576000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1494683576000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1494683576000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1494683576000},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1494683576000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1494683576000},{"_id":"source/images/facenet_triplet1.png","hash":"a298c92ba352c4ba11c840b55f72d6c11956d93d","modified":1498654743054},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1494683576000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1494683576000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1494683576000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1494683576000},{"_id":"themes/next/source/images/favicon.ico","hash":"3354f46359b13ee21fede9a2f64a81875daae6a5","modified":1496235727302},{"_id":"themes/next/source/images/images.jpg","hash":"a83aa0596ff8d7ab87cfc47d4a57dd01d6c4ccee","modified":1496232703694},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1494683576000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1494683576000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1494683576000},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1494683576000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1494683576000},{"_id":"source/_posts/pictures/154392682.jpg","hash":"13f6dd9075eeff7ecaa90b288d2e4fadfc85d058","modified":1543926824570},{"_id":"source/_posts/pictures/1543927335.jpg","hash":"fa9a292bf25931438e5e9b912c4803c9341bdd49","modified":1543927335378},{"_id":"source/_posts/pictures/QQ图片20181202121159.png","hash":"184c96876b2730869d81bea5e6542cdab5bde1f9","modified":1543723916904},{"_id":"source/_posts/pictures/facenet_triplet1.png","hash":"a298c92ba352c4ba11c840b55f72d6c11956d93d","modified":1498654743054},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"source/images/QQ图片20181204203113.png","hash":"f80e5d6e3528f33424aab4d48f1ca8090ebf3fa6","modified":1543926671712},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"source/_posts/pictures/QQ图片20181202120825.png","hash":"fa417f9e266bdff95d8ca38fdee2646c4b669e59","modified":1543723703270},{"_id":"source/_posts/pictures/QQ图片20181202125948.png","hash":"2509eea11c138a43152b0c0da3178796cb71ec9e","modified":1543726766545},{"_id":"source/images/QQ图片20181202120956.png","hash":"d85d7a23a447e94761611a500cbc442424752a36","modified":1543723793922},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1494683576000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1494683576000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1494683576000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1494683576000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1494683576000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1494683576000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"3358d11b9a26185a2d36c96049e4340e701646e4","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"abb92620197a16ed2c0775edf18a0f044a82256e","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"1d0d01aaeb7bcde3671263d736718f8837c20182","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"af9dd8a4aed7d06cf47b363eebff48850888566c","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"1f349aa30dd1f7022f7d07a1f085eea5ace3f26d","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1494683576000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1494683576000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1494683576000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1494683576000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"06f432f328a5b8a9ef0dbd5301b002aba600b4ce","modified":1494683576000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"d6a793bcada68d4b6c58392546bc48a482e4a7d3","modified":1494683576000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1494683576000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1494683576000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1494683576000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1494683576000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1494683576000},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1494683576000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1494683576000},{"_id":"themes/next/source/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1494683576000},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1494683576000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1494683576000},{"_id":"themes/next/source/js/src/utils.js","hash":"803f684fa7d0e729115a48851023a31f6fb6d0a7","modified":1494683576000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1494683576000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1494683576000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"3587602ad777b031628bb5944864d1a4fcfea4ac","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1494683576000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1494683576000},{"_id":"source/_posts/pictures/QQ图片20181202130028.png","hash":"2509eea11c138a43152b0c0da3178796cb71ec9e","modified":1543726766545},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1494683576000},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1494683576000},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1494683576000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1494683576000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1494683576000},{"_id":"source/_posts/pictures/QQ图片20181202120956.png","hash":"d85d7a23a447e94761611a500cbc442424752a36","modified":1543723793922},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"59ad08bcc6fe9793594869ac2b4c525021453e78","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"ef089a407c90e58eca10c49bc47ec978f96e03ba","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1494683576000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"7804e31c44717c9a9ddf0f8482b9b9c1a0f74538","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1494683576000},{"_id":"source/_posts/pictures/QQ图片20181204204421.png","hash":"2484b5c880bb07e7e1b875ac5e50e8d3ed5f197a","modified":1543927457286},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"2053bb030c530cc3981e4c322705f8263195a6d9","modified":1544001443185},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e3e23751d4ad24e8714b425d768cf68e37de7ded","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1494683576000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"d9c0b3dc9158e717fde36f554709e6c3a22b5f85","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1494683576000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1494683576000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1494683576000},{"_id":"source/_posts/pictures/QQ图片20181204203113.png","hash":"f80e5d6e3528f33424aab4d48f1ca8090ebf3fa6","modified":1543926671712},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"38e48f275ad00daa9dcdcb8d9b44e576acda4707","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"740d37f428b8f4574a76fc95cc25e50e0565f45e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"dbc07ec641a537df5918b41ce40a6466712a44f6","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"88c7d75646b66b168213190ee4cd874609afd5e3","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"c089419916988d0f51d89b225460fe11b631e0a3","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8c0276883398651336853d5ec0e9da267a00dd86","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"5f6ea57aabfa30a437059bf8352f1ad829dbd4ff","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a2ec22ef4a6817bbb2abe8660fcd99fe4ca0cc5e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"dd310c2d999185e881db007360176ee2f811df10","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"bb3be8374c31c372ed0995bd8030d2b920d581de","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1494683576000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1494683576000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1494683576000},{"_id":"public/search.xml","hash":"b7264518e6329f211cf3a460bcad2735864ff719","modified":1544002175099},{"_id":"public/atom.xml","hash":"f9e0c7a782d9743662051531a735c69ff6f5982e","modified":1544002175077},{"_id":"public/sitemap.xml","hash":"932401d653d8c50ada9011ca89dfff988dc29b3f","modified":1544002175101},{"_id":"public/about/index.html","hash":"fac9d3f44791c78c0c08c842863b103ff2113f81","modified":1544001970202},{"_id":"public/2018/04/27/手册&指南/index.html","hash":"3c8cb9f60d32975f093821c933db029c7ff847df","modified":1544001970203},{"_id":"public/2018/01/29/人脸识别回顾/index.html","hash":"a8c8b48b2e9661e93e8fc388e971bf1e81a13075","modified":1544001970203},{"_id":"public/2017/07/04/DeepID3论文笔记/index.html","hash":"47ce90f10e467cbac413c84ee32d4382035b6c2b","modified":1544001970204},{"_id":"public/2017/06/29/Center-loss笔记/index.html","hash":"92577afd5f101ca32b1b6e59be4ef4f9267a20b7","modified":1544002175167},{"_id":"public/2017/06/28/FaceNet论文笔记/index.html","hash":"fcc9b857e14a56a9367ca5c88eb77c2a5a0fe75b","modified":1544002175168},{"_id":"public/2017/06/17/Caffe调参/index.html","hash":"17d7ada75d9bf061fa4fa483618894cc836df32f","modified":1544001970204},{"_id":"public/2017/06/09/Python/index.html","hash":"bb039377a6f29dfdb49e1e4e9c5bb3ce6709342b","modified":1544001970204},{"_id":"public/2017/06/02/Triplet loss/index.html","hash":"87c265c1c461a99324a70164d4939b39adc6b2a0","modified":1544002175168},{"_id":"public/2017/06/02/Hexo相关/index.html","hash":"9dc3a1ff24154b116e2647a95662e4995b6e3614","modified":1544001970204},{"_id":"public/2017/06/02/待办及进度/index.html","hash":"a4f17c3de7f5e2cedbbfc75481bead3d0c2c263c","modified":1544001970205},{"_id":"public/2017/06/01/杂知识点/index.html","hash":"dd59176487f425c7f78f88ff53276689d97a0481","modified":1544001970205},{"_id":"public/archives/index.html","hash":"0105a21fcebae6017128d37953f4bfc4af2c885f","modified":1544001970205},{"_id":"public/archives/page/2/index.html","hash":"d32df82f63f0942a54ea4ad4cf516f39628ad3c6","modified":1544001970205},{"_id":"public/archives/2017/index.html","hash":"e06e3fb4a19316e0c1ec71b619cebed6caef6664","modified":1544001970205},{"_id":"public/archives/2017/06/index.html","hash":"a788315ba4cfe5bdd46158ce4d6fc44089a8553e","modified":1544001970205},{"_id":"public/archives/2017/07/index.html","hash":"370387ca1862bebd0777460ad77552e7cfdbe10e","modified":1544001970206},{"_id":"public/archives/2018/index.html","hash":"6a70295c497b7b32dbd6b266c00bb3d2e238117d","modified":1544001970206},{"_id":"public/archives/2018/01/index.html","hash":"e3106e15723fc1ffb3140361824a56d332030892","modified":1544001970206},{"_id":"public/archives/2018/04/index.html","hash":"1c3325fd864fb1dacbf2415a93439894bea07bea","modified":1544001970206},{"_id":"public/2018/12/05/每日阅读/index.html","hash":"3df6d151524edcafa8c6011f7b8bd8624af37916","modified":1544001970203},{"_id":"public/2018/12/05/年龄估计/index.html","hash":"ee81efb4fc7356bef13b2f1f630d8441b9e2e2ef","modified":1544001970203},{"_id":"public/archives/2018/12/index.html","hash":"fc92111d21ab9455901795681812268b7aa1fa02","modified":1544001970206},{"_id":"public/categories/index.html","hash":"97c7cdca1123e76807aa20ccdfdb6b59c658c05a","modified":1544002175167},{"_id":"public/categories/深度学习/index.html","hash":"84f87e90891267c2b452852d67200e2c5c255d25","modified":1544001970205},{"_id":"public/categories/Face/index.html","hash":"10e3f13a66555993256b53b779ad738374870e7d","modified":1544000964200},{"_id":"public/categories/论文笔记/index.html","hash":"012b42a1e637d07f7f560dee2004ab0e3541feb7","modified":1544002175168},{"_id":"public/categories/Hexo/index.html","hash":"c3eaefd1e340a9b3abe6041b94aae97ba1a83715","modified":1544001970205},{"_id":"public/categories/技能/index.html","hash":"adcacdade14c17aeb8cc9e3f073e50669c79f3f5","modified":1544001970205},{"_id":"public/index.html","hash":"6e48047e64b6bfe897f09907368b9450ede5f8d8","modified":1544002175168},{"_id":"public/page/2/index.html","hash":"7f50e922c8914cdc9b702443430476c8fb9b923b","modified":1544001970206},{"_id":"public/tags/index.html","hash":"244cd3cbcba32f952094537461eab001f4897fef","modified":1544001970203},{"_id":"public/tags/调参/index.html","hash":"f81b13862fb9986c2d462dff59419b9b686c6b6a","modified":1544001970206},{"_id":"public/tags/深度学习，论文笔记/index.html","hash":"d12473034fe73d9385b5a655ca1bf16f94a946ef","modified":1544001970206},{"_id":"public/tags/caffe/index.html","hash":"16cf2c000731fda1f4fda7a57d19d31408abf448","modified":1544001970206},{"_id":"public/tags/Face/index.html","hash":"ebda9b976371e7a7d88eabd3f629f00cb7bc3d3f","modified":1544001970207},{"_id":"public/tags/笔记/index.html","hash":"635bae44dde36d9ea1eb16f77b7cc6f70aa21f8a","modified":1544001970207},{"_id":"public/tags/笔记，人脸识别/index.html","hash":"cc675f26c7732d5c0e04d06846738789684e1f05","modified":1544001970207},{"_id":"public/tags/Hexo/index.html","hash":"eff88f74366cfd03fdc9482a61fb1ac2c30cf6b1","modified":1544001970207},{"_id":"public/tags/Python/index.html","hash":"119695a1f76a2a61c93e93a22a12ca606d2a9ec0","modified":1544001970207},{"_id":"public/tags/深度学习，人脸识别/index.html","hash":"657bf5bdebfe6f77cbe455732c775d29ff9b4700","modified":1544001970207},{"_id":"public/tags/人脸识别/index.html","hash":"fd59464d88faf346eb6032bba613d1f577c2b98f","modified":1544001970207},{"_id":"public/tags/工具/index.html","hash":"1c8efd248aca4fa64440140186b9e271cb26b2e7","modified":1544001970207},{"_id":"public/tags/深度学习/index.html","hash":"7ca618d37f1eb3705ff7cc7cad747f24ff091aff","modified":1544001970208},{"_id":"public/tags/神经网络/index.html","hash":"02c47f6ec6b6b6aeb9a0350889f4c989382945e8","modified":1544001970208},{"_id":"public/tags/杂/index.html","hash":"cf6f1ba7eff3fd75a25a8591e1e196814113ac22","modified":1544001970208},{"_id":"public/tags/年龄估计/index.html","hash":"c444528a0a358199dd98c9282472528d52444660","modified":1544001970208},{"_id":"public/images/4]M8LF]VF14SLOBE)L2KZFE.png","hash":"501e9168bb7bd53b7e0583bbc563a0667bb8fa2c","modified":1544000078521},{"_id":"public/images/Q83KC{3A7Y}PVX~YU`383RN.png","hash":"d0c1523dd9f54920454c3d3edfeb6f8496652e86","modified":1544000078521},{"_id":"public/images/QQ图片20170724143700.png","hash":"1d2b883010480b3cff8226fefffc4b3635d9bd01","modified":1544000078521},{"_id":"public/images/QQ图片20181202121223.png","hash":"9daf4efb4e70b6d6363d695bb88e9daf5324e143","modified":1544000078522},{"_id":"public/images/images.jpg","hash":"a83aa0596ff8d7ab87cfc47d4a57dd01d6c4ccee","modified":1544000078522},{"_id":"public/images/Q`4VLL2A~D`}00V$`([L4T6.png","hash":"9094446ffe38a5b71219615c605ae59323b7f5d0","modified":1544000078522},{"_id":"public/images/唐小蔓.png","hash":"09b378d1237988b9575046e3b31446323aea5437","modified":1544000078522},{"_id":"public/images/v2-affecbef53e66d2554440778fa8fdefb_hd.jpg","hash":"edcf7b255240fa242743ee01054f4d968c59e6b7","modified":1544000078522},{"_id":"public/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1544000078522},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1544000078523},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1544000078523},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1544000078523},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1544000078523},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1544000078523},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1544000078523},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1544000078523},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1544000078523},{"_id":"public/images/favicon.ico","hash":"3354f46359b13ee21fede9a2f64a81875daae6a5","modified":1544000078523},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1544000078524},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1544000078524},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1544000078524},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1544000078524},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1544000078524},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1544000078524},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1544000078524},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1544000078524},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1544000078524},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1544000078525},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1544000078525},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1544000078525},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1544000078525},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1544000078525},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1544000078525},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1544000078525},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1544000078525},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1544000078525},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1544000078525},{"_id":"public/images/QQ图片20181202121159.png","hash":"184c96876b2730869d81bea5e6542cdab5bde1f9","modified":1544000080859},{"_id":"public/images/154392682.jpg","hash":"13f6dd9075eeff7ecaa90b288d2e4fadfc85d058","modified":1544000080861},{"_id":"public/images/facenet_triplet1.png","hash":"a298c92ba352c4ba11c840b55f72d6c11956d93d","modified":1544000080899},{"_id":"public/images/1543927335.jpg","hash":"fa9a292bf25931438e5e9b912c4803c9341bdd49","modified":1544000080899},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1544000080900},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1544000080900},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1544000080933},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1544000080934},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1544000080934},{"_id":"public/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1544000080934},{"_id":"public/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1544000080934},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1544000080935},{"_id":"public/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1544000080935},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1544000080935},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1544000080935},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1544000080936},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1544000080936},{"_id":"public/js/src/utils.js","hash":"803f684fa7d0e729115a48851023a31f6fb6d0a7","modified":1544000080936},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"3587602ad777b031628bb5944864d1a4fcfea4ac","modified":1544000080936},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1544000080936},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1544000080937},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1544000080937},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1544000080937},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1544000080937},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1544000080937},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1544000080938},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1544000080938},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1544000080938},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1544000080939},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1544000080939},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1544000080939},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1544000080939},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1544000080939},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1544000080940},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1544000080940},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1544000080940},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1544000080940},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1544000080940},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1544000080941},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1544000080941},{"_id":"public/css/main.css","hash":"325595097ffc1b93274adf8d81b376ace5ee1c80","modified":1544000080941},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1544000080941},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1544000080941},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1544000080942},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1544000080942},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1544000080942},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1544000080942},{"_id":"public/lib/Han/dist/han.min.css","hash":"d9c0b3dc9158e717fde36f554709e6c3a22b5f85","modified":1544000080942},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1544000080943},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1544000080943},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1544000080943},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1544000080943},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1544000080943},{"_id":"public/lib/Han/dist/han.css","hash":"38e48f275ad00daa9dcdcb8d9b44e576acda4707","modified":1544000080944},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1544000080944},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1544000080944},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1544000080944},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1544000080945},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1544000080945},{"_id":"public/images/QQ图片20181202120825.png","hash":"fa417f9e266bdff95d8ca38fdee2646c4b669e59","modified":1544000080946},{"_id":"public/images/QQ图片20181202125948.png","hash":"2509eea11c138a43152b0c0da3178796cb71ec9e","modified":1544000080947},{"_id":"public/images/QQ图片20181204204421.png","hash":"2484b5c880bb07e7e1b875ac5e50e8d3ed5f197a","modified":1544000080948},{"_id":"public/images/QQ图片20181202130028.png","hash":"2509eea11c138a43152b0c0da3178796cb71ec9e","modified":1544000080950},{"_id":"public/images/QQ图片20181204203113.png","hash":"f80e5d6e3528f33424aab4d48f1ca8090ebf3fa6","modified":1544000080951},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1544000080952},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1544000080952},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1544000080953},{"_id":"public/images/QQ图片20181202120956.png","hash":"d85d7a23a447e94761611a500cbc442424752a36","modified":1544000081045},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1544000081073},{"_id":"public/categories/Face，论文笔记/index.html","hash":"5cb3684d90ad7d0f36823aa58911f83910f0910a","modified":1544001970229},{"_id":"public/categories/实验记录/index.html","hash":"1b520cc437a298359973ea5d269f18f76d4b60b9","modified":1544001970229}],"Category":[{"name":"Face","_id":"cjpaxs0i60004s0vw1tpoa71w"},{"name":"深度学习","_id":"cjpaxs0jp000as0vw6h6g55ue"},{"name":"论文笔记","_id":"cjpaxs0kn000gs0vw3c1c7re0"},{"name":"Hexo","_id":"cjpaxs0lk000ms0vwr7nybq8f"},{"name":"技能","_id":"cjpaxs0n2000us0vwogunn2sl"},{"name":"Face，论文笔记","_id":"cjpaywldt00006gvwtg0e95ls"},{"name":"实验记录","_id":"cjpaywler00046gvwpqy3ko6r"}],"Data":[],"Page":[{"title":"about","date":"2017-05-31T12:22:54.000Z","_content":"\n本来无一物\n\nFrom Calyp\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-05-31 20:22:54\n---\n\n本来无一物\n\nFrom Calyp\n","updated":"2017-05-31T12:24:06.892Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjpaxs0hf0001s0vw9vw4k37i","content":"<p>本来无一物</p>\n<p>From Calyp</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本来无一物</p>\n<p>From Calyp</p>\n"},{"title":"categories","date":"2017-05-31T12:21:40.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-05-31 20:21:40\ntype: \"categories\"\n---\n","updated":"2017-05-31T12:21:57.600Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjpaxs0i20003s0vwt3fw4u8s","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2017-05-31T12:19:32.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2017-05-31 20:19:32\ntype: \"tags\"\n---\n","updated":"2017-05-31T12:20:34.309Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjpaxs0is0007s0vwo0se5nyz","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Center loss笔记","date":"2017-06-29T09:05:51.000Z","description":"Center loss论文阅读笔记","mathjax":null,"_content":"\n论文：[A Discriminative Feature Learning Approach for Deep Face Recognition](https://link.springer.com/chapter/10.1007%2F978-3-319-46478-7_31)\n\n# 摘要\n\n对于一般的CNN网络，softmax通常作为监督信号来训练深层网络，为了增强提取的特征的可辨别性（discriminative），提出center loss，应对人脸识别任务。center loss同时学习每个类别的深层特征中心和惩罚深层特征和它们对应的类中心的距离（the center loss simultaneously learns a center for deep\nfeatures of each class and penalizes the distances between the deep features and their corresponding class centers）。将softmax和center loss联合起来，可以训练一个健壮的CNN来获取深层特征的两个关键目标，类内紧凑和类间分散。\n\n# 介绍\n\n预先收集所有可能的测试身份用于训练是不现实的，所以CNN的标签预测并不总是适用的。经过深层网络获取得到的特征不仅需要可分开性(separable)，更需要识别性(discriminative)和广义性，足够用来识别新的没有遇见过的类。可识别性的特征可以利用最邻近(NN)或k-NN算法很好的分类，就没有必要依赖标签预测了。但是softmax只能产生可分开(separable)特征，结果特征就不足以用以人脸识别。\n\n![Separabale Feature Vs Discriminative Feature](https://static.leiphone.com/uploads/new/article/740_740/201612/585bb8742235c.png?imageMogr2/format/jpg/quality/90)\n\n\n\n因为随机梯度下降(SGD)优化CNN是基于mini-batch，不能很好的反映深层特征的全局分布，由于训练集的庞大，在每次迭代中输入所有的训练样本是不现实的。constractive loss和triplet loss分别作为图像对和三元组的loss函数。然而，与图像样本相比，训练图像对或三元组的数量显著增长，导致收敛缓慢和不稳定性。仔细选择图像对或者三元组，问题可能会部分缓解，但是它增加了计算复杂度，训练过程变得不方便。为了解决这个问题，提出center loss，用于有效的增强特征的可识别性，我们将会得到每个类的深层特征的中心。在训练阶段，我们同时更新中心和最小化特征和它们相应的类中心的距离。CNN同时在softmax loss和center loss的监督下进行训练，通过一个超参来平衡这两个监督信号。直觉上，softmax loss将不同类别的特征分开，center loss有效的将同一类别的特征拉向类的中心，使得类内特征分布变得紧凑。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/Center-loss笔记.md","raw":"---\ntitle: Center loss笔记\ndate: 2017-06-29 17:05:51\ncategories: 论文笔记\ntags: [深度学习，论文笔记]\ndescription: Center loss论文阅读笔记\nmathjax:\n---\n\n论文：[A Discriminative Feature Learning Approach for Deep Face Recognition](https://link.springer.com/chapter/10.1007%2F978-3-319-46478-7_31)\n\n# 摘要\n\n对于一般的CNN网络，softmax通常作为监督信号来训练深层网络，为了增强提取的特征的可辨别性（discriminative），提出center loss，应对人脸识别任务。center loss同时学习每个类别的深层特征中心和惩罚深层特征和它们对应的类中心的距离（the center loss simultaneously learns a center for deep\nfeatures of each class and penalizes the distances between the deep features and their corresponding class centers）。将softmax和center loss联合起来，可以训练一个健壮的CNN来获取深层特征的两个关键目标，类内紧凑和类间分散。\n\n# 介绍\n\n预先收集所有可能的测试身份用于训练是不现实的，所以CNN的标签预测并不总是适用的。经过深层网络获取得到的特征不仅需要可分开性(separable)，更需要识别性(discriminative)和广义性，足够用来识别新的没有遇见过的类。可识别性的特征可以利用最邻近(NN)或k-NN算法很好的分类，就没有必要依赖标签预测了。但是softmax只能产生可分开(separable)特征，结果特征就不足以用以人脸识别。\n\n![Separabale Feature Vs Discriminative Feature](https://static.leiphone.com/uploads/new/article/740_740/201612/585bb8742235c.png?imageMogr2/format/jpg/quality/90)\n\n\n\n因为随机梯度下降(SGD)优化CNN是基于mini-batch，不能很好的反映深层特征的全局分布，由于训练集的庞大，在每次迭代中输入所有的训练样本是不现实的。constractive loss和triplet loss分别作为图像对和三元组的loss函数。然而，与图像样本相比，训练图像对或三元组的数量显著增长，导致收敛缓慢和不稳定性。仔细选择图像对或者三元组，问题可能会部分缓解，但是它增加了计算复杂度，训练过程变得不方便。为了解决这个问题，提出center loss，用于有效的增强特征的可识别性，我们将会得到每个类的深层特征的中心。在训练阶段，我们同时更新中心和最小化特征和它们相应的类中心的距离。CNN同时在softmax loss和center loss的监督下进行训练，通过一个超参来平衡这两个监督信号。直觉上，softmax loss将不同类别的特征分开，center loss有效的将同一类别的特征拉向类的中心，使得类内特征分布变得紧凑。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"Center-loss笔记","published":1,"updated":"2018-12-05T09:29:14.096Z","_id":"cjpaxs0gg0000s0vwhkok5jdm","comments":1,"layout":"post","photos":[],"link":"","content":"<p>论文：<a href=\"https://link.springer.com/chapter/10.1007%2F978-3-319-46478-7_31\" target=\"_blank\" rel=\"external\">A Discriminative Feature Learning Approach for Deep Face Recognition</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>对于一般的CNN网络，softmax通常作为监督信号来训练深层网络，为了增强提取的特征的可辨别性（discriminative），提出center loss，应对人脸识别任务。center loss同时学习每个类别的深层特征中心和惩罚深层特征和它们对应的类中心的距离（the center loss simultaneously learns a center for deep<br>features of each class and penalizes the distances between the deep features and their corresponding class centers）。将softmax和center loss联合起来，可以训练一个健壮的CNN来获取深层特征的两个关键目标，类内紧凑和类间分散。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>预先收集所有可能的测试身份用于训练是不现实的，所以CNN的标签预测并不总是适用的。经过深层网络获取得到的特征不仅需要可分开性(separable)，更需要识别性(discriminative)和广义性，足够用来识别新的没有遇见过的类。可识别性的特征可以利用最邻近(NN)或k-NN算法很好的分类，就没有必要依赖标签预测了。但是softmax只能产生可分开(separable)特征，结果特征就不足以用以人脸识别。</p>\n<p><img src=\"https://static.leiphone.com/uploads/new/article/740_740/201612/585bb8742235c.png?imageMogr2/format/jpg/quality/90\" alt=\"Separabale Feature Vs Discriminative Feature\"></p>\n<p>因为随机梯度下降(SGD)优化CNN是基于mini-batch，不能很好的反映深层特征的全局分布，由于训练集的庞大，在每次迭代中输入所有的训练样本是不现实的。constractive loss和triplet loss分别作为图像对和三元组的loss函数。然而，与图像样本相比，训练图像对或三元组的数量显著增长，导致收敛缓慢和不稳定性。仔细选择图像对或者三元组，问题可能会部分缓解，但是它增加了计算复杂度，训练过程变得不方便。为了解决这个问题，提出center loss，用于有效的增强特征的可识别性，我们将会得到每个类的深层特征的中心。在训练阶段，我们同时更新中心和最小化特征和它们相应的类中心的距离。CNN同时在softmax loss和center loss的监督下进行训练，通过一个超参来平衡这两个监督信号。直觉上，softmax loss将不同类别的特征分开，center loss有效的将同一类别的特征拉向类的中心，使得类内特征分布变得紧凑。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文：<a href=\"https://link.springer.com/chapter/10.1007%2F978-3-319-46478-7_31\" target=\"_blank\" rel=\"external\">A Discriminative Feature Learning Approach for Deep Face Recognition</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>对于一般的CNN网络，softmax通常作为监督信号来训练深层网络，为了增强提取的特征的可辨别性（discriminative），提出center loss，应对人脸识别任务。center loss同时学习每个类别的深层特征中心和惩罚深层特征和它们对应的类中心的距离（the center loss simultaneously learns a center for deep<br>features of each class and penalizes the distances between the deep features and their corresponding class centers）。将softmax和center loss联合起来，可以训练一个健壮的CNN来获取深层特征的两个关键目标，类内紧凑和类间分散。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>预先收集所有可能的测试身份用于训练是不现实的，所以CNN的标签预测并不总是适用的。经过深层网络获取得到的特征不仅需要可分开性(separable)，更需要识别性(discriminative)和广义性，足够用来识别新的没有遇见过的类。可识别性的特征可以利用最邻近(NN)或k-NN算法很好的分类，就没有必要依赖标签预测了。但是softmax只能产生可分开(separable)特征，结果特征就不足以用以人脸识别。</p>\n<p><img src=\"https://static.leiphone.com/uploads/new/article/740_740/201612/585bb8742235c.png?imageMogr2/format/jpg/quality/90\" alt=\"Separabale Feature Vs Discriminative Feature\"></p>\n<p>因为随机梯度下降(SGD)优化CNN是基于mini-batch，不能很好的反映深层特征的全局分布，由于训练集的庞大，在每次迭代中输入所有的训练样本是不现实的。constractive loss和triplet loss分别作为图像对和三元组的loss函数。然而，与图像样本相比，训练图像对或三元组的数量显著增长，导致收敛缓慢和不稳定性。仔细选择图像对或者三元组，问题可能会部分缓解，但是它增加了计算复杂度，训练过程变得不方便。为了解决这个问题，提出center loss，用于有效的增强特征的可识别性，我们将会得到每个类的深层特征的中心。在训练阶段，我们同时更新中心和最小化特征和它们相应的类中心的距离。CNN同时在softmax loss和center loss的监督下进行训练，通过一个超参来平衡这两个监督信号。直觉上，softmax loss将不同类别的特征分开，center loss有效的将同一类别的特征拉向类的中心，使得类内特征分布变得紧凑。</p>\n"},{"title":"Caffe调参","date":"2017-06-17T01:26:12.000Z","description":"caffe调参的一些经验","mathjax":null,"_content":"\n# loss为nan\n\n**梯度爆炸**\n\n**原因**：梯度变得非常大，使得学习过程难以继续\n\n**现象：**观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。\n\n**措施**： \n\n1. 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 \n2. 设置clip gradient，用于限制过大的diff\n\n## \n\n**不当的损失函数**\n\n**原因**：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。\n\n**现象**：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。\n\n**措施**：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。\n\n示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。\n\n## \n\n**不当的输入**\n\n**原因**：输入中就含有NaN。\n\n**现象**：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。\n\n**措施**：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。\n\n**案例**：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。\n\n**池化层中步长比核的尺寸大**\n\n如下例所示，当池化层中stride > kernel的时候会在y中产生NaN\n\n```\n    layer {\n      name: \"faulty_pooling\"\n      type: \"Pooling\"\n      bottom: \"x\"\n      top: \"y\"\n      pooling_param {\n      pool: AVE\n      stride: 5\n      kernel: 3\n      }\n    }\n```\n\n**致谢**\n\n*http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training*\n\n\n\n# Accuracy一直为0\n\n考虑标签是否从0开始递增\n\n","source":"_posts/Caffe调参.md","raw":"---\ntitle: Caffe调参\ndate: 2017-06-17 09:26:12\ncategories: 深度学习\ntags: [caffe,调参]\ndescription: caffe调参的一些经验\nmathjax:\n---\n\n# loss为nan\n\n**梯度爆炸**\n\n**原因**：梯度变得非常大，使得学习过程难以继续\n\n**现象：**观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。\n\n**措施**： \n\n1. 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 \n2. 设置clip gradient，用于限制过大的diff\n\n## \n\n**不当的损失函数**\n\n**原因**：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。\n\n**现象**：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。\n\n**措施**：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。\n\n示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。\n\n## \n\n**不当的输入**\n\n**原因**：输入中就含有NaN。\n\n**现象**：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。\n\n**措施**：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。\n\n**案例**：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。\n\n**池化层中步长比核的尺寸大**\n\n如下例所示，当池化层中stride > kernel的时候会在y中产生NaN\n\n```\n    layer {\n      name: \"faulty_pooling\"\n      type: \"Pooling\"\n      bottom: \"x\"\n      top: \"y\"\n      pooling_param {\n      pool: AVE\n      stride: 5\n      kernel: 3\n      }\n    }\n```\n\n**致谢**\n\n*http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training*\n\n\n\n# Accuracy一直为0\n\n考虑标签是否从0开始递增\n\n","slug":"Caffe调参","published":1,"updated":"2018-12-05T09:19:47.823Z","_id":"cjpaxs0hu0002s0vw03tnnzqa","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"loss为nan\"><a href=\"#loss为nan\" class=\"headerlink\" title=\"loss为nan\"></a>loss为nan</h1><p><strong>梯度爆炸</strong></p>\n<p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续</p>\n<p><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。</p>\n<p><strong>措施</strong>： </p>\n<ol>\n<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>\n<li>设置clip gradient，用于限制过大的diff</li>\n</ol>\n<p>## </p>\n<p><strong>不当的损失函数</strong></p>\n<p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p>\n<p><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>\n<p><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。</p>\n<p>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>\n<p>## </p>\n<p><strong>不当的输入</strong></p>\n<p><strong>原因</strong>：输入中就含有NaN。</p>\n<p><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>\n<p><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。</p>\n<p><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>\n<p><strong>池化层中步长比核的尺寸大</strong></p>\n<p>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">  name: &quot;faulty_pooling&quot;</div><div class=\"line\">  type: &quot;Pooling&quot;</div><div class=\"line\">  bottom: &quot;x&quot;</div><div class=\"line\">  top: &quot;y&quot;</div><div class=\"line\">  pooling_param &#123;</div><div class=\"line\">  pool: AVE</div><div class=\"line\">  stride: 5</div><div class=\"line\">  kernel: 3</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p><strong>致谢</strong></p>\n<p><em><a href=\"http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training</a></em></p>\n<h1 id=\"Accuracy一直为0\"><a href=\"#Accuracy一直为0\" class=\"headerlink\" title=\"Accuracy一直为0\"></a>Accuracy一直为0</h1><p>考虑标签是否从0开始递增</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"loss为nan\"><a href=\"#loss为nan\" class=\"headerlink\" title=\"loss为nan\"></a>loss为nan</h1><p><strong>梯度爆炸</strong></p>\n<p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续</p>\n<p><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。</p>\n<p><strong>措施</strong>： </p>\n<ol>\n<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>\n<li>设置clip gradient，用于限制过大的diff</li>\n</ol>\n<p>## </p>\n<p><strong>不当的损失函数</strong></p>\n<p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p>\n<p><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>\n<p><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。</p>\n<p>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>\n<p>## </p>\n<p><strong>不当的输入</strong></p>\n<p><strong>原因</strong>：输入中就含有NaN。</p>\n<p><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>\n<p><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。</p>\n<p><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>\n<p><strong>池化层中步长比核的尺寸大</strong></p>\n<p>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">  name: &quot;faulty_pooling&quot;</div><div class=\"line\">  type: &quot;Pooling&quot;</div><div class=\"line\">  bottom: &quot;x&quot;</div><div class=\"line\">  top: &quot;y&quot;</div><div class=\"line\">  pooling_param &#123;</div><div class=\"line\">  pool: AVE</div><div class=\"line\">  stride: 5</div><div class=\"line\">  kernel: 3</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p><strong>致谢</strong></p>\n<p><em><a href=\"http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training</a></em></p>\n<h1 id=\"Accuracy一直为0\"><a href=\"#Accuracy一直为0\" class=\"headerlink\" title=\"Accuracy一直为0\"></a>Accuracy一直为0</h1><p>考虑标签是否从0开始递增</p>\n"},{"title":"DeepID3论文笔记","date":"2017-07-04T02:34:31.000Z","description":"DeepID3论文阅读笔记","mathjax":null,"_content":"\n# 摘要\n\n深层的网络由于良好的学习能力，近年来在目标识别领域取得巨大成功，基于此，提出两个非常深的网络架构，DeepID3，这两个架构分别基于VGG net和GoogLeNet通过改进使之用于人脸识别。Joint face identification-verification supervisory signals are added to both intermediate and\nfinal feature extraction layers during training.        \n\n\n\n","source":"_posts/DeepID3论文笔记.md","raw":"---\ntitle: DeepID3论文笔记\ndate: 2017-07-04 10:34:31\ncategories: 论文笔记\ntags: [Face,笔记]\ndescription: DeepID3论文阅读笔记\nmathjax:\n---\n\n# 摘要\n\n深层的网络由于良好的学习能力，近年来在目标识别领域取得巨大成功，基于此，提出两个非常深的网络架构，DeepID3，这两个架构分别基于VGG net和GoogLeNet通过改进使之用于人脸识别。Joint face identification-verification supervisory signals are added to both intermediate and\nfinal feature extraction layers during training.        \n\n\n\n","slug":"DeepID3论文笔记","published":1,"updated":"2018-12-05T09:20:35.318Z","_id":"cjpaxs0ii0006s0vwibpejvrb","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>深层的网络由于良好的学习能力，近年来在目标识别领域取得巨大成功，基于此，提出两个非常深的网络架构，DeepID3，这两个架构分别基于VGG net和GoogLeNet通过改进使之用于人脸识别。Joint face identification-verification supervisory signals are added to both intermediate and<br>final feature extraction layers during training.        </p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>深层的网络由于良好的学习能力，近年来在目标识别领域取得巨大成功，基于此，提出两个非常深的网络架构，DeepID3，这两个架构分别基于VGG net和GoogLeNet通过改进使之用于人脸识别。Joint face identification-verification supervisory signals are added to both intermediate and<br>final feature extraction layers during training.        </p>\n"},{"title":"FaceNet论文笔记","date":"2017-06-28T02:11:42.000Z","description":"FaceNet论文阅读笔记","mathjax":null,"_content":"\n*原文链接*:[FaceNet:A unified embedding for face recognition and clustering](https://arxiv.org/abs/1503.03832)\n\n## 简介\n\nFaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。\n\n## 前言\n\nFaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。\n\n当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。选取的triplets包含两个匹配脸部缩略图（为紧密裁剪的脸部区域，不用使用2d、3d对齐以及放大转换等预处理）和一个非匹配的脸部缩略图，loss函数目标是通过距离边界区分正负类。\n\n本文中，探索了两类深度卷积神经网络，第一类为Zeiler&Fergus研究中使用的神经网络，包含多个交错的卷积层、非线性激励函数，局部相应归一化和最大池化层。我们额外的添加了一些1x1xd的卷积层。第二种结构是基于Inception model，这种网络利用了一些不同的卷积层和池化层并行和级联响应。我们发现这些模型可以减小20倍以上的参数数量，并且可能会减少FLOPS数量。\n\ntriplet loss的启发是传统loss函数趋向于将有一类特征的人脸图像映射到同一个空间，而triplet loss尝试将一个个体的人脸图像和其他人脸图像分开。\n\n![facenet_triplet1](/images/facenet_triplet1.png)\n\n\n\n\n\n## 总结\n\n- 三元组的目标函数并不是这篇论文首创，我在之前的一些Hash索引的论文中也见过相似的应用。可见，并不是所有的学习特征的模型都必须用softmax。用其他的效果也会好。\n- 三元组比softmax的优势在于\n  - softmax不直接，（三元组直接优化距离），因而性能也不好。\n  - softmax产生的特征表示向量都很大，一般超过1000维。\n- FaceNet并没有像DeepFace和DeepID那样需要对齐。\n- FaceNet得到最终表示后不用像DeepID那样需要再训练模型进行分类，直接计算距离就好了，简单而有效。\n- 论文并未探讨二元对的有效性，直接使用的三元对。\n\n\n\n## 参考文献\n\n[谷歌人脸识别系统FaceNet解析](https://zhuanlan.zhihu.com/p/24837264)\n\n[FaceNet--Google的人脸识别](http://blog.csdn.net/stdcoutzyx/article/details/46687471)\n\n","source":"_posts/FaceNet论文笔记.md","raw":"---\ntitle: FaceNet论文笔记\ndate: 2017-06-28 10:11:42\ncategories: 论文笔记\ntags: [笔记，人脸识别]\ndescription: FaceNet论文阅读笔记\nmathjax:\n---\n\n*原文链接*:[FaceNet:A unified embedding for face recognition and clustering](https://arxiv.org/abs/1503.03832)\n\n## 简介\n\nFaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。\n\n## 前言\n\nFaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。\n\n当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。选取的triplets包含两个匹配脸部缩略图（为紧密裁剪的脸部区域，不用使用2d、3d对齐以及放大转换等预处理）和一个非匹配的脸部缩略图，loss函数目标是通过距离边界区分正负类。\n\n本文中，探索了两类深度卷积神经网络，第一类为Zeiler&Fergus研究中使用的神经网络，包含多个交错的卷积层、非线性激励函数，局部相应归一化和最大池化层。我们额外的添加了一些1x1xd的卷积层。第二种结构是基于Inception model，这种网络利用了一些不同的卷积层和池化层并行和级联响应。我们发现这些模型可以减小20倍以上的参数数量，并且可能会减少FLOPS数量。\n\ntriplet loss的启发是传统loss函数趋向于将有一类特征的人脸图像映射到同一个空间，而triplet loss尝试将一个个体的人脸图像和其他人脸图像分开。\n\n![facenet_triplet1](/images/facenet_triplet1.png)\n\n\n\n\n\n## 总结\n\n- 三元组的目标函数并不是这篇论文首创，我在之前的一些Hash索引的论文中也见过相似的应用。可见，并不是所有的学习特征的模型都必须用softmax。用其他的效果也会好。\n- 三元组比softmax的优势在于\n  - softmax不直接，（三元组直接优化距离），因而性能也不好。\n  - softmax产生的特征表示向量都很大，一般超过1000维。\n- FaceNet并没有像DeepFace和DeepID那样需要对齐。\n- FaceNet得到最终表示后不用像DeepID那样需要再训练模型进行分类，直接计算距离就好了，简单而有效。\n- 论文并未探讨二元对的有效性，直接使用的三元对。\n\n\n\n## 参考文献\n\n[谷歌人脸识别系统FaceNet解析](https://zhuanlan.zhihu.com/p/24837264)\n\n[FaceNet--Google的人脸识别](http://blog.csdn.net/stdcoutzyx/article/details/46687471)\n\n","slug":"FaceNet论文笔记","published":1,"updated":"2018-12-05T09:29:09.723Z","_id":"cjpaxs0j00008s0vw5j0obxtu","comments":1,"layout":"post","photos":[],"link":"","content":"<p><em>原文链接</em>:<a href=\"https://arxiv.org/abs/1503.03832\" target=\"_blank\" rel=\"external\">FaceNet:A unified embedding for face recognition and clustering</a></p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>FaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。</p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>FaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。</p>\n<p>当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。选取的triplets包含两个匹配脸部缩略图（为紧密裁剪的脸部区域，不用使用2d、3d对齐以及放大转换等预处理）和一个非匹配的脸部缩略图，loss函数目标是通过距离边界区分正负类。</p>\n<p>本文中，探索了两类深度卷积神经网络，第一类为Zeiler&amp;Fergus研究中使用的神经网络，包含多个交错的卷积层、非线性激励函数，局部相应归一化和最大池化层。我们额外的添加了一些1x1xd的卷积层。第二种结构是基于Inception model，这种网络利用了一些不同的卷积层和池化层并行和级联响应。我们发现这些模型可以减小20倍以上的参数数量，并且可能会减少FLOPS数量。</p>\n<p>triplet loss的启发是传统loss函数趋向于将有一类特征的人脸图像映射到同一个空间，而triplet loss尝试将一个个体的人脸图像和其他人脸图像分开。</p>\n<p><img src=\"/images/facenet_triplet1.png\" alt=\"facenet_triplet1\"></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>三元组的目标函数并不是这篇论文首创，我在之前的一些Hash索引的论文中也见过相似的应用。可见，并不是所有的学习特征的模型都必须用softmax。用其他的效果也会好。</li>\n<li>三元组比softmax的优势在于<ul>\n<li>softmax不直接，（三元组直接优化距离），因而性能也不好。</li>\n<li>softmax产生的特征表示向量都很大，一般超过1000维。</li>\n</ul>\n</li>\n<li>FaceNet并没有像DeepFace和DeepID那样需要对齐。</li>\n<li>FaceNet得到最终表示后不用像DeepID那样需要再训练模型进行分类，直接计算距离就好了，简单而有效。</li>\n<li>论文并未探讨二元对的有效性，直接使用的三元对。</li>\n</ul>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><p><a href=\"https://zhuanlan.zhihu.com/p/24837264\" target=\"_blank\" rel=\"external\">谷歌人脸识别系统FaceNet解析</a></p>\n<p><a href=\"http://blog.csdn.net/stdcoutzyx/article/details/46687471\" target=\"_blank\" rel=\"external\">FaceNet–Google的人脸识别</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><em>原文链接</em>:<a href=\"https://arxiv.org/abs/1503.03832\" target=\"_blank\" rel=\"external\">FaceNet:A unified embedding for face recognition and clustering</a></p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>FaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。</p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>FaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。</p>\n<p>当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。选取的triplets包含两个匹配脸部缩略图（为紧密裁剪的脸部区域，不用使用2d、3d对齐以及放大转换等预处理）和一个非匹配的脸部缩略图，loss函数目标是通过距离边界区分正负类。</p>\n<p>本文中，探索了两类深度卷积神经网络，第一类为Zeiler&amp;Fergus研究中使用的神经网络，包含多个交错的卷积层、非线性激励函数，局部相应归一化和最大池化层。我们额外的添加了一些1x1xd的卷积层。第二种结构是基于Inception model，这种网络利用了一些不同的卷积层和池化层并行和级联响应。我们发现这些模型可以减小20倍以上的参数数量，并且可能会减少FLOPS数量。</p>\n<p>triplet loss的启发是传统loss函数趋向于将有一类特征的人脸图像映射到同一个空间，而triplet loss尝试将一个个体的人脸图像和其他人脸图像分开。</p>\n<p><img src=\"/images/facenet_triplet1.png\" alt=\"facenet_triplet1\"></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>三元组的目标函数并不是这篇论文首创，我在之前的一些Hash索引的论文中也见过相似的应用。可见，并不是所有的学习特征的模型都必须用softmax。用其他的效果也会好。</li>\n<li>三元组比softmax的优势在于<ul>\n<li>softmax不直接，（三元组直接优化距离），因而性能也不好。</li>\n<li>softmax产生的特征表示向量都很大，一般超过1000维。</li>\n</ul>\n</li>\n<li>FaceNet并没有像DeepFace和DeepID那样需要对齐。</li>\n<li>FaceNet得到最终表示后不用像DeepID那样需要再训练模型进行分类，直接计算距离就好了，简单而有效。</li>\n<li>论文并未探讨二元对的有效性，直接使用的三元对。</li>\n</ul>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><p><a href=\"https://zhuanlan.zhihu.com/p/24837264\" target=\"_blank\" rel=\"external\">谷歌人脸识别系统FaceNet解析</a></p>\n<p><a href=\"http://blog.csdn.net/stdcoutzyx/article/details/46687471\" target=\"_blank\" rel=\"external\">FaceNet–Google的人脸识别</a></p>\n"},{"title":"Hexo相关","mathjax":true,"date":"2017-06-02T08:05:45.000Z","description":"Hexo相关的配置说明","_content":"\n# 问题\n\n## Hexo无法正常显示公式\n\n善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：\n\n```\n# MathJax Support\nmathjax:\n  enable: true\n  per_page: true\n```\n\n另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：\n\n```\n---\ntitle: index.html\ndate: 2016-12-28 21:01:30\ntags:\nmathjax: true\n--\n```\n\n题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：\n\n```\ntitle: {{ title }}\ndate: {{ date }}\ncategories: \ntags:\ndescription: \nmathjax: \n```\n\n\n\n# 优化\n\n[Hexo的版本控制与持续集成](https://formulahendry.github.io/2016/12/04/hexo-ci/)\n\n","source":"_posts/Hexo相关.md","raw":"---\ntitle: Hexo相关\nmathjax: true\ndate: 2017-06-02 16:05:45\ncategories: Hexo\ntags: [Hexo]\ndescription: Hexo相关的配置说明\n---\n\n# 问题\n\n## Hexo无法正常显示公式\n\n善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：\n\n```\n# MathJax Support\nmathjax:\n  enable: true\n  per_page: true\n```\n\n另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：\n\n```\n---\ntitle: index.html\ndate: 2016-12-28 21:01:30\ntags:\nmathjax: true\n--\n```\n\n题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：\n\n```\ntitle: {{ title }}\ndate: {{ date }}\ncategories: \ntags:\ndescription: \nmathjax: \n```\n\n\n\n# 优化\n\n[Hexo的版本控制与持续集成](https://formulahendry.github.io/2016/12/04/hexo-ci/)\n\n","slug":"Hexo相关","published":1,"updated":"2018-12-05T09:21:07.498Z","_id":"cjpaxs0j90009s0vwg4rdbq2m","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><h2 id=\"Hexo无法正常显示公式\"><a href=\"#Hexo无法正常显示公式\" class=\"headerlink\" title=\"Hexo无法正常显示公式\"></a>Hexo无法正常显示公式</h2><p>善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"># MathJax Support</div><div class=\"line\">mathjax:</div><div class=\"line\">  enable: true</div><div class=\"line\">  per_page: true</div></pre></td></tr></table></figure>\n<p>另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">---</div><div class=\"line\">title: index.html</div><div class=\"line\">date: 2016-12-28 21:01:30</div><div class=\"line\">tags:</div><div class=\"line\">mathjax: true</div><div class=\"line\">--</div></pre></td></tr></table></figure>\n<p>题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">title: &#123;&#123; title &#125;&#125;</div><div class=\"line\">date: &#123;&#123; date &#125;&#125;</div><div class=\"line\">categories: </div><div class=\"line\">tags:</div><div class=\"line\">description: </div><div class=\"line\">mathjax:</div></pre></td></tr></table></figure>\n<h1 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h1><p><a href=\"https://formulahendry.github.io/2016/12/04/hexo-ci/\" target=\"_blank\" rel=\"external\">Hexo的版本控制与持续集成</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><h2 id=\"Hexo无法正常显示公式\"><a href=\"#Hexo无法正常显示公式\" class=\"headerlink\" title=\"Hexo无法正常显示公式\"></a>Hexo无法正常显示公式</h2><p>善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"># MathJax Support</div><div class=\"line\">mathjax:</div><div class=\"line\">  enable: true</div><div class=\"line\">  per_page: true</div></pre></td></tr></table></figure>\n<p>另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">---</div><div class=\"line\">title: index.html</div><div class=\"line\">date: 2016-12-28 21:01:30</div><div class=\"line\">tags:</div><div class=\"line\">mathjax: true</div><div class=\"line\">--</div></pre></td></tr></table></figure>\n<p>题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">title: &#123;&#123; title &#125;&#125;</div><div class=\"line\">date: &#123;&#123; date &#125;&#125;</div><div class=\"line\">categories: </div><div class=\"line\">tags:</div><div class=\"line\">description: </div><div class=\"line\">mathjax:</div></pre></td></tr></table></figure>\n<h1 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h1><p><a href=\"https://formulahendry.github.io/2016/12/04/hexo-ci/\" target=\"_blank\" rel=\"external\">Hexo的版本控制与持续集成</a></p>\n"},{"title":"Python","date":"2017-06-09T12:52:56.000Z","description":"运用python处理一些简单问题的记录","mathjax":false,"_content":"\n# 图像操作\n\n## 关于PIL image和skimage的图像处理\n\n### 对skimage图像\n\n镜像处理：\n\n```\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\nimg=io.imread(\"test.jpg\")\n#img=transform.rotate(img,180)\nimg=cv2.flip(img,1)\nplt.figure('skimage')\nplt.imshow(img)\nplt.show()\nprint img.shape\nprint(img.dtype)\n```\n\n# 文本处理\n\n## 提取两个文件中相同的部分\n\na.txt内容：\n\n```\naaa 0\nbbb 0\nccc 0\n```\n\nb.txt内容：\n\n```\naaa 0\nbbb 1\nccc 0\n```\n\n### 提取相同的部分\n\n写入到c.txt\n\n```\nfa=open('a.txt','r')\na=fa.readlines()\nfa.close()\nfb=open('b.txt','r')\nb=fb.readlines()\nfb.close()\nc= [i for i in a if i in b]\nfc=open('c.txt','w')\nfc.writelines(c)\nfc.close()\nprint 'Done'\n```\n\n最后c.txt内容\n\n```\naaa 0\nccc 0\n```\n\n\n\n# 读取文件并绘制图片\n\n## 散点图\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    X = []\n    y = []\n    for line in inFile:\n        trainingSet = line.split()  \n        X.append(trainingSet[0])\n        y.append(trainingSet[1])\n    return (X, y)\n\n\ndef plotData(X, y):\n    length = len(y)\n    plt.figure(1)\n    #plt.plot(X, y, 'rx')\n    plt.scatter(X,y,c='r',marker='.')\n    plt.xlabel('eye_width')\n    plt.ylabel('eye_height')\n    #plt.show()\n    plt.savefig('dis.png')\n\nif __name__ == '__main__':\n    (X, y) = loadData('dis.txt')\n\n    plotData(X, y)\n```\n\n## 折线图\n\n```\nimport matplotlib.pyplot as plt \n\ny1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] \nx1=range(0,200,10) \n\nnum = 0\nfor i in range(20):\n    num+=y1[i]\nprint num\nplt.plot(x1,y1,label='Frist line',linewidth=1,color='r',marker='o', \nmarkerfacecolor='blue',markersize=6) \nplt.xlabel('eye_width distribute') \nplt.ylabel('Num') \nplt.title('Eye\\nCheck it out') \nplt.legend()\nplt.savefig('figure.png') \nplt.show() \n```\n\n\n\n# 统计文件中的数据分布\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    x_lines=inFile.readlines()#x_lines为str的list\n    x_distribute=[0]*20 #对列表元素进行重复复制\n    for x_line in x_lines:\n        x_point=x_line.split()[0]\n        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型\n        x_distribute[i]+=1\n    print x_distribute\n\nif __name__ == '__main__':\n    loadData('dis.txt')\n```\n\n# windows下安装OpenCV for Python\n\n1. Download Python, Numpy, OpenCV from their official sites.\n\n2. Extract OpenCV (will be extracted to a folder opencv)\n\n3. Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd\n\n4. Paste it in C:\\Python27\\Lib\\site-packages\n\n5. Open Python IDLE or terminal, and type\n\n   ```\n   >>> import cv2\n   ```\n\n# 读取文件，进行批量创建目录\n\n存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：\n\n```\nxxx/0.jpg\nyyy/0.jpg\nzzz/0.jpg\n```\n\n创建xxx目录，yyy目录，zzz目录\n\n```\nimport sys\nimport os\nimport numpy as np\nimport skimage\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef read_file(path):\n    with open(path) as f:\n        return list(f)\n\n\ndef make_dir(image_path):   \n    image_lines = read_file(image_path)\n    if not image_lines:\n        print 'empty file'\n        return\n    i = 0\n    for image_line in image_lines:\n        image_line = image_line.strip('\\n')\n        subdir_name = image_line.split('/')[0]\n        print subdir_name\n        isExists=os.path.exists(subdir_name)\n        if not isExists:\n            os.mkdir(subdir_name)\n            print subdir_name+\"created successfully!\"\n       \n        i = i+1\n        sys.stdout.write('\\riter %d\\n' %(i))\n        sys.stdout.flush()\n\n    print 'Done'\n\nif __name__=='__main__': \n    image_path='./image.txt'\n    make_dir(image_path)\n\n```\n\n","source":"_posts/Python.md","raw":"---\ntitle: Python\ndate: 2017-06-09 20:52:56\ncategories:\ntags: Python\ndescription: 运用python处理一些简单问题的记录\nmathjax: False\n---\n\n# 图像操作\n\n## 关于PIL image和skimage的图像处理\n\n### 对skimage图像\n\n镜像处理：\n\n```\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\nimg=io.imread(\"test.jpg\")\n#img=transform.rotate(img,180)\nimg=cv2.flip(img,1)\nplt.figure('skimage')\nplt.imshow(img)\nplt.show()\nprint img.shape\nprint(img.dtype)\n```\n\n# 文本处理\n\n## 提取两个文件中相同的部分\n\na.txt内容：\n\n```\naaa 0\nbbb 0\nccc 0\n```\n\nb.txt内容：\n\n```\naaa 0\nbbb 1\nccc 0\n```\n\n### 提取相同的部分\n\n写入到c.txt\n\n```\nfa=open('a.txt','r')\na=fa.readlines()\nfa.close()\nfb=open('b.txt','r')\nb=fb.readlines()\nfb.close()\nc= [i for i in a if i in b]\nfc=open('c.txt','w')\nfc.writelines(c)\nfc.close()\nprint 'Done'\n```\n\n最后c.txt内容\n\n```\naaa 0\nccc 0\n```\n\n\n\n# 读取文件并绘制图片\n\n## 散点图\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    X = []\n    y = []\n    for line in inFile:\n        trainingSet = line.split()  \n        X.append(trainingSet[0])\n        y.append(trainingSet[1])\n    return (X, y)\n\n\ndef plotData(X, y):\n    length = len(y)\n    plt.figure(1)\n    #plt.plot(X, y, 'rx')\n    plt.scatter(X,y,c='r',marker='.')\n    plt.xlabel('eye_width')\n    plt.ylabel('eye_height')\n    #plt.show()\n    plt.savefig('dis.png')\n\nif __name__ == '__main__':\n    (X, y) = loadData('dis.txt')\n\n    plotData(X, y)\n```\n\n## 折线图\n\n```\nimport matplotlib.pyplot as plt \n\ny1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] \nx1=range(0,200,10) \n\nnum = 0\nfor i in range(20):\n    num+=y1[i]\nprint num\nplt.plot(x1,y1,label='Frist line',linewidth=1,color='r',marker='o', \nmarkerfacecolor='blue',markersize=6) \nplt.xlabel('eye_width distribute') \nplt.ylabel('Num') \nplt.title('Eye\\nCheck it out') \nplt.legend()\nplt.savefig('figure.png') \nplt.show() \n```\n\n\n\n# 统计文件中的数据分布\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    x_lines=inFile.readlines()#x_lines为str的list\n    x_distribute=[0]*20 #对列表元素进行重复复制\n    for x_line in x_lines:\n        x_point=x_line.split()[0]\n        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型\n        x_distribute[i]+=1\n    print x_distribute\n\nif __name__ == '__main__':\n    loadData('dis.txt')\n```\n\n# windows下安装OpenCV for Python\n\n1. Download Python, Numpy, OpenCV from their official sites.\n\n2. Extract OpenCV (will be extracted to a folder opencv)\n\n3. Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd\n\n4. Paste it in C:\\Python27\\Lib\\site-packages\n\n5. Open Python IDLE or terminal, and type\n\n   ```\n   >>> import cv2\n   ```\n\n# 读取文件，进行批量创建目录\n\n存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：\n\n```\nxxx/0.jpg\nyyy/0.jpg\nzzz/0.jpg\n```\n\n创建xxx目录，yyy目录，zzz目录\n\n```\nimport sys\nimport os\nimport numpy as np\nimport skimage\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef read_file(path):\n    with open(path) as f:\n        return list(f)\n\n\ndef make_dir(image_path):   \n    image_lines = read_file(image_path)\n    if not image_lines:\n        print 'empty file'\n        return\n    i = 0\n    for image_line in image_lines:\n        image_line = image_line.strip('\\n')\n        subdir_name = image_line.split('/')[0]\n        print subdir_name\n        isExists=os.path.exists(subdir_name)\n        if not isExists:\n            os.mkdir(subdir_name)\n            print subdir_name+\"created successfully!\"\n       \n        i = i+1\n        sys.stdout.write('\\riter %d\\n' %(i))\n        sys.stdout.flush()\n\n    print 'Done'\n\nif __name__=='__main__': \n    image_path='./image.txt'\n    make_dir(image_path)\n\n```\n\n","slug":"Python","published":1,"updated":"2018-12-05T09:21:41.670Z","_id":"cjpaxs0jz000ds0vwzvetr0qk","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"图像操作\"><a href=\"#图像操作\" class=\"headerlink\" title=\"图像操作\"></a>图像操作</h1><h2 id=\"关于PIL-image和skimage的图像处理\"><a href=\"#关于PIL-image和skimage的图像处理\" class=\"headerlink\" title=\"关于PIL image和skimage的图像处理\"></a>关于PIL image和skimage的图像处理</h2><h3 id=\"对skimage图像\"><a href=\"#对skimage图像\" class=\"headerlink\" title=\"对skimage图像\"></a>对skimage图像</h3><p>镜像处理：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">img=io.imread(&quot;test.jpg&quot;)</div><div class=\"line\">#img=transform.rotate(img,180)</div><div class=\"line\">img=cv2.flip(img,1)</div><div class=\"line\">plt.figure(&apos;skimage&apos;)</div><div class=\"line\">plt.imshow(img)</div><div class=\"line\">plt.show()</div><div class=\"line\">print img.shape</div><div class=\"line\">print(img.dtype)</div></pre></td></tr></table></figure>\n<h1 id=\"文本处理\"><a href=\"#文本处理\" class=\"headerlink\" title=\"文本处理\"></a>文本处理</h1><h2 id=\"提取两个文件中相同的部分\"><a href=\"#提取两个文件中相同的部分\" class=\"headerlink\" title=\"提取两个文件中相同的部分\"></a>提取两个文件中相同的部分</h2><p>a.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<p>b.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 1</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h3 id=\"提取相同的部分\"><a href=\"#提取相同的部分\" class=\"headerlink\" title=\"提取相同的部分\"></a>提取相同的部分</h3><p>写入到c.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">fa=open(&apos;a.txt&apos;,&apos;r&apos;)</div><div class=\"line\">a=fa.readlines()</div><div class=\"line\">fa.close()</div><div class=\"line\">fb=open(&apos;b.txt&apos;,&apos;r&apos;)</div><div class=\"line\">b=fb.readlines()</div><div class=\"line\">fb.close()</div><div class=\"line\">c= [i for i in a if i in b]</div><div class=\"line\">fc=open(&apos;c.txt&apos;,&apos;w&apos;)</div><div class=\"line\">fc.writelines(c)</div><div class=\"line\">fc.close()</div><div class=\"line\">print &apos;Done&apos;</div></pre></td></tr></table></figure>\n<p>最后c.txt内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h1 id=\"读取文件并绘制图片\"><a href=\"#读取文件并绘制图片\" class=\"headerlink\" title=\"读取文件并绘制图片\"></a>读取文件并绘制图片</h1><h2 id=\"散点图\"><a href=\"#散点图\" class=\"headerlink\" title=\"散点图\"></a>散点图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    X = []</div><div class=\"line\">    y = []</div><div class=\"line\">    for line in inFile:</div><div class=\"line\">        trainingSet = line.split()  </div><div class=\"line\">        X.append(trainingSet[0])</div><div class=\"line\">        y.append(trainingSet[1])</div><div class=\"line\">    return (X, y)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def plotData(X, y):</div><div class=\"line\">    length = len(y)</div><div class=\"line\">    plt.figure(1)</div><div class=\"line\">    #plt.plot(X, y, &apos;rx&apos;)</div><div class=\"line\">    plt.scatter(X,y,c=&apos;r&apos;,marker=&apos;.&apos;)</div><div class=\"line\">    plt.xlabel(&apos;eye_width&apos;)</div><div class=\"line\">    plt.ylabel(&apos;eye_height&apos;)</div><div class=\"line\">    #plt.show()</div><div class=\"line\">    plt.savefig(&apos;dis.png&apos;)</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    (X, y) = loadData(&apos;dis.txt&apos;)</div><div class=\"line\"></div><div class=\"line\">    plotData(X, y)</div></pre></td></tr></table></figure>\n<h2 id=\"折线图\"><a href=\"#折线图\" class=\"headerlink\" title=\"折线图\"></a>折线图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib.pyplot as plt </div><div class=\"line\"></div><div class=\"line\">y1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] </div><div class=\"line\">x1=range(0,200,10) </div><div class=\"line\"></div><div class=\"line\">num = 0</div><div class=\"line\">for i in range(20):</div><div class=\"line\">    num+=y1[i]</div><div class=\"line\">print num</div><div class=\"line\">plt.plot(x1,y1,label=&apos;Frist line&apos;,linewidth=1,color=&apos;r&apos;,marker=&apos;o&apos;, </div><div class=\"line\">markerfacecolor=&apos;blue&apos;,markersize=6) </div><div class=\"line\">plt.xlabel(&apos;eye_width distribute&apos;) </div><div class=\"line\">plt.ylabel(&apos;Num&apos;) </div><div class=\"line\">plt.title(&apos;Eye\\nCheck it out&apos;) </div><div class=\"line\">plt.legend()</div><div class=\"line\">plt.savefig(&apos;figure.png&apos;) </div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure>\n<h1 id=\"统计文件中的数据分布\"><a href=\"#统计文件中的数据分布\" class=\"headerlink\" title=\"统计文件中的数据分布\"></a>统计文件中的数据分布</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import numpy as np</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    x_lines=inFile.readlines()#x_lines为str的list</div><div class=\"line\">    x_distribute=[0]*20 #对列表元素进行重复复制</div><div class=\"line\">    for x_line in x_lines:</div><div class=\"line\">        x_point=x_line.split()[0]</div><div class=\"line\">        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型</div><div class=\"line\">        x_distribute[i]+=1</div><div class=\"line\">    print x_distribute</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    loadData(&apos;dis.txt&apos;)</div></pre></td></tr></table></figure>\n<h1 id=\"windows下安装OpenCV-for-Python\"><a href=\"#windows下安装OpenCV-for-Python\" class=\"headerlink\" title=\"windows下安装OpenCV for Python\"></a>windows下安装OpenCV for Python</h1><ol>\n<li><p>Download Python, Numpy, OpenCV from their official sites.</p>\n</li>\n<li><p>Extract OpenCV (will be extracted to a folder opencv)</p>\n</li>\n<li><p>Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd</p>\n</li>\n<li><p>Paste it in C:\\Python27\\Lib\\site-packages</p>\n</li>\n<li><p>Open Python IDLE or terminal, and type</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&gt;&gt;&gt; import cv2</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"读取文件，进行批量创建目录\"><a href=\"#读取文件，进行批量创建目录\" class=\"headerlink\" title=\"读取文件，进行批量创建目录\"></a>读取文件，进行批量创建目录</h1><p>存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">xxx/0.jpg</div><div class=\"line\">yyy/0.jpg</div><div class=\"line\">zzz/0.jpg</div></pre></td></tr></table></figure>\n<p>创建xxx目录，yyy目录，zzz目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">import sys</div><div class=\"line\">import os</div><div class=\"line\">import numpy as np</div><div class=\"line\">import skimage</div><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">def read_file(path):</div><div class=\"line\">    with open(path) as f:</div><div class=\"line\">        return list(f)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def make_dir(image_path):   </div><div class=\"line\">    image_lines = read_file(image_path)</div><div class=\"line\">    if not image_lines:</div><div class=\"line\">        print &apos;empty file&apos;</div><div class=\"line\">        return</div><div class=\"line\">    i = 0</div><div class=\"line\">    for image_line in image_lines:</div><div class=\"line\">        image_line = image_line.strip(&apos;\\n&apos;)</div><div class=\"line\">        subdir_name = image_line.split(&apos;/&apos;)[0]</div><div class=\"line\">        print subdir_name</div><div class=\"line\">        isExists=os.path.exists(subdir_name)</div><div class=\"line\">        if not isExists:</div><div class=\"line\">            os.mkdir(subdir_name)</div><div class=\"line\">            print subdir_name+&quot;created successfully!&quot;</div><div class=\"line\">       </div><div class=\"line\">        i = i+1</div><div class=\"line\">        sys.stdout.write(&apos;\\riter %d\\n&apos; %(i))</div><div class=\"line\">        sys.stdout.flush()</div><div class=\"line\"></div><div class=\"line\">    print &apos;Done&apos;</div><div class=\"line\"></div><div class=\"line\">if __name__==&apos;__main__&apos;: </div><div class=\"line\">    image_path=&apos;./image.txt&apos;</div><div class=\"line\">    make_dir(image_path)</div></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"图像操作\"><a href=\"#图像操作\" class=\"headerlink\" title=\"图像操作\"></a>图像操作</h1><h2 id=\"关于PIL-image和skimage的图像处理\"><a href=\"#关于PIL-image和skimage的图像处理\" class=\"headerlink\" title=\"关于PIL image和skimage的图像处理\"></a>关于PIL image和skimage的图像处理</h2><h3 id=\"对skimage图像\"><a href=\"#对skimage图像\" class=\"headerlink\" title=\"对skimage图像\"></a>对skimage图像</h3><p>镜像处理：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">img=io.imread(&quot;test.jpg&quot;)</div><div class=\"line\">#img=transform.rotate(img,180)</div><div class=\"line\">img=cv2.flip(img,1)</div><div class=\"line\">plt.figure(&apos;skimage&apos;)</div><div class=\"line\">plt.imshow(img)</div><div class=\"line\">plt.show()</div><div class=\"line\">print img.shape</div><div class=\"line\">print(img.dtype)</div></pre></td></tr></table></figure>\n<h1 id=\"文本处理\"><a href=\"#文本处理\" class=\"headerlink\" title=\"文本处理\"></a>文本处理</h1><h2 id=\"提取两个文件中相同的部分\"><a href=\"#提取两个文件中相同的部分\" class=\"headerlink\" title=\"提取两个文件中相同的部分\"></a>提取两个文件中相同的部分</h2><p>a.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<p>b.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 1</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h3 id=\"提取相同的部分\"><a href=\"#提取相同的部分\" class=\"headerlink\" title=\"提取相同的部分\"></a>提取相同的部分</h3><p>写入到c.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">fa=open(&apos;a.txt&apos;,&apos;r&apos;)</div><div class=\"line\">a=fa.readlines()</div><div class=\"line\">fa.close()</div><div class=\"line\">fb=open(&apos;b.txt&apos;,&apos;r&apos;)</div><div class=\"line\">b=fb.readlines()</div><div class=\"line\">fb.close()</div><div class=\"line\">c= [i for i in a if i in b]</div><div class=\"line\">fc=open(&apos;c.txt&apos;,&apos;w&apos;)</div><div class=\"line\">fc.writelines(c)</div><div class=\"line\">fc.close()</div><div class=\"line\">print &apos;Done&apos;</div></pre></td></tr></table></figure>\n<p>最后c.txt内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h1 id=\"读取文件并绘制图片\"><a href=\"#读取文件并绘制图片\" class=\"headerlink\" title=\"读取文件并绘制图片\"></a>读取文件并绘制图片</h1><h2 id=\"散点图\"><a href=\"#散点图\" class=\"headerlink\" title=\"散点图\"></a>散点图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    X = []</div><div class=\"line\">    y = []</div><div class=\"line\">    for line in inFile:</div><div class=\"line\">        trainingSet = line.split()  </div><div class=\"line\">        X.append(trainingSet[0])</div><div class=\"line\">        y.append(trainingSet[1])</div><div class=\"line\">    return (X, y)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def plotData(X, y):</div><div class=\"line\">    length = len(y)</div><div class=\"line\">    plt.figure(1)</div><div class=\"line\">    #plt.plot(X, y, &apos;rx&apos;)</div><div class=\"line\">    plt.scatter(X,y,c=&apos;r&apos;,marker=&apos;.&apos;)</div><div class=\"line\">    plt.xlabel(&apos;eye_width&apos;)</div><div class=\"line\">    plt.ylabel(&apos;eye_height&apos;)</div><div class=\"line\">    #plt.show()</div><div class=\"line\">    plt.savefig(&apos;dis.png&apos;)</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    (X, y) = loadData(&apos;dis.txt&apos;)</div><div class=\"line\"></div><div class=\"line\">    plotData(X, y)</div></pre></td></tr></table></figure>\n<h2 id=\"折线图\"><a href=\"#折线图\" class=\"headerlink\" title=\"折线图\"></a>折线图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib.pyplot as plt </div><div class=\"line\"></div><div class=\"line\">y1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] </div><div class=\"line\">x1=range(0,200,10) </div><div class=\"line\"></div><div class=\"line\">num = 0</div><div class=\"line\">for i in range(20):</div><div class=\"line\">    num+=y1[i]</div><div class=\"line\">print num</div><div class=\"line\">plt.plot(x1,y1,label=&apos;Frist line&apos;,linewidth=1,color=&apos;r&apos;,marker=&apos;o&apos;, </div><div class=\"line\">markerfacecolor=&apos;blue&apos;,markersize=6) </div><div class=\"line\">plt.xlabel(&apos;eye_width distribute&apos;) </div><div class=\"line\">plt.ylabel(&apos;Num&apos;) </div><div class=\"line\">plt.title(&apos;Eye\\nCheck it out&apos;) </div><div class=\"line\">plt.legend()</div><div class=\"line\">plt.savefig(&apos;figure.png&apos;) </div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure>\n<h1 id=\"统计文件中的数据分布\"><a href=\"#统计文件中的数据分布\" class=\"headerlink\" title=\"统计文件中的数据分布\"></a>统计文件中的数据分布</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import numpy as np</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    x_lines=inFile.readlines()#x_lines为str的list</div><div class=\"line\">    x_distribute=[0]*20 #对列表元素进行重复复制</div><div class=\"line\">    for x_line in x_lines:</div><div class=\"line\">        x_point=x_line.split()[0]</div><div class=\"line\">        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型</div><div class=\"line\">        x_distribute[i]+=1</div><div class=\"line\">    print x_distribute</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    loadData(&apos;dis.txt&apos;)</div></pre></td></tr></table></figure>\n<h1 id=\"windows下安装OpenCV-for-Python\"><a href=\"#windows下安装OpenCV-for-Python\" class=\"headerlink\" title=\"windows下安装OpenCV for Python\"></a>windows下安装OpenCV for Python</h1><ol>\n<li><p>Download Python, Numpy, OpenCV from their official sites.</p>\n</li>\n<li><p>Extract OpenCV (will be extracted to a folder opencv)</p>\n</li>\n<li><p>Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd</p>\n</li>\n<li><p>Paste it in C:\\Python27\\Lib\\site-packages</p>\n</li>\n<li><p>Open Python IDLE or terminal, and type</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&gt;&gt;&gt; import cv2</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"读取文件，进行批量创建目录\"><a href=\"#读取文件，进行批量创建目录\" class=\"headerlink\" title=\"读取文件，进行批量创建目录\"></a>读取文件，进行批量创建目录</h1><p>存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">xxx/0.jpg</div><div class=\"line\">yyy/0.jpg</div><div class=\"line\">zzz/0.jpg</div></pre></td></tr></table></figure>\n<p>创建xxx目录，yyy目录，zzz目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">import sys</div><div class=\"line\">import os</div><div class=\"line\">import numpy as np</div><div class=\"line\">import skimage</div><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">def read_file(path):</div><div class=\"line\">    with open(path) as f:</div><div class=\"line\">        return list(f)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def make_dir(image_path):   </div><div class=\"line\">    image_lines = read_file(image_path)</div><div class=\"line\">    if not image_lines:</div><div class=\"line\">        print &apos;empty file&apos;</div><div class=\"line\">        return</div><div class=\"line\">    i = 0</div><div class=\"line\">    for image_line in image_lines:</div><div class=\"line\">        image_line = image_line.strip(&apos;\\n&apos;)</div><div class=\"line\">        subdir_name = image_line.split(&apos;/&apos;)[0]</div><div class=\"line\">        print subdir_name</div><div class=\"line\">        isExists=os.path.exists(subdir_name)</div><div class=\"line\">        if not isExists:</div><div class=\"line\">            os.mkdir(subdir_name)</div><div class=\"line\">            print subdir_name+&quot;created successfully!&quot;</div><div class=\"line\">       </div><div class=\"line\">        i = i+1</div><div class=\"line\">        sys.stdout.write(&apos;\\riter %d\\n&apos; %(i))</div><div class=\"line\">        sys.stdout.flush()</div><div class=\"line\"></div><div class=\"line\">    print &apos;Done&apos;</div><div class=\"line\"></div><div class=\"line\">if __name__==&apos;__main__&apos;: </div><div class=\"line\">    image_path=&apos;./image.txt&apos;</div><div class=\"line\">    make_dir(image_path)</div></pre></td></tr></table></figure>\n"},{"title":"Triplet loss","date":"2017-06-02T12:26:35.000Z","description":"Triplet loss论文阅读笔记","mathjax":true,"_content":"\n# 原理\n\nTriplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。\n\n\n\n\n\n![Triplet Loss 示意图](http://img.blog.csdn.net/20160727090101355)\n\n\n\n### Triplet loss中的margin取值分析\n\n我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。\n\n```\n当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。\n\n当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。\n\n因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。\n```\n\n\n\n## 相关\n\n区分相似图形，除了triplet loss，还有一篇CVPR：[《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》](http://blog.csdn.net/u010167269/article/details/51783446)提出的Coupled Cluster Loss.\n\n\n\n**本文参考**：\n\n[triplet loss 原理以及梯度推导](http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html)\n\n[如何在Caffe中增加layer以及Caffe中triplet loss layer的实现](http://www.voidcn.com/blog/mao_kun/article/p-6246924.html)\n\n\n\n\n\n\n\n\n\n","source":"_posts/Triplet loss.md","raw":"---\ntitle: Triplet loss\ndate: 2017-06-02 20:26:35\ncategories: 论文笔记\ntags: [深度学习，人脸识别]\ndescription: Triplet loss论文阅读笔记\nmathjax: true\n---\n\n# 原理\n\nTriplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。\n\n\n\n\n\n![Triplet Loss 示意图](http://img.blog.csdn.net/20160727090101355)\n\n\n\n### Triplet loss中的margin取值分析\n\n我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。\n\n```\n当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。\n\n当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。\n\n因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。\n```\n\n\n\n## 相关\n\n区分相似图形，除了triplet loss，还有一篇CVPR：[《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》](http://blog.csdn.net/u010167269/article/details/51783446)提出的Coupled Cluster Loss.\n\n\n\n**本文参考**：\n\n[triplet loss 原理以及梯度推导](http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html)\n\n[如何在Caffe中增加layer以及Caffe中triplet loss layer的实现](http://www.voidcn.com/blog/mao_kun/article/p-6246924.html)\n\n\n\n\n\n\n\n\n\n","slug":"Triplet loss","published":1,"updated":"2018-12-05T09:29:18.170Z","_id":"cjpaxs0kj000fs0vw9p3oz5sf","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h1><p>Triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。</p>\n<p><img src=\"http://img.blog.csdn.net/20160727090101355\" alt=\"Triplet Loss 示意图\"></p>\n<h3 id=\"Triplet-loss中的margin取值分析\"><a href=\"#Triplet-loss中的margin取值分析\" class=\"headerlink\" title=\"Triplet loss中的margin取值分析\"></a>Triplet loss中的margin取值分析</h3><p>我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。</div><div class=\"line\"></div><div class=\"line\">当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。</div><div class=\"line\"></div><div class=\"line\">因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。</div></pre></td></tr></table></figure>\n<h2 id=\"相关\"><a href=\"#相关\" class=\"headerlink\" title=\"相关\"></a>相关</h2><p>区分相似图形，除了triplet loss，还有一篇CVPR：<a href=\"http://blog.csdn.net/u010167269/article/details/51783446\" target=\"_blank\" rel=\"external\">《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》</a>提出的Coupled Cluster Loss.</p>\n<p><strong>本文参考</strong>：</p>\n<p><a href=\"http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html\" target=\"_blank\" rel=\"external\">triplet loss 原理以及梯度推导</a></p>\n<p><a href=\"http://www.voidcn.com/blog/mao_kun/article/p-6246924.html\" target=\"_blank\" rel=\"external\">如何在Caffe中增加layer以及Caffe中triplet loss layer的实现</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h1><p>Triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。</p>\n<p><img src=\"http://img.blog.csdn.net/20160727090101355\" alt=\"Triplet Loss 示意图\"></p>\n<h3 id=\"Triplet-loss中的margin取值分析\"><a href=\"#Triplet-loss中的margin取值分析\" class=\"headerlink\" title=\"Triplet loss中的margin取值分析\"></a>Triplet loss中的margin取值分析</h3><p>我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。</div><div class=\"line\"></div><div class=\"line\">当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。</div><div class=\"line\"></div><div class=\"line\">因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。</div></pre></td></tr></table></figure>\n<h2 id=\"相关\"><a href=\"#相关\" class=\"headerlink\" title=\"相关\"></a>相关</h2><p>区分相似图形，除了triplet loss，还有一篇CVPR：<a href=\"http://blog.csdn.net/u010167269/article/details/51783446\" target=\"_blank\" rel=\"external\">《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》</a>提出的Coupled Cluster Loss.</p>\n<p><strong>本文参考</strong>：</p>\n<p><a href=\"http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html\" target=\"_blank\" rel=\"external\">triplet loss 原理以及梯度推导</a></p>\n<p><a href=\"http://www.voidcn.com/blog/mao_kun/article/p-6246924.html\" target=\"_blank\" rel=\"external\">如何在Caffe中增加layer以及Caffe中triplet loss layer的实现</a></p>\n"},{"title":"人脸识别回顾","date":"2018-01-29T01:47:23.000Z","description":"人脸识别的一般流程","mathjax":null,"_content":"\n# 数据准备\n\n**数据清洗**：去除错误标签，图片质量不好的样本，如果有benchmark，还需要去除和benchmark重复的样本。\n\n**构造验证集**:  对于给定的数据集，可能没有划分train set和validation set ，需要手动从给定的训练集中按照一定的比例分离出验证集（比如9:1）\n\n**数据均衡**：数据集可能存在类别不均衡的问题，可以通过重新组合不均衡数据, 使之均衡，方式一: 复制或者合成（比如jitter操作）少数部分的样本, 使之和多数部分差不多数量； 方式二: 砍掉一些多数部分, 使两者数量差不多\n\n**数据扩充**：对于一些样本数据比较少的数据集，为了更好的训练网络，有时候需要人为增加一些训练样本，比如随机的剪裁、缩放和旋转等。\n\n**预处理**：常见的就是减均值、除方差。\n\n\n\n# 训练模型\n\n## 1. 模型选择：\n\n根据具体任务和数据集规模选择合适的网络结构，对于分类任务来说，如果数据集的规模不大，则网络的层数不应太深，结构也不应太复杂。\n\n## 2. 激励函数的选择\n\n+ **sigmoid函数**：取值范围为(0,1)，可以将一个实数映射到(0,1)的区间，可以用来做二分类，在特征相差比较复杂或是相差不是特别大时效果比较好，缺点是：激活函数计算量大，反向传播求梯度时，求导涉及除法，反向传播时，很容易出现梯度消失的情况\n\n+ **Tanh函数**：取值范围为[-1,1]，在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果，与sigmoid的区别是,tanh是0均值的，因此实际应用中，tanh会比sigmoid更好。\n\n+ **ReLU函数**：使用ReLU得到的SGD的收敛速度会比sigmoid/tanh快很多，缺点是训练的时候很”脆弱”，很容易就”die”了例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。\n\n  选择的时候，就是根据各个函数的优缺点来配置，例如：\n\n  如果使用 ReLU，要小心设置 learning rate，注意不要让网络出现很多 “dead” 神经元，如果不好解决，可以试试 Leaky ReLU、PReLU 或者 Maxout.\n\n  **详细细节请参看**：[常用激活函数比较](https://www.jianshu.com/p/22d9720dbf1a)\n\n\n## 3. 卷积tricks\n\n图片输入是2的幂次方，例如32、64、96、224等。\n\n卷积核大小是3*3或者5*5。\n\n输入图片上下左右需要用0补充，即padding，且假如卷积核大小是5，那么padding就是2（图片左右上下都补充2）；卷积核大小是3，padding大小就是1。\n\n## 4. pooling层tricks\n\npoolin层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。max pooling比avg pooling效果会好一些。avg-global pooling进入全卷积时代。\n\n## 5. Loss函数的选择\n\n+ **softmax loss**\n\n+ contrastive loss\n\n+ triplet loss\n\n+ **center loss**\n\n  **triplet loss比softmax的优势**\n\n  - 在于softmax不直接，（三元组直接优化距离），因而性能也不好。\n  - softmax产生的特征表示向量都很大，一般超过1000维。\n  - 利用triplet loss的metric learning目的在于减小类内的L2距离，同时增大类间的距离\n\n  **center loss VS triplet loss**\n\n  - triplet loss:dramatic data expansion\n  - center loss:more directly and efficiently\n\n\n\n\n\n\n\n# 神经网络的设计模式\n\n1. **Architectural Structure follows the Application**（架构遵循应用）\n\n   应该根据自己的应用场景选择合适的网络架构。\n\n2. **Proliferate Paths**（路径扩增）\n\n   每年ImageNet Challenge的赢家都比上一年的冠军使用更加深层的网络。从AlexNet 到Inception到Resnets，Smith和他的团队也观察到“网络的路径数量成倍增长”的趋势，ResNet可以是不同长度的网络的指数集合。\n\n3. **Strive for Simplicity**（简洁原则）\n\n   更大的并不一定是更好的。在名为“Bigger is not necessarily better”的论文中，Springenberg 等人演示了如何用更少的单元实现最先进的结果。\n\n4. **Increase Symmetry** （增加对称性）\n\n   无论是在建筑上，还是在生物上，对称性被认为是质量和工艺的标志。Smith 将 FractalNet 的优雅归功于网络的对称性。\n\n5. **Pyramid Shape** （金字塔型）\n\n   在表征能力和减少冗余或者无用信息之间权衡。CNNs通常会降低激活函数的采样，并会增加从输入层到最终层之间的连接通道。\n\n6. **Over-train** （过训练）\n\n   另一个权衡是训练准确率和泛化能力。用正则化的方法类似 drop-out 或 drop-path进行提升泛化能力，这是神经网络的重要优势。用比实际用例更难的问题训练你的网络，以提高泛化性能。\n\n7. **Cover the Problem Space** （覆盖问题空间）\n\n   为了扩大你的训练数据和提升泛化能力，要使用噪声和人工增加训练集的大小，例如随机旋转、裁剪和一些图像操作。\n\n8. **Incremental Feature Construction** （递增的功能结构）\n\n   当架构变得成功时，它们会简化每一层的“工作”。在非常深的神经网络中，每个 层只会递增地修改输入。在ResNets中，每一层的输出可能类似于输入。所以，在实践中，请在ResNet中使用短的跳过长度。\n\n9. **Normalize Layer Inputs** （规范化层输入）\n\n   标准化是可以使计算层的工作变得更加容易的一条捷径，并且在实际中可以提升训练的准确性。批量标准化的发明者认为标准化发挥作用的原因在于处理内部的协变量，但Smith 认为，“标准化把所有层的输入样本放在了一个平等的基础上（类似于单位转换），这允许反向传播可以更有效地训练”。\n\n10. **Input Transition** （输入转换）\n\n  研究表明，在Wide ResNets中,性能随着通道数量的增加而提升，但是要权衡训练成本与准确性。AlexNet，VGG，Inception和ResNets都是在第一层中进行输入变换，以保证多种方式检查输入数据。\n\n11. **Available Resources Guide Layer Widths** （可用资源决定层宽度）\n\n    可供选择的输出数量并不明显，相应的是，这取决于您的硬件功能和所需的准确性。\n\n12. **Summation Joining** （求和连接）\n\n    Summation是一种流行的合并分支的方式。在 ResNets 中，使用求和作为连接机制可以让每一个分支都能计算残差和整体近似。如果输入跳跃连接始终存在，那么Summation会让每一层学到正确地东西（例如：输入的差别）。在任何分支都可以被丢弃的网络（例如 FractalNet）中，你应该使用这种方式保持输出的平滑。\n\n13. **Down-sampling Transition** （下采样变换）\n\n    在汇聚的时候，利用级联连接来增加输出的数量。当使用大于1的步幅时，这会同时处理加入并增加通道的数量。\n\n14. **Maxout for Competition ** （用于竞争的Maxout）\n\n    Maxout 用在只需要选择一个激活函数的局部竞争网络中。用的方法包含所有的激活函数，不同之处在于 maxout 只选择一个“胜出者”。Maxout 的一个明显的用例是每个分支具有不同大小的内核，而 Maxout 可以尺度不变。\n\n\n\n\n# Caffe调参\n\n##1. loss为nan(83.3365)  [标签错误、学习率太大]\n\n**梯度爆炸**\n\n**原因**：梯度变得非常大，使得学习过程难以继续\n\n**现象：**观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。\n\n**措施**： \n\n1. 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 \n2. 设置clip gradient，用于限制过大的diff\n\n\n\n**不当的损失函数**\n\n**原因**：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。\n\n**现象**：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。\n\n**措施**：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。\n\n示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。\n\n\n\n**不当的输入**\n\n**原因**：输入中就含有NaN。\n\n**现象**：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。\n\n**措施**：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。\n\n**案例**：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。\n\n**池化层中步长比核的尺寸大**\n\n如下例所示，当池化层中stride > kernel的时候会在y中产生NaN\n\n```\n    layer {\n      name: \"faulty_pooling\"\n      type: \"Pooling\"\n      bottom: \"x\"\n      top: \"y\"\n      pooling_param {\n      pool: AVE\n      stride: 5\n      kernel: 3\n      }\n    }\n```\n\n##2. 避免overfitting\n\n- simpler model structure\n\n- regularization\n\n- data augmentation\n\n- dropall(dropout+drop-path)\n\n- Bootstrap/Bagging\n\n- ensemble\n\n- early stopping\n\n- utilize invariance\n\n- Bayesian\n\n  ​\n\n\n\n# 人脸识别流程\n\n**一般流程**:数据准备->人脸检测->人脸对齐->生成数据文件->训练模型->调参->model\n\n**tricks**:\n\n+ 在数据准备阶段，可以采用jitter等操作对样本数量较少的种类进行扩充，弥补类别不均衡造成的影响\n+ 在生成模型阶段，可以采用ensemble的方式对多个模型进行融合，比如mirror。\n\n\n\n**参考及致谢**\n\n[Common causes of nans during training](https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training)\n\n[常用激活函数比较](https://www.jianshu.com/p/22d9720dbf1a)\n\n[深度学习之五常见tricks](https://chenrudan.github.io/blog/2015/08/04/dl5tricks.html)\n\n《Deep convolutional neural network design patterns 》\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/人脸识别回顾.md","raw":"---\ntitle: 人脸识别回顾\ndate: 2018-01-29 09:47:23\ncategories: 深度学习\ntags: [人脸识别]\ndescription: 人脸识别的一般流程\nmathjax:\n---\n\n# 数据准备\n\n**数据清洗**：去除错误标签，图片质量不好的样本，如果有benchmark，还需要去除和benchmark重复的样本。\n\n**构造验证集**:  对于给定的数据集，可能没有划分train set和validation set ，需要手动从给定的训练集中按照一定的比例分离出验证集（比如9:1）\n\n**数据均衡**：数据集可能存在类别不均衡的问题，可以通过重新组合不均衡数据, 使之均衡，方式一: 复制或者合成（比如jitter操作）少数部分的样本, 使之和多数部分差不多数量； 方式二: 砍掉一些多数部分, 使两者数量差不多\n\n**数据扩充**：对于一些样本数据比较少的数据集，为了更好的训练网络，有时候需要人为增加一些训练样本，比如随机的剪裁、缩放和旋转等。\n\n**预处理**：常见的就是减均值、除方差。\n\n\n\n# 训练模型\n\n## 1. 模型选择：\n\n根据具体任务和数据集规模选择合适的网络结构，对于分类任务来说，如果数据集的规模不大，则网络的层数不应太深，结构也不应太复杂。\n\n## 2. 激励函数的选择\n\n+ **sigmoid函数**：取值范围为(0,1)，可以将一个实数映射到(0,1)的区间，可以用来做二分类，在特征相差比较复杂或是相差不是特别大时效果比较好，缺点是：激活函数计算量大，反向传播求梯度时，求导涉及除法，反向传播时，很容易出现梯度消失的情况\n\n+ **Tanh函数**：取值范围为[-1,1]，在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果，与sigmoid的区别是,tanh是0均值的，因此实际应用中，tanh会比sigmoid更好。\n\n+ **ReLU函数**：使用ReLU得到的SGD的收敛速度会比sigmoid/tanh快很多，缺点是训练的时候很”脆弱”，很容易就”die”了例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。\n\n  选择的时候，就是根据各个函数的优缺点来配置，例如：\n\n  如果使用 ReLU，要小心设置 learning rate，注意不要让网络出现很多 “dead” 神经元，如果不好解决，可以试试 Leaky ReLU、PReLU 或者 Maxout.\n\n  **详细细节请参看**：[常用激活函数比较](https://www.jianshu.com/p/22d9720dbf1a)\n\n\n## 3. 卷积tricks\n\n图片输入是2的幂次方，例如32、64、96、224等。\n\n卷积核大小是3*3或者5*5。\n\n输入图片上下左右需要用0补充，即padding，且假如卷积核大小是5，那么padding就是2（图片左右上下都补充2）；卷积核大小是3，padding大小就是1。\n\n## 4. pooling层tricks\n\npoolin层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。max pooling比avg pooling效果会好一些。avg-global pooling进入全卷积时代。\n\n## 5. Loss函数的选择\n\n+ **softmax loss**\n\n+ contrastive loss\n\n+ triplet loss\n\n+ **center loss**\n\n  **triplet loss比softmax的优势**\n\n  - 在于softmax不直接，（三元组直接优化距离），因而性能也不好。\n  - softmax产生的特征表示向量都很大，一般超过1000维。\n  - 利用triplet loss的metric learning目的在于减小类内的L2距离，同时增大类间的距离\n\n  **center loss VS triplet loss**\n\n  - triplet loss:dramatic data expansion\n  - center loss:more directly and efficiently\n\n\n\n\n\n\n\n# 神经网络的设计模式\n\n1. **Architectural Structure follows the Application**（架构遵循应用）\n\n   应该根据自己的应用场景选择合适的网络架构。\n\n2. **Proliferate Paths**（路径扩增）\n\n   每年ImageNet Challenge的赢家都比上一年的冠军使用更加深层的网络。从AlexNet 到Inception到Resnets，Smith和他的团队也观察到“网络的路径数量成倍增长”的趋势，ResNet可以是不同长度的网络的指数集合。\n\n3. **Strive for Simplicity**（简洁原则）\n\n   更大的并不一定是更好的。在名为“Bigger is not necessarily better”的论文中，Springenberg 等人演示了如何用更少的单元实现最先进的结果。\n\n4. **Increase Symmetry** （增加对称性）\n\n   无论是在建筑上，还是在生物上，对称性被认为是质量和工艺的标志。Smith 将 FractalNet 的优雅归功于网络的对称性。\n\n5. **Pyramid Shape** （金字塔型）\n\n   在表征能力和减少冗余或者无用信息之间权衡。CNNs通常会降低激活函数的采样，并会增加从输入层到最终层之间的连接通道。\n\n6. **Over-train** （过训练）\n\n   另一个权衡是训练准确率和泛化能力。用正则化的方法类似 drop-out 或 drop-path进行提升泛化能力，这是神经网络的重要优势。用比实际用例更难的问题训练你的网络，以提高泛化性能。\n\n7. **Cover the Problem Space** （覆盖问题空间）\n\n   为了扩大你的训练数据和提升泛化能力，要使用噪声和人工增加训练集的大小，例如随机旋转、裁剪和一些图像操作。\n\n8. **Incremental Feature Construction** （递增的功能结构）\n\n   当架构变得成功时，它们会简化每一层的“工作”。在非常深的神经网络中，每个 层只会递增地修改输入。在ResNets中，每一层的输出可能类似于输入。所以，在实践中，请在ResNet中使用短的跳过长度。\n\n9. **Normalize Layer Inputs** （规范化层输入）\n\n   标准化是可以使计算层的工作变得更加容易的一条捷径，并且在实际中可以提升训练的准确性。批量标准化的发明者认为标准化发挥作用的原因在于处理内部的协变量，但Smith 认为，“标准化把所有层的输入样本放在了一个平等的基础上（类似于单位转换），这允许反向传播可以更有效地训练”。\n\n10. **Input Transition** （输入转换）\n\n  研究表明，在Wide ResNets中,性能随着通道数量的增加而提升，但是要权衡训练成本与准确性。AlexNet，VGG，Inception和ResNets都是在第一层中进行输入变换，以保证多种方式检查输入数据。\n\n11. **Available Resources Guide Layer Widths** （可用资源决定层宽度）\n\n    可供选择的输出数量并不明显，相应的是，这取决于您的硬件功能和所需的准确性。\n\n12. **Summation Joining** （求和连接）\n\n    Summation是一种流行的合并分支的方式。在 ResNets 中，使用求和作为连接机制可以让每一个分支都能计算残差和整体近似。如果输入跳跃连接始终存在，那么Summation会让每一层学到正确地东西（例如：输入的差别）。在任何分支都可以被丢弃的网络（例如 FractalNet）中，你应该使用这种方式保持输出的平滑。\n\n13. **Down-sampling Transition** （下采样变换）\n\n    在汇聚的时候，利用级联连接来增加输出的数量。当使用大于1的步幅时，这会同时处理加入并增加通道的数量。\n\n14. **Maxout for Competition ** （用于竞争的Maxout）\n\n    Maxout 用在只需要选择一个激活函数的局部竞争网络中。用的方法包含所有的激活函数，不同之处在于 maxout 只选择一个“胜出者”。Maxout 的一个明显的用例是每个分支具有不同大小的内核，而 Maxout 可以尺度不变。\n\n\n\n\n# Caffe调参\n\n##1. loss为nan(83.3365)  [标签错误、学习率太大]\n\n**梯度爆炸**\n\n**原因**：梯度变得非常大，使得学习过程难以继续\n\n**现象：**观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。\n\n**措施**： \n\n1. 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 \n2. 设置clip gradient，用于限制过大的diff\n\n\n\n**不当的损失函数**\n\n**原因**：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。\n\n**现象**：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。\n\n**措施**：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。\n\n示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。\n\n\n\n**不当的输入**\n\n**原因**：输入中就含有NaN。\n\n**现象**：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。\n\n**措施**：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。\n\n**案例**：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。\n\n**池化层中步长比核的尺寸大**\n\n如下例所示，当池化层中stride > kernel的时候会在y中产生NaN\n\n```\n    layer {\n      name: \"faulty_pooling\"\n      type: \"Pooling\"\n      bottom: \"x\"\n      top: \"y\"\n      pooling_param {\n      pool: AVE\n      stride: 5\n      kernel: 3\n      }\n    }\n```\n\n##2. 避免overfitting\n\n- simpler model structure\n\n- regularization\n\n- data augmentation\n\n- dropall(dropout+drop-path)\n\n- Bootstrap/Bagging\n\n- ensemble\n\n- early stopping\n\n- utilize invariance\n\n- Bayesian\n\n  ​\n\n\n\n# 人脸识别流程\n\n**一般流程**:数据准备->人脸检测->人脸对齐->生成数据文件->训练模型->调参->model\n\n**tricks**:\n\n+ 在数据准备阶段，可以采用jitter等操作对样本数量较少的种类进行扩充，弥补类别不均衡造成的影响\n+ 在生成模型阶段，可以采用ensemble的方式对多个模型进行融合，比如mirror。\n\n\n\n**参考及致谢**\n\n[Common causes of nans during training](https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training)\n\n[常用激活函数比较](https://www.jianshu.com/p/22d9720dbf1a)\n\n[深度学习之五常见tricks](https://chenrudan.github.io/blog/2015/08/04/dl5tricks.html)\n\n《Deep convolutional neural network design patterns 》\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"人脸识别回顾","published":1,"updated":"2018-12-05T09:24:22.556Z","_id":"cjpaxs0la000js0vwdvuuk6zm","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"数据准备\"><a href=\"#数据准备\" class=\"headerlink\" title=\"数据准备\"></a>数据准备</h1><p><strong>数据清洗</strong>：去除错误标签，图片质量不好的样本，如果有benchmark，还需要去除和benchmark重复的样本。</p>\n<p><strong>构造验证集</strong>:  对于给定的数据集，可能没有划分train set和validation set ，需要手动从给定的训练集中按照一定的比例分离出验证集（比如9:1）</p>\n<p><strong>数据均衡</strong>：数据集可能存在类别不均衡的问题，可以通过重新组合不均衡数据, 使之均衡，方式一: 复制或者合成（比如jitter操作）少数部分的样本, 使之和多数部分差不多数量； 方式二: 砍掉一些多数部分, 使两者数量差不多</p>\n<p><strong>数据扩充</strong>：对于一些样本数据比较少的数据集，为了更好的训练网络，有时候需要人为增加一些训练样本，比如随机的剪裁、缩放和旋转等。</p>\n<p><strong>预处理</strong>：常见的就是减均值、除方差。</p>\n<h1 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h1><h2 id=\"1-模型选择：\"><a href=\"#1-模型选择：\" class=\"headerlink\" title=\"1. 模型选择：\"></a>1. 模型选择：</h2><p>根据具体任务和数据集规模选择合适的网络结构，对于分类任务来说，如果数据集的规模不大，则网络的层数不应太深，结构也不应太复杂。</p>\n<h2 id=\"2-激励函数的选择\"><a href=\"#2-激励函数的选择\" class=\"headerlink\" title=\"2. 激励函数的选择\"></a>2. 激励函数的选择</h2><ul>\n<li><p><strong>sigmoid函数</strong>：取值范围为(0,1)，可以将一个实数映射到(0,1)的区间，可以用来做二分类，在特征相差比较复杂或是相差不是特别大时效果比较好，缺点是：激活函数计算量大，反向传播求梯度时，求导涉及除法，反向传播时，很容易出现梯度消失的情况</p>\n</li>\n<li><p><strong>Tanh函数</strong>：取值范围为[-1,1]，在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果，与sigmoid的区别是,tanh是0均值的，因此实际应用中，tanh会比sigmoid更好。</p>\n</li>\n<li><p><strong>ReLU函数</strong>：使用ReLU得到的SGD的收敛速度会比sigmoid/tanh快很多，缺点是训练的时候很”脆弱”，很容易就”die”了例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。</p>\n<p>选择的时候，就是根据各个函数的优缺点来配置，例如：</p>\n<p>如果使用 ReLU，要小心设置 learning rate，注意不要让网络出现很多 “dead” 神经元，如果不好解决，可以试试 Leaky ReLU、PReLU 或者 Maxout.</p>\n<p><strong>详细细节请参看</strong>：<a href=\"https://www.jianshu.com/p/22d9720dbf1a\" target=\"_blank\" rel=\"external\">常用激活函数比较</a></p>\n</li>\n</ul>\n<h2 id=\"3-卷积tricks\"><a href=\"#3-卷积tricks\" class=\"headerlink\" title=\"3. 卷积tricks\"></a>3. 卷积tricks</h2><p>图片输入是2的幂次方，例如32、64、96、224等。</p>\n<p>卷积核大小是3<em>3或者5</em>5。</p>\n<p>输入图片上下左右需要用0补充，即padding，且假如卷积核大小是5，那么padding就是2（图片左右上下都补充2）；卷积核大小是3，padding大小就是1。</p>\n<h2 id=\"4-pooling层tricks\"><a href=\"#4-pooling层tricks\" class=\"headerlink\" title=\"4. pooling层tricks\"></a>4. pooling层tricks</h2><p>poolin层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。max pooling比avg pooling效果会好一些。avg-global pooling进入全卷积时代。</p>\n<h2 id=\"5-Loss函数的选择\"><a href=\"#5-Loss函数的选择\" class=\"headerlink\" title=\"5. Loss函数的选择\"></a>5. Loss函数的选择</h2><ul>\n<li><p><strong>softmax loss</strong></p>\n</li>\n<li><p>contrastive loss</p>\n</li>\n<li><p>triplet loss</p>\n</li>\n<li><p><strong>center loss</strong></p>\n<p><strong>triplet loss比softmax的优势</strong></p>\n<ul>\n<li>在于softmax不直接，（三元组直接优化距离），因而性能也不好。</li>\n<li>softmax产生的特征表示向量都很大，一般超过1000维。</li>\n<li>利用triplet loss的metric learning目的在于减小类内的L2距离，同时增大类间的距离</li>\n</ul>\n<p><strong>center loss VS triplet loss</strong></p>\n<ul>\n<li>triplet loss:dramatic data expansion</li>\n<li>center loss:more directly and efficiently</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"神经网络的设计模式\"><a href=\"#神经网络的设计模式\" class=\"headerlink\" title=\"神经网络的设计模式\"></a>神经网络的设计模式</h1><ol>\n<li><p><strong>Architectural Structure follows the Application</strong>（架构遵循应用）</p>\n<p>应该根据自己的应用场景选择合适的网络架构。</p>\n</li>\n<li><p><strong>Proliferate Paths</strong>（路径扩增）</p>\n<p>每年ImageNet Challenge的赢家都比上一年的冠军使用更加深层的网络。从AlexNet 到Inception到Resnets，Smith和他的团队也观察到“网络的路径数量成倍增长”的趋势，ResNet可以是不同长度的网络的指数集合。</p>\n</li>\n<li><p><strong>Strive for Simplicity</strong>（简洁原则）</p>\n<p>更大的并不一定是更好的。在名为“Bigger is not necessarily better”的论文中，Springenberg 等人演示了如何用更少的单元实现最先进的结果。</p>\n</li>\n<li><p><strong>Increase Symmetry</strong> （增加对称性）</p>\n<p>无论是在建筑上，还是在生物上，对称性被认为是质量和工艺的标志。Smith 将 FractalNet 的优雅归功于网络的对称性。</p>\n</li>\n<li><p><strong>Pyramid Shape</strong> （金字塔型）</p>\n<p>在表征能力和减少冗余或者无用信息之间权衡。CNNs通常会降低激活函数的采样，并会增加从输入层到最终层之间的连接通道。</p>\n</li>\n<li><p><strong>Over-train</strong> （过训练）</p>\n<p>另一个权衡是训练准确率和泛化能力。用正则化的方法类似 drop-out 或 drop-path进行提升泛化能力，这是神经网络的重要优势。用比实际用例更难的问题训练你的网络，以提高泛化性能。</p>\n</li>\n<li><p><strong>Cover the Problem Space</strong> （覆盖问题空间）</p>\n<p>为了扩大你的训练数据和提升泛化能力，要使用噪声和人工增加训练集的大小，例如随机旋转、裁剪和一些图像操作。</p>\n</li>\n<li><p><strong>Incremental Feature Construction</strong> （递增的功能结构）</p>\n<p>当架构变得成功时，它们会简化每一层的“工作”。在非常深的神经网络中，每个 层只会递增地修改输入。在ResNets中，每一层的输出可能类似于输入。所以，在实践中，请在ResNet中使用短的跳过长度。</p>\n</li>\n<li><p><strong>Normalize Layer Inputs</strong> （规范化层输入）</p>\n<p>标准化是可以使计算层的工作变得更加容易的一条捷径，并且在实际中可以提升训练的准确性。批量标准化的发明者认为标准化发挥作用的原因在于处理内部的协变量，但Smith 认为，“标准化把所有层的输入样本放在了一个平等的基础上（类似于单位转换），这允许反向传播可以更有效地训练”。</p>\n</li>\n<li><p><strong>Input Transition</strong> （输入转换）</p>\n<p>研究表明，在Wide ResNets中,性能随着通道数量的增加而提升，但是要权衡训练成本与准确性。AlexNet，VGG，Inception和ResNets都是在第一层中进行输入变换，以保证多种方式检查输入数据。</p>\n</li>\n<li><p><strong>Available Resources Guide Layer Widths</strong> （可用资源决定层宽度）</p>\n<p>可供选择的输出数量并不明显，相应的是，这取决于您的硬件功能和所需的准确性。</p>\n</li>\n<li><p><strong>Summation Joining</strong> （求和连接）</p>\n<p>Summation是一种流行的合并分支的方式。在 ResNets 中，使用求和作为连接机制可以让每一个分支都能计算残差和整体近似。如果输入跳跃连接始终存在，那么Summation会让每一层学到正确地东西（例如：输入的差别）。在任何分支都可以被丢弃的网络（例如 FractalNet）中，你应该使用这种方式保持输出的平滑。</p>\n</li>\n<li><p><strong>Down-sampling Transition</strong> （下采样变换）</p>\n<p>在汇聚的时候，利用级联连接来增加输出的数量。当使用大于1的步幅时，这会同时处理加入并增加通道的数量。</p>\n</li>\n<li><p><strong>Maxout for Competition </strong> （用于竞争的Maxout）</p>\n<p>Maxout 用在只需要选择一个激活函数的局部竞争网络中。用的方法包含所有的激活函数，不同之处在于 maxout 只选择一个“胜出者”。Maxout 的一个明显的用例是每个分支具有不同大小的内核，而 Maxout 可以尺度不变。</p>\n</li>\n</ol>\n<h1 id=\"Caffe调参\"><a href=\"#Caffe调参\" class=\"headerlink\" title=\"Caffe调参\"></a>Caffe调参</h1><p>##1. loss为nan(83.3365)  [标签错误、学习率太大]</p>\n<p><strong>梯度爆炸</strong></p>\n<p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续</p>\n<p><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。</p>\n<p><strong>措施</strong>： </p>\n<ol>\n<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>\n<li>设置clip gradient，用于限制过大的diff</li>\n</ol>\n<p><strong>不当的损失函数</strong></p>\n<p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p>\n<p><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>\n<p><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。</p>\n<p>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>\n<p><strong>不当的输入</strong></p>\n<p><strong>原因</strong>：输入中就含有NaN。</p>\n<p><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>\n<p><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。</p>\n<p><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>\n<p><strong>池化层中步长比核的尺寸大</strong></p>\n<p>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">  name: &quot;faulty_pooling&quot;</div><div class=\"line\">  type: &quot;Pooling&quot;</div><div class=\"line\">  bottom: &quot;x&quot;</div><div class=\"line\">  top: &quot;y&quot;</div><div class=\"line\">  pooling_param &#123;</div><div class=\"line\">  pool: AVE</div><div class=\"line\">  stride: 5</div><div class=\"line\">  kernel: 3</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>##2. 避免overfitting</p>\n<ul>\n<li><p>simpler model structure</p>\n</li>\n<li><p>regularization</p>\n</li>\n<li><p>data augmentation</p>\n</li>\n<li><p>dropall(dropout+drop-path)</p>\n</li>\n<li><p>Bootstrap/Bagging</p>\n</li>\n<li><p>ensemble</p>\n</li>\n<li><p>early stopping</p>\n</li>\n<li><p>utilize invariance</p>\n</li>\n<li><p>Bayesian</p>\n<p>​</p>\n</li>\n</ul>\n<h1 id=\"人脸识别流程\"><a href=\"#人脸识别流程\" class=\"headerlink\" title=\"人脸识别流程\"></a>人脸识别流程</h1><p><strong>一般流程</strong>:数据准备-&gt;人脸检测-&gt;人脸对齐-&gt;生成数据文件-&gt;训练模型-&gt;调参-&gt;model</p>\n<p><strong>tricks</strong>:</p>\n<ul>\n<li>在数据准备阶段，可以采用jitter等操作对样本数量较少的种类进行扩充，弥补类别不均衡造成的影响</li>\n<li>在生成模型阶段，可以采用ensemble的方式对多个模型进行融合，比如mirror。</li>\n</ul>\n<p><strong>参考及致谢</strong></p>\n<p><a href=\"https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training\" target=\"_blank\" rel=\"external\">Common causes of nans during training</a></p>\n<p><a href=\"https://www.jianshu.com/p/22d9720dbf1a\" target=\"_blank\" rel=\"external\">常用激活函数比较</a></p>\n<p><a href=\"https://chenrudan.github.io/blog/2015/08/04/dl5tricks.html\" target=\"_blank\" rel=\"external\">深度学习之五常见tricks</a></p>\n<p>《Deep convolutional neural network design patterns 》</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"数据准备\"><a href=\"#数据准备\" class=\"headerlink\" title=\"数据准备\"></a>数据准备</h1><p><strong>数据清洗</strong>：去除错误标签，图片质量不好的样本，如果有benchmark，还需要去除和benchmark重复的样本。</p>\n<p><strong>构造验证集</strong>:  对于给定的数据集，可能没有划分train set和validation set ，需要手动从给定的训练集中按照一定的比例分离出验证集（比如9:1）</p>\n<p><strong>数据均衡</strong>：数据集可能存在类别不均衡的问题，可以通过重新组合不均衡数据, 使之均衡，方式一: 复制或者合成（比如jitter操作）少数部分的样本, 使之和多数部分差不多数量； 方式二: 砍掉一些多数部分, 使两者数量差不多</p>\n<p><strong>数据扩充</strong>：对于一些样本数据比较少的数据集，为了更好的训练网络，有时候需要人为增加一些训练样本，比如随机的剪裁、缩放和旋转等。</p>\n<p><strong>预处理</strong>：常见的就是减均值、除方差。</p>\n<h1 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h1><h2 id=\"1-模型选择：\"><a href=\"#1-模型选择：\" class=\"headerlink\" title=\"1. 模型选择：\"></a>1. 模型选择：</h2><p>根据具体任务和数据集规模选择合适的网络结构，对于分类任务来说，如果数据集的规模不大，则网络的层数不应太深，结构也不应太复杂。</p>\n<h2 id=\"2-激励函数的选择\"><a href=\"#2-激励函数的选择\" class=\"headerlink\" title=\"2. 激励函数的选择\"></a>2. 激励函数的选择</h2><ul>\n<li><p><strong>sigmoid函数</strong>：取值范围为(0,1)，可以将一个实数映射到(0,1)的区间，可以用来做二分类，在特征相差比较复杂或是相差不是特别大时效果比较好，缺点是：激活函数计算量大，反向传播求梯度时，求导涉及除法，反向传播时，很容易出现梯度消失的情况</p>\n</li>\n<li><p><strong>Tanh函数</strong>：取值范围为[-1,1]，在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果，与sigmoid的区别是,tanh是0均值的，因此实际应用中，tanh会比sigmoid更好。</p>\n</li>\n<li><p><strong>ReLU函数</strong>：使用ReLU得到的SGD的收敛速度会比sigmoid/tanh快很多，缺点是训练的时候很”脆弱”，很容易就”die”了例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。</p>\n<p>选择的时候，就是根据各个函数的优缺点来配置，例如：</p>\n<p>如果使用 ReLU，要小心设置 learning rate，注意不要让网络出现很多 “dead” 神经元，如果不好解决，可以试试 Leaky ReLU、PReLU 或者 Maxout.</p>\n<p><strong>详细细节请参看</strong>：<a href=\"https://www.jianshu.com/p/22d9720dbf1a\" target=\"_blank\" rel=\"external\">常用激活函数比较</a></p>\n</li>\n</ul>\n<h2 id=\"3-卷积tricks\"><a href=\"#3-卷积tricks\" class=\"headerlink\" title=\"3. 卷积tricks\"></a>3. 卷积tricks</h2><p>图片输入是2的幂次方，例如32、64、96、224等。</p>\n<p>卷积核大小是3<em>3或者5</em>5。</p>\n<p>输入图片上下左右需要用0补充，即padding，且假如卷积核大小是5，那么padding就是2（图片左右上下都补充2）；卷积核大小是3，padding大小就是1。</p>\n<h2 id=\"4-pooling层tricks\"><a href=\"#4-pooling层tricks\" class=\"headerlink\" title=\"4. pooling层tricks\"></a>4. pooling层tricks</h2><p>poolin层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。max pooling比avg pooling效果会好一些。avg-global pooling进入全卷积时代。</p>\n<h2 id=\"5-Loss函数的选择\"><a href=\"#5-Loss函数的选择\" class=\"headerlink\" title=\"5. Loss函数的选择\"></a>5. Loss函数的选择</h2><ul>\n<li><p><strong>softmax loss</strong></p>\n</li>\n<li><p>contrastive loss</p>\n</li>\n<li><p>triplet loss</p>\n</li>\n<li><p><strong>center loss</strong></p>\n<p><strong>triplet loss比softmax的优势</strong></p>\n<ul>\n<li>在于softmax不直接，（三元组直接优化距离），因而性能也不好。</li>\n<li>softmax产生的特征表示向量都很大，一般超过1000维。</li>\n<li>利用triplet loss的metric learning目的在于减小类内的L2距离，同时增大类间的距离</li>\n</ul>\n<p><strong>center loss VS triplet loss</strong></p>\n<ul>\n<li>triplet loss:dramatic data expansion</li>\n<li>center loss:more directly and efficiently</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"神经网络的设计模式\"><a href=\"#神经网络的设计模式\" class=\"headerlink\" title=\"神经网络的设计模式\"></a>神经网络的设计模式</h1><ol>\n<li><p><strong>Architectural Structure follows the Application</strong>（架构遵循应用）</p>\n<p>应该根据自己的应用场景选择合适的网络架构。</p>\n</li>\n<li><p><strong>Proliferate Paths</strong>（路径扩增）</p>\n<p>每年ImageNet Challenge的赢家都比上一年的冠军使用更加深层的网络。从AlexNet 到Inception到Resnets，Smith和他的团队也观察到“网络的路径数量成倍增长”的趋势，ResNet可以是不同长度的网络的指数集合。</p>\n</li>\n<li><p><strong>Strive for Simplicity</strong>（简洁原则）</p>\n<p>更大的并不一定是更好的。在名为“Bigger is not necessarily better”的论文中，Springenberg 等人演示了如何用更少的单元实现最先进的结果。</p>\n</li>\n<li><p><strong>Increase Symmetry</strong> （增加对称性）</p>\n<p>无论是在建筑上，还是在生物上，对称性被认为是质量和工艺的标志。Smith 将 FractalNet 的优雅归功于网络的对称性。</p>\n</li>\n<li><p><strong>Pyramid Shape</strong> （金字塔型）</p>\n<p>在表征能力和减少冗余或者无用信息之间权衡。CNNs通常会降低激活函数的采样，并会增加从输入层到最终层之间的连接通道。</p>\n</li>\n<li><p><strong>Over-train</strong> （过训练）</p>\n<p>另一个权衡是训练准确率和泛化能力。用正则化的方法类似 drop-out 或 drop-path进行提升泛化能力，这是神经网络的重要优势。用比实际用例更难的问题训练你的网络，以提高泛化性能。</p>\n</li>\n<li><p><strong>Cover the Problem Space</strong> （覆盖问题空间）</p>\n<p>为了扩大你的训练数据和提升泛化能力，要使用噪声和人工增加训练集的大小，例如随机旋转、裁剪和一些图像操作。</p>\n</li>\n<li><p><strong>Incremental Feature Construction</strong> （递增的功能结构）</p>\n<p>当架构变得成功时，它们会简化每一层的“工作”。在非常深的神经网络中，每个 层只会递增地修改输入。在ResNets中，每一层的输出可能类似于输入。所以，在实践中，请在ResNet中使用短的跳过长度。</p>\n</li>\n<li><p><strong>Normalize Layer Inputs</strong> （规范化层输入）</p>\n<p>标准化是可以使计算层的工作变得更加容易的一条捷径，并且在实际中可以提升训练的准确性。批量标准化的发明者认为标准化发挥作用的原因在于处理内部的协变量，但Smith 认为，“标准化把所有层的输入样本放在了一个平等的基础上（类似于单位转换），这允许反向传播可以更有效地训练”。</p>\n</li>\n<li><p><strong>Input Transition</strong> （输入转换）</p>\n<p>研究表明，在Wide ResNets中,性能随着通道数量的增加而提升，但是要权衡训练成本与准确性。AlexNet，VGG，Inception和ResNets都是在第一层中进行输入变换，以保证多种方式检查输入数据。</p>\n</li>\n<li><p><strong>Available Resources Guide Layer Widths</strong> （可用资源决定层宽度）</p>\n<p>可供选择的输出数量并不明显，相应的是，这取决于您的硬件功能和所需的准确性。</p>\n</li>\n<li><p><strong>Summation Joining</strong> （求和连接）</p>\n<p>Summation是一种流行的合并分支的方式。在 ResNets 中，使用求和作为连接机制可以让每一个分支都能计算残差和整体近似。如果输入跳跃连接始终存在，那么Summation会让每一层学到正确地东西（例如：输入的差别）。在任何分支都可以被丢弃的网络（例如 FractalNet）中，你应该使用这种方式保持输出的平滑。</p>\n</li>\n<li><p><strong>Down-sampling Transition</strong> （下采样变换）</p>\n<p>在汇聚的时候，利用级联连接来增加输出的数量。当使用大于1的步幅时，这会同时处理加入并增加通道的数量。</p>\n</li>\n<li><p><strong>Maxout for Competition </strong> （用于竞争的Maxout）</p>\n<p>Maxout 用在只需要选择一个激活函数的局部竞争网络中。用的方法包含所有的激活函数，不同之处在于 maxout 只选择一个“胜出者”。Maxout 的一个明显的用例是每个分支具有不同大小的内核，而 Maxout 可以尺度不变。</p>\n</li>\n</ol>\n<h1 id=\"Caffe调参\"><a href=\"#Caffe调参\" class=\"headerlink\" title=\"Caffe调参\"></a>Caffe调参</h1><p>##1. loss为nan(83.3365)  [标签错误、学习率太大]</p>\n<p><strong>梯度爆炸</strong></p>\n<p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续</p>\n<p><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。</p>\n<p><strong>措施</strong>： </p>\n<ol>\n<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>\n<li>设置clip gradient，用于限制过大的diff</li>\n</ol>\n<p><strong>不当的损失函数</strong></p>\n<p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p>\n<p><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>\n<p><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。</p>\n<p>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>\n<p><strong>不当的输入</strong></p>\n<p><strong>原因</strong>：输入中就含有NaN。</p>\n<p><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>\n<p><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。</p>\n<p><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>\n<p><strong>池化层中步长比核的尺寸大</strong></p>\n<p>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">  name: &quot;faulty_pooling&quot;</div><div class=\"line\">  type: &quot;Pooling&quot;</div><div class=\"line\">  bottom: &quot;x&quot;</div><div class=\"line\">  top: &quot;y&quot;</div><div class=\"line\">  pooling_param &#123;</div><div class=\"line\">  pool: AVE</div><div class=\"line\">  stride: 5</div><div class=\"line\">  kernel: 3</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>##2. 避免overfitting</p>\n<ul>\n<li><p>simpler model structure</p>\n</li>\n<li><p>regularization</p>\n</li>\n<li><p>data augmentation</p>\n</li>\n<li><p>dropall(dropout+drop-path)</p>\n</li>\n<li><p>Bootstrap/Bagging</p>\n</li>\n<li><p>ensemble</p>\n</li>\n<li><p>early stopping</p>\n</li>\n<li><p>utilize invariance</p>\n</li>\n<li><p>Bayesian</p>\n<p>​</p>\n</li>\n</ul>\n<h1 id=\"人脸识别流程\"><a href=\"#人脸识别流程\" class=\"headerlink\" title=\"人脸识别流程\"></a>人脸识别流程</h1><p><strong>一般流程</strong>:数据准备-&gt;人脸检测-&gt;人脸对齐-&gt;生成数据文件-&gt;训练模型-&gt;调参-&gt;model</p>\n<p><strong>tricks</strong>:</p>\n<ul>\n<li>在数据准备阶段，可以采用jitter等操作对样本数量较少的种类进行扩充，弥补类别不均衡造成的影响</li>\n<li>在生成模型阶段，可以采用ensemble的方式对多个模型进行融合，比如mirror。</li>\n</ul>\n<p><strong>参考及致谢</strong></p>\n<p><a href=\"https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training\" target=\"_blank\" rel=\"external\">Common causes of nans during training</a></p>\n<p><a href=\"https://www.jianshu.com/p/22d9720dbf1a\" target=\"_blank\" rel=\"external\">常用激活函数比较</a></p>\n<p><a href=\"https://chenrudan.github.io/blog/2015/08/04/dl5tricks.html\" target=\"_blank\" rel=\"external\">深度学习之五常见tricks</a></p>\n<p>《Deep convolutional neural network design patterns 》</p>\n"},{"title":"待办及进度","date":"2017-06-02T07:15:46.000Z","description":"人脸识别实验记录","_content":"# step1:Mirror face相关\n\n| Model              | PCA Size | Threshold | Score  |\n| ------------------ | -------- | --------- | ------ |\n| Mirror             | 192      | 0.64      | 99.42% |\n| Mirror Concat      | 192      | 0.65      | 99.42% |\n| Mirror Add/Average | 184      | 0.64      | 99.47% |\n| Mirror Max         | 144      | 0.65      | 99.43% |\n| Mirror Min         | 168      | 0.65      | 99.48% |\n| Mirror Avg+min     | 168      | 0.65      | 99.45% |\n\n# step2:单眼Patch model\n\n| eye_width | num      | eye_width | num    |\n| --------- | -------- | --------- | ------ |\n| 0~10      | 14       | 90~100    | 8,5674 |\n| 10~20     | 3329     | 100~110   | 2,9481 |\n| 20~30     | 21,3675  | 110~120   | 9051   |\n| 30~40     | 45,1416  | 120~130   | 2894   |\n| 40~50     | 49,1913  | 130~140   | 932    |\n| 50~60     | 72,8911  | 140~150   | 279    |\n| 60~70     | 137,9232 | 150~160   | 86     |\n| 70~80     | 128,7442 | 160~170   | 14     |\n| 80~90     | 30,9026  | 170~180   | 6      |\n\n## 问题\n\n1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y...],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)*0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5*eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句\n```\nif eye_width*0.5>eye_left_x:\n            eye_width=eye_left_x*2\n```\n导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张\n\n重新生成眼睛宽度估算文件，其分布如下\n\n| eye_width | num      | eye_width | num  |\n| --------- | -------- | --------- | ---- |\n| 0~10      | 1,8975   | 100~110   | 98   |\n| 10~20     | 71,3760  | 110~120   | 17   |\n| 20~30     | 126,4102 | 120~130   | 3    |\n| 30~40     | 226,4348 | 130~140   | 0    |\n| 40~50     | 59,3351  | 140~150   | 0    |\n| 50~60     | 10,6502  | 150~160   | 0    |\n| 60~70     | 2,4588   | 160~170   | 1    |\n| 70~80     | 5592     | 170~180   | 0    |\n| 80~90     | 1588     | 180~190   | 8    |\n| 90~100    | 421      | 190~200   | 21   |\n\n\n\n2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可\n3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。\n最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526\n\n<u>17个小时，迭代12万次，26.5%的准确率，loss=5.5。</u>\n\n\n\n# step3:双眼patch model\n\n生成双眼宽度估算文件，其分布如下\n\n| eye_width | num      | eye_width | num     | eye_width | num  |\n| --------- | -------- | --------- | ------- | --------- | ---- |\n| 0~10      | 30       | 100~110   | 18,8771 | 200~210   | 383  |\n| 10~20     | 5888     | 110~120   | 8,8508  | 210~220   | 196  |\n| 20~30     | 11,4595  | 120~130   | 4,6243  | 220~230   | 100  |\n| 30~40     | 35,5451  | 130~140   | 2,4982  | 230~240   | 52   |\n| 40~50     | 42,6729  | 140~150   | 1,2749  | 240~250   | 33   |\n| 50~60     | 49,1312  | 150~160   | 6714    | 250~260   | 15   |\n| 60~70     | 69,0394  | 160~170   | 3413    | 260~270   | 3    |\n| 70~80     | 104,9353 | 170~180   | 1909    | 270~280   | 2    |\n| 80~90     | 100,6618 | 180~190   | 1091    | 280~290   | 0    |\n| 90~100    | 47,7219  | 190~200   | 622     | 290~300   | 0    |\n\n筛选crop后，宽度在20~130区间的图片，共90523个类别，4932655张。\n\ntrain.txt：3982004 \n\nval.txt：950661 \n\n<u>18万次迭代之后，准确率只有66%左右。</u>\n\n\n\n# step4:crop对齐后的图片的眼睛，训练单眼模型\n\n数据集大小：5044507(90525个类)（\"/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt\"，\"/home/yf/data/msclean\"）\n\n       \"ref_points\": [\n       \t\t30.2946, 51.6963, \n            65.5318, 51.5014, \n            48.0252, 71.7366,\n            33.5493, 92.3655, \n            62.7299, 92.2041\n        ]\n    \n    \n         eye_width=(ref_points[2]-ref_points[0])*0.8\n            eye_height=eye_width\n            x1=ref_points[0]-0.5*eye_width=16\n            x2=ref_points[0]+0.5*eye_width=44\n            y1=ref_points[1]-0.5*eye_height=37\n            y2=ref_points[1]+0.5*eye_height=65\ntrain:4071324 张\n\nval:973183张\n\n\n\n# step5:crop对齐后的图片的眼睛，训练双眼模型\n\n数据集大小：5044507(90525个类)（\"/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt\"，\"/home/yf/data/msclean\"）\n\n\n\n       \"ref_points\": [\n       \t30.2946, 51.6963, \n        65.5318, 51.5014, \n        48.0252, 71.7366,\n        33.5493, 92.3655, \n        62.7299, 92.2041\n    ]\n         eye_width=(ref_points[2]-ref_points[0])*0.8\n        eye_height=eye_width\n        x1=ref_points[0]-0.5*eye_width=16\n        x2=ref_points[2]+0.5*eye_width=79\n        y1=ref_points[1]-0.5*eye_height=37\n        y2=ref_points[1]+0.5*eye_height=65\n<u>迭代16万次，精度为72.73%，loss=2.52</u>\n\n<u>在lfw上测试，精度最高达到77.04%</u>\n\n# step6:Center face+dropout+finetune on softmax\n\n在msclean测试集上达到93.53% \n\n| Model                             | PCA_Size | Threshold | Score  |\n| :-------------------------------- | -------- | --------- | ------ |\n| dropcenter                        | 168      | 0.64      | 99.42% |\n| dropcenter_mirror                 | 136      | 0.64      | 99.38% |\n| dropcenter +dropcenter_mirror+Min | 128      | 0.64      | 99.45% |\n| dropcenter+dropcenter_mirror+ Add | 128      | 0.64      | 99.45% |\n| center +dropcenter +Min           | 400      | 0.64      | 99.40% |\n| centermirror+dropcenter+Min       | 128      | 0.64      | 99.45% |\n| centermirror+dropcenter+Add       | 160      | 0.64      | 99.43% |\n| centermirror+dropcenter+Max       | 160      | 0.64      | 99.43% |\n| centermirror+dropcenter+Concate   | 192      | 0.65      | 99.47% |\n\n| Model                                    | PCA Size | Threshold | Score  |\n| ---------------------------------------- | -------- | --------- | ------ |\n| center_min_mirror+dropcenter+Concate     | 192      | 0.65      | 99.47% |\n| center_min_mirror+dropcenter+Min         | 128      | 0.65      | 99.42% |\n| center_min_mirror+dropcenter+Add         | 136      | 0.64      | 99.47% |\n| eye_model                                | 160      | 0.57      | 77.04% |\n| eyemodel+center+Con                      | 208      | 0.66      | 74.80% |\n| 三模型                                      |          |           |        |\n| center+center_min_mirror+dropoutcenter+Concate | 128      | 0.65      | 99.43% |\n| center+softmax+dropoutcenter+Concate     | 168      | 0.66      | 99.43% |\n| center+softmax+dropoutcenter+Add         | 496      | 0.65      | 99.42% |\n\n\n\n\n\n# step7:balance\n\n## step7.1:减小过采样的数量，防止过拟合\n\n对/home/yf/data/clean.txt中每种类别进行统计各有多少个数：\n\n| 每种类别包含图片张数 | 类别数   | 每种类别包含图片张数 | 类别数   |\n| ---------- | ----- | ---------- | ----- |\n| <10        | 1213  | 10~20      | 11617 |\n| 20~30      | 10868 | 30~40      | 9692  |\n| 40~50      | 9020  | 50~60      | 8426  |\n| 60~70      | 8443  | 70~80      | 8783  |\n| 80~90      | 8762  | 90~100     | 7317  |\n| 100~110    | 4277  | 110~120    | 1753  |\n| 120~130    | 354   |            |       |\n\n类别总数共90525。由上图可知，类别严重不均衡，之前处理类别不均衡的方法主要是欠抽样和过抽样结合，对于多数类样本丢弃一部分样本，对于少数类样本复制生成，最后的训练数据分布如下：\n\n| 每种类别包含图片张数 | 类别数   | 每种类别包含图片张数 | 类别数   |\n| ---------- | ----- | ---------- | ----- |\n| 70~80      | 3673  | 80~90      | 19068 |\n| 90~100     | 27069 | 100~110    | 40715 |\n\n由于少数类占了大多数，但是重复太多，可能导致过拟合问题，于是将每个类别的图片张数减去30，重新生成balance的训练数据，并训练模型。\n\n<u>迭代17万次后，msdata测试集上准确率达到92.28%，loss=0.27</u>\n\n<u>lfw上精度为99.18%</u>\n\n<u>mirror:99.27%</u>\n\n<u>add:99.27%</u>\n\n## step7.2:EasyEmsemble法均衡类别\n\nstep7.1的方法属于欠抽样和过抽样结合：\n\n- 对于欠抽样算法，将多数类样本删除有可能会导致分类器**丢失有关多数类的重要信息**。\n- 对于过抽样算法，虽然只是简单地将复制后的数据添加到原始数据集中，且某些样本的多个实例都是“**并列的**”，但这样也可能会导致分类器学习出现**过拟合现象**，对于同一个样本的多个复本产生多个规则条例，这就使得**规则过于具体化**；虽然在这种情况下，分类器的训练精度会很高，但在位置样本的分类性能就会非常不理想。\n\n**EasyEnsemble 核心思想是：**\n\n- 首先通过从多数类中**独立随机**抽取出若干子集\n\n- 将每个子集与少数类数据**联合**起来**训练**生成多个基分类器\n\n- 最终将这些基分类器**组合形成**一个集成学习系统\n\n  设立一个阈值50，对于类别样本数超过50的，将其分写到两个不同的文件；对于类别样本数不超过50的，利用过采样进行增添，所以最终得到两个有交集的训练集A,B，两个训练集的样本数都是\n\n  90525*50=4526250\n\n<u>训练两个model，然后提取特征，对特征进行融合。</u>\n\n\n\n| Model(acc/loss)                          | Pca Size | Threshold | Score  |\n| ---------------------------------------- | -------- | --------- | ------ |\n| model1(90.75%/0.41)                      | 176      | 0.64      | 99.05% |\n| model1(92.14%/0.26)                      | 200      | 0.62      | 99.28% |\n| model1(92.14%/0.26) Mirror               | 280      | 0.63      | 99.32% |\n| model1(92.14%/0.26) Add Mirror           | 128      | 0.65      | 99.30% |\n| model2(92.56%/0.41)                      | 128      | 0.64      | 99.27% |\n| model2(92.56%/0.41) Mirror               | 192      | 0.63      | 99.35% |\n| model2(92.56%/0.41) Add Mirror           | 192      | 0.64      | 99.32% |\n| model1 add model2                        | 152      | 0.64      | 99.33% |\n| model1 mirror add model2 mirror          | 136      | 0.65      | 99.37% |\n| model1 add model2 mirror                 | 152      | 0.64      | 99.37% |\n| model1_add_mirror add model2_add_mirror  | 152      | 0.64      | 99.38% |\n| model1_add_mirror concate model2_add_mirror | 128      | 0.65      | 99.32% |\n| model1 mirror min model2 mirror          | 168      | 0.64      | 99.37% |\n\n\n\n# step8:UMDFaces\n\n对UMDFaces数据集进行人脸对齐处理\n\nbatch1:175,534(3554类)\n\nbatch2:115,126(2590类)\n\nbatch3:77,228(2133类)\n\nframes:3,735,475(3106类)\n\n提取4个数据集的类别名称，经过处理分析后发现frames的类别属于batch1类别的子集，将3个batch与frames的数据集整合到一个数据集下，因为当静态图片和视频帧进行结合后训练的模型往往既能兼顾个体之间的差异（静态图片特征）也能学习到同一个个体的姿态变化（视频帧特征），要注意的一点就是对于frames和batch1中同一个类别的要放在一个目录下，并重新生成类别标签。\n\n数据总量:4103363(8276个类别)\n\n数据整理已经完成，接下来是在这个数据集上进行metric learning的训练。\n\ntrain:3286012\nval:817351\n\n# step9:Megaface测试\n\n| Model                           | Dataset            | Score(Megaface/LFW) |\n| ------------------------------- | ------------------ | ------------------- |\n| center-face                     | FaceScrub Set1/LFW | 67.32%/99.42%       |\n| balance-reduced                 | FaceScrub Set1/LFW | 70.99%/99.18%       |\n| easyensemble                    | FaceScrub Set1     | 73.91%/99.33%       |\n| easyensemble  concat addmirror  | FaceScrub Set1     | 74.21%/99.37%       |\n| balance-cent-soft               | FaceScrub Set1/LFW | 74.47%/99.33%       |\n| balance-cent-soft concat mirror | FaceScrub Set1     | 75.65%              |\n\n## step9.1:Megaface测试（续）\n\n| Model                                    | Dataset                 | Score(Megaface/LFW) |\n| ---------------------------------------- | ----------------------- | ------------------- |\n| Dropout_center Concat mirror             | FaceScrub Set1/LFW      | 69.34%/99.47%       |\n| normface easyensemble model1 Concart mirror | FaceScrub Set1/LFW      | 70.32%              |\n| normface easyensemble 2models Concat mirror | FaceScrub Set1          | 70.49%              |\n| balance concat mirror                    | FaceScrub(matlab_mtcnn) | 78.84%              |\n| easyensemble concat addmirror            | FaceScrub(matlab_mtcnn) | 75.92%              |\n| jitter_center_iter_190000 concat mirror  | FaceScrub(matlab_mtcnn) | 77.69%              |\n| jitter_softmax_iter_180000 concat mirror | FaceScrub(matlab_mtcnn) | 83.13%              |\n| jitter_softmax_iter_180000 only mirror   | FaceScrub(matlab_mtcnn) | 82.08%              |\n| jitter_softmax_iter_180000 add mirror    | FaceScrub(matlab_mtcnn) | 82.87%              |\n| jitter_softmax_iter_184000 concat mirror | FaceScrub(matlab_mtcnn) | 83.16%              |\n| jitter_softmax_iter_180000 concat mirror | FaceScrub(python_mtcnn) | 78.33%              |\n| jitter_softmax_iter_184000 concat mirror(matlab align) | FaceScrub(matlab_mtcnn) | 79.05%              |\n| normface_jitter_iter_124000 concat mirror(python align) | FaceScrub(python_mtcnn) | 76.50%              |\n| normface_jitter_iter124000 cancat mirror(python align) | FaceScrub(matlab_mtcnn) | 78.92%              |\n\n\n\n# step 10:MTCNN(matlab)人脸检测及对齐\n\n## step 10.1：对齐Megaface和FaceScrub\n\n主要是Megaface数据集（1028062张）,FaceScrub数据集(91712张)，其中FaceScrub数据集中通过mtcnn（/home/yf/align/align_megaface.m）检测到的有89751张，剩余的1961张需要利用数据集中提供的3个关键点进行对齐，首先需要获取未检测到的图片的路径，然后利用python 脚本(/home/yf/megaface/devkit/templatelists/analysis/analysis_json.py)解析对应的存储该图片中人脸关键点的json文件，最后在利用matlab脚本(/home/yf/align/for_not_detect/align_megaface.m)进行批量对齐。Megaface数据集中未检测到的数据集同样处理。\n\n**问题**：\n\n在进行了41万次对齐后，出现了imread的错误，然后将从目录读取路径改成了从存储图片路径的文件中(/home/yf/megaface/tests/MegaFace_align_list_image.txt)直接获取路径，并输出每次进行处理的文件名，重新进行对齐操作，然后重现了这个错误，最后比对MegaFace_align_list_image.txt的下一张图片，发现有张图片是输入为空的。\n\n## step 10.2:对齐msceleb数据\n\n重新对齐msceleb数据集用于训练。\n\n# step 11:Normface训练\n\nNormface(paper:[NormFace: L2 Hypersphere Embedding for Face Verification](https://arxiv.org/pdf/1704.06369.pdf))\n\n## step 11.1:训练EasyEnsemble模型\n\nmodel1在测试集上的准确率为92.88%，model2在测试集上的准确率为92.85%。\n\n暂时只测了单个的model1 concate mirror在Megaface(还是原始python版mtcnn对齐的)上的准确率只有70.32%。下周继续测试两个模型的效果。\n\n## step 11.2:训练Balance模型\n\n刚生成完训练的数据集，下周开始训练。\n\n# step 12:Image Jitter\n\n对图片增加随机扰动，包括缩放、角度变换、镜像操作，主要还是msceleb数据集(/home/yf/data/msclean)上进行，由于该数据集类别不均衡，所以对于样本数较少的类别可以采用这种办法增加样本容量。最终将每个类别的样本数控制在80~160之间。\n\n10240892\n\ntrain:9257534\n\nval:983342\n\n正在生成训练的数据集lmdb。\n\njitter_center_iter_190000 concat mirror  FaceScrub(matlab_mtcnn)  77.69%  \n\njitter_softmax_iter_180000 concat mirror  FaceScrub(matlab_mtcnn)  83.18%\n\njitter_softmax_iter_180000 only mirror  FaceScrub(matlab_mtcnn)  82.08%\n\njitter_softmax_iter_180000 add mirror  FaceScrub(matlab_mtcnn)  82.87%\n\njitter_softmax_iter_184000 concat mirror  FaceScrub(matlab_mtcnn)  83.16%\n\n\n\n# step 13:Gender test\n\n### 1.0\n\nVGG16在lfw上准确率90.04%，在imdb(15590测试样本)上准确率90.92%\n\nmodel training:female(69847),male(86061)\n\ntrain:124728\n\nval:15590\n\ntest:15590\n\n### 2.0\n\n网络：AlexNet 在清理后的数据集上，迭代29500次后，训练准确率为98.16%，loss=0.55\n\n在测试集上达到98.11%（15127/15418)\n\n\n\n\n\n# Todo1\n\n- [x] Create umdfaces-->lmdb\n- [x] EasyEnsemble train and test\n- [x] Use matcaffe for metric learning\n- [ ] Megaface test\n      - - [x] center face\n        - [x] balance-cent-soft\n        - [x] reduced\n        - [x] mirror or concatenate\n        - [x] EasyEnsemble\n\n\n- [x] Paper reading:One-shot face recognition by promoting underrepresented classes\n\n      ​\n\n\n# Todo2\n\n- [ ] jitter model training(softmax first)\n\n- [ ] balance model retrain on normface(include center loss)\n\n- [ ] aligned by matlab_mtcnn megaface(balance model 75.65% version)\n\n- [ ] gender classfication model training\n\n      - check the dataset (detect and crop by matlab_mtcnn)\n    \n      - generate lmdb\n    \n      - choose a model(ResNet?)\n    \n        ​\n\n### 参考链接\n\n[happynear-face-verification](https://github.com/happynear/FaceVerification)\n\n[dlib-jitter](https://github.com/davisking/dlib/blob/cbd187fb6109d21406f6a76bb0e9aa0689b1e54a/examples/dnn_face_recognition_ex.cpp)\n\n[dlib-face-verification-blog](http://blog.dlib.net)\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/待办及进度.md","raw":"---\ntitle: 待办及进度\ndate: 2017-06-02 15:15:46\ncategories: 实验记录\ntags:\ndescription: 人脸识别实验记录\n---\n# step1:Mirror face相关\n\n| Model              | PCA Size | Threshold | Score  |\n| ------------------ | -------- | --------- | ------ |\n| Mirror             | 192      | 0.64      | 99.42% |\n| Mirror Concat      | 192      | 0.65      | 99.42% |\n| Mirror Add/Average | 184      | 0.64      | 99.47% |\n| Mirror Max         | 144      | 0.65      | 99.43% |\n| Mirror Min         | 168      | 0.65      | 99.48% |\n| Mirror Avg+min     | 168      | 0.65      | 99.45% |\n\n# step2:单眼Patch model\n\n| eye_width | num      | eye_width | num    |\n| --------- | -------- | --------- | ------ |\n| 0~10      | 14       | 90~100    | 8,5674 |\n| 10~20     | 3329     | 100~110   | 2,9481 |\n| 20~30     | 21,3675  | 110~120   | 9051   |\n| 30~40     | 45,1416  | 120~130   | 2894   |\n| 40~50     | 49,1913  | 130~140   | 932    |\n| 50~60     | 72,8911  | 140~150   | 279    |\n| 60~70     | 137,9232 | 150~160   | 86     |\n| 70~80     | 128,7442 | 160~170   | 14     |\n| 80~90     | 30,9026  | 170~180   | 6      |\n\n## 问题\n\n1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y...],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)*0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5*eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句\n```\nif eye_width*0.5>eye_left_x:\n            eye_width=eye_left_x*2\n```\n导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张\n\n重新生成眼睛宽度估算文件，其分布如下\n\n| eye_width | num      | eye_width | num  |\n| --------- | -------- | --------- | ---- |\n| 0~10      | 1,8975   | 100~110   | 98   |\n| 10~20     | 71,3760  | 110~120   | 17   |\n| 20~30     | 126,4102 | 120~130   | 3    |\n| 30~40     | 226,4348 | 130~140   | 0    |\n| 40~50     | 59,3351  | 140~150   | 0    |\n| 50~60     | 10,6502  | 150~160   | 0    |\n| 60~70     | 2,4588   | 160~170   | 1    |\n| 70~80     | 5592     | 170~180   | 0    |\n| 80~90     | 1588     | 180~190   | 8    |\n| 90~100    | 421      | 190~200   | 21   |\n\n\n\n2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可\n3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。\n最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526\n\n<u>17个小时，迭代12万次，26.5%的准确率，loss=5.5。</u>\n\n\n\n# step3:双眼patch model\n\n生成双眼宽度估算文件，其分布如下\n\n| eye_width | num      | eye_width | num     | eye_width | num  |\n| --------- | -------- | --------- | ------- | --------- | ---- |\n| 0~10      | 30       | 100~110   | 18,8771 | 200~210   | 383  |\n| 10~20     | 5888     | 110~120   | 8,8508  | 210~220   | 196  |\n| 20~30     | 11,4595  | 120~130   | 4,6243  | 220~230   | 100  |\n| 30~40     | 35,5451  | 130~140   | 2,4982  | 230~240   | 52   |\n| 40~50     | 42,6729  | 140~150   | 1,2749  | 240~250   | 33   |\n| 50~60     | 49,1312  | 150~160   | 6714    | 250~260   | 15   |\n| 60~70     | 69,0394  | 160~170   | 3413    | 260~270   | 3    |\n| 70~80     | 104,9353 | 170~180   | 1909    | 270~280   | 2    |\n| 80~90     | 100,6618 | 180~190   | 1091    | 280~290   | 0    |\n| 90~100    | 47,7219  | 190~200   | 622     | 290~300   | 0    |\n\n筛选crop后，宽度在20~130区间的图片，共90523个类别，4932655张。\n\ntrain.txt：3982004 \n\nval.txt：950661 \n\n<u>18万次迭代之后，准确率只有66%左右。</u>\n\n\n\n# step4:crop对齐后的图片的眼睛，训练单眼模型\n\n数据集大小：5044507(90525个类)（\"/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt\"，\"/home/yf/data/msclean\"）\n\n       \"ref_points\": [\n       \t\t30.2946, 51.6963, \n            65.5318, 51.5014, \n            48.0252, 71.7366,\n            33.5493, 92.3655, \n            62.7299, 92.2041\n        ]\n    \n    \n         eye_width=(ref_points[2]-ref_points[0])*0.8\n            eye_height=eye_width\n            x1=ref_points[0]-0.5*eye_width=16\n            x2=ref_points[0]+0.5*eye_width=44\n            y1=ref_points[1]-0.5*eye_height=37\n            y2=ref_points[1]+0.5*eye_height=65\ntrain:4071324 张\n\nval:973183张\n\n\n\n# step5:crop对齐后的图片的眼睛，训练双眼模型\n\n数据集大小：5044507(90525个类)（\"/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt\"，\"/home/yf/data/msclean\"）\n\n\n\n       \"ref_points\": [\n       \t30.2946, 51.6963, \n        65.5318, 51.5014, \n        48.0252, 71.7366,\n        33.5493, 92.3655, \n        62.7299, 92.2041\n    ]\n         eye_width=(ref_points[2]-ref_points[0])*0.8\n        eye_height=eye_width\n        x1=ref_points[0]-0.5*eye_width=16\n        x2=ref_points[2]+0.5*eye_width=79\n        y1=ref_points[1]-0.5*eye_height=37\n        y2=ref_points[1]+0.5*eye_height=65\n<u>迭代16万次，精度为72.73%，loss=2.52</u>\n\n<u>在lfw上测试，精度最高达到77.04%</u>\n\n# step6:Center face+dropout+finetune on softmax\n\n在msclean测试集上达到93.53% \n\n| Model                             | PCA_Size | Threshold | Score  |\n| :-------------------------------- | -------- | --------- | ------ |\n| dropcenter                        | 168      | 0.64      | 99.42% |\n| dropcenter_mirror                 | 136      | 0.64      | 99.38% |\n| dropcenter +dropcenter_mirror+Min | 128      | 0.64      | 99.45% |\n| dropcenter+dropcenter_mirror+ Add | 128      | 0.64      | 99.45% |\n| center +dropcenter +Min           | 400      | 0.64      | 99.40% |\n| centermirror+dropcenter+Min       | 128      | 0.64      | 99.45% |\n| centermirror+dropcenter+Add       | 160      | 0.64      | 99.43% |\n| centermirror+dropcenter+Max       | 160      | 0.64      | 99.43% |\n| centermirror+dropcenter+Concate   | 192      | 0.65      | 99.47% |\n\n| Model                                    | PCA Size | Threshold | Score  |\n| ---------------------------------------- | -------- | --------- | ------ |\n| center_min_mirror+dropcenter+Concate     | 192      | 0.65      | 99.47% |\n| center_min_mirror+dropcenter+Min         | 128      | 0.65      | 99.42% |\n| center_min_mirror+dropcenter+Add         | 136      | 0.64      | 99.47% |\n| eye_model                                | 160      | 0.57      | 77.04% |\n| eyemodel+center+Con                      | 208      | 0.66      | 74.80% |\n| 三模型                                      |          |           |        |\n| center+center_min_mirror+dropoutcenter+Concate | 128      | 0.65      | 99.43% |\n| center+softmax+dropoutcenter+Concate     | 168      | 0.66      | 99.43% |\n| center+softmax+dropoutcenter+Add         | 496      | 0.65      | 99.42% |\n\n\n\n\n\n# step7:balance\n\n## step7.1:减小过采样的数量，防止过拟合\n\n对/home/yf/data/clean.txt中每种类别进行统计各有多少个数：\n\n| 每种类别包含图片张数 | 类别数   | 每种类别包含图片张数 | 类别数   |\n| ---------- | ----- | ---------- | ----- |\n| <10        | 1213  | 10~20      | 11617 |\n| 20~30      | 10868 | 30~40      | 9692  |\n| 40~50      | 9020  | 50~60      | 8426  |\n| 60~70      | 8443  | 70~80      | 8783  |\n| 80~90      | 8762  | 90~100     | 7317  |\n| 100~110    | 4277  | 110~120    | 1753  |\n| 120~130    | 354   |            |       |\n\n类别总数共90525。由上图可知，类别严重不均衡，之前处理类别不均衡的方法主要是欠抽样和过抽样结合，对于多数类样本丢弃一部分样本，对于少数类样本复制生成，最后的训练数据分布如下：\n\n| 每种类别包含图片张数 | 类别数   | 每种类别包含图片张数 | 类别数   |\n| ---------- | ----- | ---------- | ----- |\n| 70~80      | 3673  | 80~90      | 19068 |\n| 90~100     | 27069 | 100~110    | 40715 |\n\n由于少数类占了大多数，但是重复太多，可能导致过拟合问题，于是将每个类别的图片张数减去30，重新生成balance的训练数据，并训练模型。\n\n<u>迭代17万次后，msdata测试集上准确率达到92.28%，loss=0.27</u>\n\n<u>lfw上精度为99.18%</u>\n\n<u>mirror:99.27%</u>\n\n<u>add:99.27%</u>\n\n## step7.2:EasyEmsemble法均衡类别\n\nstep7.1的方法属于欠抽样和过抽样结合：\n\n- 对于欠抽样算法，将多数类样本删除有可能会导致分类器**丢失有关多数类的重要信息**。\n- 对于过抽样算法，虽然只是简单地将复制后的数据添加到原始数据集中，且某些样本的多个实例都是“**并列的**”，但这样也可能会导致分类器学习出现**过拟合现象**，对于同一个样本的多个复本产生多个规则条例，这就使得**规则过于具体化**；虽然在这种情况下，分类器的训练精度会很高，但在位置样本的分类性能就会非常不理想。\n\n**EasyEnsemble 核心思想是：**\n\n- 首先通过从多数类中**独立随机**抽取出若干子集\n\n- 将每个子集与少数类数据**联合**起来**训练**生成多个基分类器\n\n- 最终将这些基分类器**组合形成**一个集成学习系统\n\n  设立一个阈值50，对于类别样本数超过50的，将其分写到两个不同的文件；对于类别样本数不超过50的，利用过采样进行增添，所以最终得到两个有交集的训练集A,B，两个训练集的样本数都是\n\n  90525*50=4526250\n\n<u>训练两个model，然后提取特征，对特征进行融合。</u>\n\n\n\n| Model(acc/loss)                          | Pca Size | Threshold | Score  |\n| ---------------------------------------- | -------- | --------- | ------ |\n| model1(90.75%/0.41)                      | 176      | 0.64      | 99.05% |\n| model1(92.14%/0.26)                      | 200      | 0.62      | 99.28% |\n| model1(92.14%/0.26) Mirror               | 280      | 0.63      | 99.32% |\n| model1(92.14%/0.26) Add Mirror           | 128      | 0.65      | 99.30% |\n| model2(92.56%/0.41)                      | 128      | 0.64      | 99.27% |\n| model2(92.56%/0.41) Mirror               | 192      | 0.63      | 99.35% |\n| model2(92.56%/0.41) Add Mirror           | 192      | 0.64      | 99.32% |\n| model1 add model2                        | 152      | 0.64      | 99.33% |\n| model1 mirror add model2 mirror          | 136      | 0.65      | 99.37% |\n| model1 add model2 mirror                 | 152      | 0.64      | 99.37% |\n| model1_add_mirror add model2_add_mirror  | 152      | 0.64      | 99.38% |\n| model1_add_mirror concate model2_add_mirror | 128      | 0.65      | 99.32% |\n| model1 mirror min model2 mirror          | 168      | 0.64      | 99.37% |\n\n\n\n# step8:UMDFaces\n\n对UMDFaces数据集进行人脸对齐处理\n\nbatch1:175,534(3554类)\n\nbatch2:115,126(2590类)\n\nbatch3:77,228(2133类)\n\nframes:3,735,475(3106类)\n\n提取4个数据集的类别名称，经过处理分析后发现frames的类别属于batch1类别的子集，将3个batch与frames的数据集整合到一个数据集下，因为当静态图片和视频帧进行结合后训练的模型往往既能兼顾个体之间的差异（静态图片特征）也能学习到同一个个体的姿态变化（视频帧特征），要注意的一点就是对于frames和batch1中同一个类别的要放在一个目录下，并重新生成类别标签。\n\n数据总量:4103363(8276个类别)\n\n数据整理已经完成，接下来是在这个数据集上进行metric learning的训练。\n\ntrain:3286012\nval:817351\n\n# step9:Megaface测试\n\n| Model                           | Dataset            | Score(Megaface/LFW) |\n| ------------------------------- | ------------------ | ------------------- |\n| center-face                     | FaceScrub Set1/LFW | 67.32%/99.42%       |\n| balance-reduced                 | FaceScrub Set1/LFW | 70.99%/99.18%       |\n| easyensemble                    | FaceScrub Set1     | 73.91%/99.33%       |\n| easyensemble  concat addmirror  | FaceScrub Set1     | 74.21%/99.37%       |\n| balance-cent-soft               | FaceScrub Set1/LFW | 74.47%/99.33%       |\n| balance-cent-soft concat mirror | FaceScrub Set1     | 75.65%              |\n\n## step9.1:Megaface测试（续）\n\n| Model                                    | Dataset                 | Score(Megaface/LFW) |\n| ---------------------------------------- | ----------------------- | ------------------- |\n| Dropout_center Concat mirror             | FaceScrub Set1/LFW      | 69.34%/99.47%       |\n| normface easyensemble model1 Concart mirror | FaceScrub Set1/LFW      | 70.32%              |\n| normface easyensemble 2models Concat mirror | FaceScrub Set1          | 70.49%              |\n| balance concat mirror                    | FaceScrub(matlab_mtcnn) | 78.84%              |\n| easyensemble concat addmirror            | FaceScrub(matlab_mtcnn) | 75.92%              |\n| jitter_center_iter_190000 concat mirror  | FaceScrub(matlab_mtcnn) | 77.69%              |\n| jitter_softmax_iter_180000 concat mirror | FaceScrub(matlab_mtcnn) | 83.13%              |\n| jitter_softmax_iter_180000 only mirror   | FaceScrub(matlab_mtcnn) | 82.08%              |\n| jitter_softmax_iter_180000 add mirror    | FaceScrub(matlab_mtcnn) | 82.87%              |\n| jitter_softmax_iter_184000 concat mirror | FaceScrub(matlab_mtcnn) | 83.16%              |\n| jitter_softmax_iter_180000 concat mirror | FaceScrub(python_mtcnn) | 78.33%              |\n| jitter_softmax_iter_184000 concat mirror(matlab align) | FaceScrub(matlab_mtcnn) | 79.05%              |\n| normface_jitter_iter_124000 concat mirror(python align) | FaceScrub(python_mtcnn) | 76.50%              |\n| normface_jitter_iter124000 cancat mirror(python align) | FaceScrub(matlab_mtcnn) | 78.92%              |\n\n\n\n# step 10:MTCNN(matlab)人脸检测及对齐\n\n## step 10.1：对齐Megaface和FaceScrub\n\n主要是Megaface数据集（1028062张）,FaceScrub数据集(91712张)，其中FaceScrub数据集中通过mtcnn（/home/yf/align/align_megaface.m）检测到的有89751张，剩余的1961张需要利用数据集中提供的3个关键点进行对齐，首先需要获取未检测到的图片的路径，然后利用python 脚本(/home/yf/megaface/devkit/templatelists/analysis/analysis_json.py)解析对应的存储该图片中人脸关键点的json文件，最后在利用matlab脚本(/home/yf/align/for_not_detect/align_megaface.m)进行批量对齐。Megaface数据集中未检测到的数据集同样处理。\n\n**问题**：\n\n在进行了41万次对齐后，出现了imread的错误，然后将从目录读取路径改成了从存储图片路径的文件中(/home/yf/megaface/tests/MegaFace_align_list_image.txt)直接获取路径，并输出每次进行处理的文件名，重新进行对齐操作，然后重现了这个错误，最后比对MegaFace_align_list_image.txt的下一张图片，发现有张图片是输入为空的。\n\n## step 10.2:对齐msceleb数据\n\n重新对齐msceleb数据集用于训练。\n\n# step 11:Normface训练\n\nNormface(paper:[NormFace: L2 Hypersphere Embedding for Face Verification](https://arxiv.org/pdf/1704.06369.pdf))\n\n## step 11.1:训练EasyEnsemble模型\n\nmodel1在测试集上的准确率为92.88%，model2在测试集上的准确率为92.85%。\n\n暂时只测了单个的model1 concate mirror在Megaface(还是原始python版mtcnn对齐的)上的准确率只有70.32%。下周继续测试两个模型的效果。\n\n## step 11.2:训练Balance模型\n\n刚生成完训练的数据集，下周开始训练。\n\n# step 12:Image Jitter\n\n对图片增加随机扰动，包括缩放、角度变换、镜像操作，主要还是msceleb数据集(/home/yf/data/msclean)上进行，由于该数据集类别不均衡，所以对于样本数较少的类别可以采用这种办法增加样本容量。最终将每个类别的样本数控制在80~160之间。\n\n10240892\n\ntrain:9257534\n\nval:983342\n\n正在生成训练的数据集lmdb。\n\njitter_center_iter_190000 concat mirror  FaceScrub(matlab_mtcnn)  77.69%  \n\njitter_softmax_iter_180000 concat mirror  FaceScrub(matlab_mtcnn)  83.18%\n\njitter_softmax_iter_180000 only mirror  FaceScrub(matlab_mtcnn)  82.08%\n\njitter_softmax_iter_180000 add mirror  FaceScrub(matlab_mtcnn)  82.87%\n\njitter_softmax_iter_184000 concat mirror  FaceScrub(matlab_mtcnn)  83.16%\n\n\n\n# step 13:Gender test\n\n### 1.0\n\nVGG16在lfw上准确率90.04%，在imdb(15590测试样本)上准确率90.92%\n\nmodel training:female(69847),male(86061)\n\ntrain:124728\n\nval:15590\n\ntest:15590\n\n### 2.0\n\n网络：AlexNet 在清理后的数据集上，迭代29500次后，训练准确率为98.16%，loss=0.55\n\n在测试集上达到98.11%（15127/15418)\n\n\n\n\n\n# Todo1\n\n- [x] Create umdfaces-->lmdb\n- [x] EasyEnsemble train and test\n- [x] Use matcaffe for metric learning\n- [ ] Megaface test\n      - - [x] center face\n        - [x] balance-cent-soft\n        - [x] reduced\n        - [x] mirror or concatenate\n        - [x] EasyEnsemble\n\n\n- [x] Paper reading:One-shot face recognition by promoting underrepresented classes\n\n      ​\n\n\n# Todo2\n\n- [ ] jitter model training(softmax first)\n\n- [ ] balance model retrain on normface(include center loss)\n\n- [ ] aligned by matlab_mtcnn megaface(balance model 75.65% version)\n\n- [ ] gender classfication model training\n\n      - check the dataset (detect and crop by matlab_mtcnn)\n    \n      - generate lmdb\n    \n      - choose a model(ResNet?)\n    \n        ​\n\n### 参考链接\n\n[happynear-face-verification](https://github.com/happynear/FaceVerification)\n\n[dlib-jitter](https://github.com/davisking/dlib/blob/cbd187fb6109d21406f6a76bb0e9aa0689b1e54a/examples/dnn_face_recognition_ex.cpp)\n\n[dlib-face-verification-blog](http://blog.dlib.net)\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"待办及进度","published":1,"updated":"2018-12-05T09:23:48.239Z","_id":"cjpaxs0le000ks0vwpud9zaxz","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"step1-Mirror-face相关\"><a href=\"#step1-Mirror-face相关\" class=\"headerlink\" title=\"step1:Mirror face相关\"></a>step1:Mirror face相关</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>PCA Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Mirror</td>\n<td>192</td>\n<td>0.64</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Concat</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Add/Average</td>\n<td>184</td>\n<td>0.64</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>Mirror Max</td>\n<td>144</td>\n<td>0.65</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>Mirror Min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.48%</td>\n</tr>\n<tr>\n<td>Mirror Avg+min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.45%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step2-单眼Patch-model\"><a href=\"#step2-单眼Patch-model\" class=\"headerlink\" title=\"step2:单眼Patch model\"></a>step2:单眼Patch model</h1><table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>14</td>\n<td>90~100</td>\n<td>8,5674</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>3329</td>\n<td>100~110</td>\n<td>2,9481</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>21,3675</td>\n<td>110~120</td>\n<td>9051</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>45,1416</td>\n<td>120~130</td>\n<td>2894</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>49,1913</td>\n<td>130~140</td>\n<td>932</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>72,8911</td>\n<td>140~150</td>\n<td>279</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>137,9232</td>\n<td>150~160</td>\n<td>86</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>128,7442</td>\n<td>160~170</td>\n<td>14</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>30,9026</td>\n<td>170~180</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y…],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)<em>0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5</em>eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">if eye_width*0.5&gt;eye_left_x:</div><div class=\"line\">            eye_width=eye_left_x*2</div></pre></td></tr></table></figure></p>\n<p>导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张</p>\n<p>重新生成眼睛宽度估算文件，其分布如下</p>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>1,8975</td>\n<td>100~110</td>\n<td>98</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>71,3760</td>\n<td>110~120</td>\n<td>17</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>126,4102</td>\n<td>120~130</td>\n<td>3</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>226,4348</td>\n<td>130~140</td>\n<td>0</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>59,3351</td>\n<td>140~150</td>\n<td>0</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>10,6502</td>\n<td>150~160</td>\n<td>0</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>2,4588</td>\n<td>160~170</td>\n<td>1</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>5592</td>\n<td>170~180</td>\n<td>0</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>1588</td>\n<td>180~190</td>\n<td>8</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>421</td>\n<td>190~200</td>\n<td>21</td>\n</tr>\n</tbody>\n</table>\n<p>2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可<br>3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。<br>最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526</p>\n<p><u>17个小时，迭代12万次，26.5%的准确率，loss=5.5。</u></p>\n<h1 id=\"step3-双眼patch-model\"><a href=\"#step3-双眼patch-model\" class=\"headerlink\" title=\"step3:双眼patch model\"></a>step3:双眼patch model</h1><p>生成双眼宽度估算文件，其分布如下</p>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>30</td>\n<td>100~110</td>\n<td>18,8771</td>\n<td>200~210</td>\n<td>383</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>5888</td>\n<td>110~120</td>\n<td>8,8508</td>\n<td>210~220</td>\n<td>196</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>11,4595</td>\n<td>120~130</td>\n<td>4,6243</td>\n<td>220~230</td>\n<td>100</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>35,5451</td>\n<td>130~140</td>\n<td>2,4982</td>\n<td>230~240</td>\n<td>52</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>42,6729</td>\n<td>140~150</td>\n<td>1,2749</td>\n<td>240~250</td>\n<td>33</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>49,1312</td>\n<td>150~160</td>\n<td>6714</td>\n<td>250~260</td>\n<td>15</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>69,0394</td>\n<td>160~170</td>\n<td>3413</td>\n<td>260~270</td>\n<td>3</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>104,9353</td>\n<td>170~180</td>\n<td>1909</td>\n<td>270~280</td>\n<td>2</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>100,6618</td>\n<td>180~190</td>\n<td>1091</td>\n<td>280~290</td>\n<td>0</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>47,7219</td>\n<td>190~200</td>\n<td>622</td>\n<td>290~300</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<p>筛选crop后，宽度在20~130区间的图片，共90523个类别，4932655张。</p>\n<p>train.txt：3982004 </p>\n<p>val.txt：950661 </p>\n<p><u>18万次迭代之后，准确率只有66%左右。</u></p>\n<h1 id=\"step4-crop对齐后的图片的眼睛，训练单眼模型\"><a href=\"#step4-crop对齐后的图片的眼睛，训练单眼模型\" class=\"headerlink\" title=\"step4:crop对齐后的图片的眼睛，训练单眼模型\"></a>step4:crop对齐后的图片的眼睛，训练单眼模型</h1><p>数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”）</p>\n<pre><code>&quot;ref_points&quot;: [\n        30.2946, 51.6963, \n     65.5318, 51.5014, \n     48.0252, 71.7366,\n     33.5493, 92.3655, \n     62.7299, 92.2041\n ]\n\n\n  eye_width=(ref_points[2]-ref_points[0])*0.8\n     eye_height=eye_width\n     x1=ref_points[0]-0.5*eye_width=16\n     x2=ref_points[0]+0.5*eye_width=44\n     y1=ref_points[1]-0.5*eye_height=37\n     y2=ref_points[1]+0.5*eye_height=65\n</code></pre><p>train:4071324 张</p>\n<p>val:973183张</p>\n<h1 id=\"step5-crop对齐后的图片的眼睛，训练双眼模型\"><a href=\"#step5-crop对齐后的图片的眼睛，训练双眼模型\" class=\"headerlink\" title=\"step5:crop对齐后的图片的眼睛，训练双眼模型\"></a>step5:crop对齐后的图片的眼睛，训练双眼模型</h1><p>数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”）</p>\n<pre><code>   &quot;ref_points&quot;: [\n       30.2946, 51.6963, \n    65.5318, 51.5014, \n    48.0252, 71.7366,\n    33.5493, 92.3655, \n    62.7299, 92.2041\n]\n     eye_width=(ref_points[2]-ref_points[0])*0.8\n    eye_height=eye_width\n    x1=ref_points[0]-0.5*eye_width=16\n    x2=ref_points[2]+0.5*eye_width=79\n    y1=ref_points[1]-0.5*eye_height=37\n    y2=ref_points[1]+0.5*eye_height=65\n</code></pre><p><u>迭代16万次，精度为72.73%，loss=2.52</u></p>\n<p><u>在lfw上测试，精度最高达到77.04%</u></p>\n<h1 id=\"step6-Center-face-dropout-finetune-on-softmax\"><a href=\"#step6-Center-face-dropout-finetune-on-softmax\" class=\"headerlink\" title=\"step6:Center face+dropout+finetune on softmax\"></a>step6:Center face+dropout+finetune on softmax</h1><p>在msclean测试集上达到93.53% </p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Model</th>\n<th>PCA_Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">dropcenter</td>\n<td>168</td>\n<td>0.64</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter_mirror</td>\n<td>136</td>\n<td>0.64</td>\n<td>99.38%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter +dropcenter_mirror+Min</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter+dropcenter_mirror+ Add</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">center +dropcenter +Min</td>\n<td>400</td>\n<td>0.64</td>\n<td>99.40%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Min</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Add</td>\n<td>160</td>\n<td>0.64</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Max</td>\n<td>160</td>\n<td>0.64</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Concate</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.47%</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>PCA Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>center_min_mirror+dropcenter+Concate</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>center_min_mirror+dropcenter+Min</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>center_min_mirror+dropcenter+Add</td>\n<td>136</td>\n<td>0.64</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>eye_model</td>\n<td>160</td>\n<td>0.57</td>\n<td>77.04%</td>\n</tr>\n<tr>\n<td>eyemodel+center+Con</td>\n<td>208</td>\n<td>0.66</td>\n<td>74.80%</td>\n</tr>\n<tr>\n<td>三模型</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>center+center_min_mirror+dropoutcenter+Concate</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>center+softmax+dropoutcenter+Concate</td>\n<td>168</td>\n<td>0.66</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>center+softmax+dropoutcenter+Add</td>\n<td>496</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step7-balance\"><a href=\"#step7-balance\" class=\"headerlink\" title=\"step7:balance\"></a>step7:balance</h1><h2 id=\"step7-1-减小过采样的数量，防止过拟合\"><a href=\"#step7-1-减小过采样的数量，防止过拟合\" class=\"headerlink\" title=\"step7.1:减小过采样的数量，防止过拟合\"></a>step7.1:减小过采样的数量，防止过拟合</h2><p>对/home/yf/data/clean.txt中每种类别进行统计各有多少个数：</p>\n<table>\n<thead>\n<tr>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>&lt;10</td>\n<td>1213</td>\n<td>10~20</td>\n<td>11617</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>10868</td>\n<td>30~40</td>\n<td>9692</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>9020</td>\n<td>50~60</td>\n<td>8426</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>8443</td>\n<td>70~80</td>\n<td>8783</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>8762</td>\n<td>90~100</td>\n<td>7317</td>\n</tr>\n<tr>\n<td>100~110</td>\n<td>4277</td>\n<td>110~120</td>\n<td>1753</td>\n</tr>\n<tr>\n<td>120~130</td>\n<td>354</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>类别总数共90525。由上图可知，类别严重不均衡，之前处理类别不均衡的方法主要是欠抽样和过抽样结合，对于多数类样本丢弃一部分样本，对于少数类样本复制生成，最后的训练数据分布如下：</p>\n<table>\n<thead>\n<tr>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>70~80</td>\n<td>3673</td>\n<td>80~90</td>\n<td>19068</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>27069</td>\n<td>100~110</td>\n<td>40715</td>\n</tr>\n</tbody>\n</table>\n<p>由于少数类占了大多数，但是重复太多，可能导致过拟合问题，于是将每个类别的图片张数减去30，重新生成balance的训练数据，并训练模型。</p>\n<p><u>迭代17万次后，msdata测试集上准确率达到92.28%，loss=0.27</u></p>\n<p><u>lfw上精度为99.18%</u></p>\n<p><u>mirror:99.27%</u></p>\n<p><u>add:99.27%</u></p>\n<h2 id=\"step7-2-EasyEmsemble法均衡类别\"><a href=\"#step7-2-EasyEmsemble法均衡类别\" class=\"headerlink\" title=\"step7.2:EasyEmsemble法均衡类别\"></a>step7.2:EasyEmsemble法均衡类别</h2><p>step7.1的方法属于欠抽样和过抽样结合：</p>\n<ul>\n<li>对于欠抽样算法，将多数类样本删除有可能会导致分类器<strong>丢失有关多数类的重要信息</strong>。</li>\n<li>对于过抽样算法，虽然只是简单地将复制后的数据添加到原始数据集中，且某些样本的多个实例都是“<strong>并列的</strong>”，但这样也可能会导致分类器学习出现<strong>过拟合现象</strong>，对于同一个样本的多个复本产生多个规则条例，这就使得<strong>规则过于具体化</strong>；虽然在这种情况下，分类器的训练精度会很高，但在位置样本的分类性能就会非常不理想。</li>\n</ul>\n<p><strong>EasyEnsemble 核心思想是：</strong></p>\n<ul>\n<li><p>首先通过从多数类中<strong>独立随机</strong>抽取出若干子集</p>\n</li>\n<li><p>将每个子集与少数类数据<strong>联合</strong>起来<strong>训练</strong>生成多个基分类器</p>\n</li>\n<li><p>最终将这些基分类器<strong>组合形成</strong>一个集成学习系统</p>\n<p>设立一个阈值50，对于类别样本数超过50的，将其分写到两个不同的文件；对于类别样本数不超过50的，利用过采样进行增添，所以最终得到两个有交集的训练集A,B，两个训练集的样本数都是</p>\n<p>90525*50=4526250</p>\n</li>\n</ul>\n<p><u>训练两个model，然后提取特征，对特征进行融合。</u></p>\n<table>\n<thead>\n<tr>\n<th>Model(acc/loss)</th>\n<th>Pca Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>model1(90.75%/0.41)</td>\n<td>176</td>\n<td>0.64</td>\n<td>99.05%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26)</td>\n<td>200</td>\n<td>0.62</td>\n<td>99.28%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26) Mirror</td>\n<td>280</td>\n<td>0.63</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26) Add Mirror</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.30%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41)</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.27%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41) Mirror</td>\n<td>192</td>\n<td>0.63</td>\n<td>99.35%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41) Add Mirror</td>\n<td>192</td>\n<td>0.64</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1 add model2</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.33%</td>\n</tr>\n<tr>\n<td>model1 mirror add model2 mirror</td>\n<td>136</td>\n<td>0.65</td>\n<td>99.37%</td>\n</tr>\n<tr>\n<td>model1 add model2 mirror</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.37%</td>\n</tr>\n<tr>\n<td>model1_add_mirror add model2_add_mirror</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.38%</td>\n</tr>\n<tr>\n<td>model1_add_mirror concate model2_add_mirror</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1 mirror min model2 mirror</td>\n<td>168</td>\n<td>0.64</td>\n<td>99.37%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step8-UMDFaces\"><a href=\"#step8-UMDFaces\" class=\"headerlink\" title=\"step8:UMDFaces\"></a>step8:UMDFaces</h1><p>对UMDFaces数据集进行人脸对齐处理</p>\n<p>batch1:175,534(3554类)</p>\n<p>batch2:115,126(2590类)</p>\n<p>batch3:77,228(2133类)</p>\n<p>frames:3,735,475(3106类)</p>\n<p>提取4个数据集的类别名称，经过处理分析后发现frames的类别属于batch1类别的子集，将3个batch与frames的数据集整合到一个数据集下，因为当静态图片和视频帧进行结合后训练的模型往往既能兼顾个体之间的差异（静态图片特征）也能学习到同一个个体的姿态变化（视频帧特征），要注意的一点就是对于frames和batch1中同一个类别的要放在一个目录下，并重新生成类别标签。</p>\n<p>数据总量:4103363(8276个类别)</p>\n<p>数据整理已经完成，接下来是在这个数据集上进行metric learning的训练。</p>\n<p>train:3286012<br>val:817351</p>\n<h1 id=\"step9-Megaface测试\"><a href=\"#step9-Megaface测试\" class=\"headerlink\" title=\"step9:Megaface测试\"></a>step9:Megaface测试</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Dataset</th>\n<th>Score(Megaface/LFW)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>center-face</td>\n<td>FaceScrub Set1/LFW</td>\n<td>67.32%/99.42%</td>\n</tr>\n<tr>\n<td>balance-reduced</td>\n<td>FaceScrub Set1/LFW</td>\n<td>70.99%/99.18%</td>\n</tr>\n<tr>\n<td>easyensemble</td>\n<td>FaceScrub Set1</td>\n<td>73.91%/99.33%</td>\n</tr>\n<tr>\n<td>easyensemble  concat addmirror</td>\n<td>FaceScrub Set1</td>\n<td>74.21%/99.37%</td>\n</tr>\n<tr>\n<td>balance-cent-soft</td>\n<td>FaceScrub Set1/LFW</td>\n<td>74.47%/99.33%</td>\n</tr>\n<tr>\n<td>balance-cent-soft concat mirror</td>\n<td>FaceScrub Set1</td>\n<td>75.65%</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"step9-1-Megaface测试（续）\"><a href=\"#step9-1-Megaface测试（续）\" class=\"headerlink\" title=\"step9.1:Megaface测试（续）\"></a>step9.1:Megaface测试（续）</h2><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Dataset</th>\n<th>Score(Megaface/LFW)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Dropout_center Concat mirror</td>\n<td>FaceScrub Set1/LFW</td>\n<td>69.34%/99.47%</td>\n</tr>\n<tr>\n<td>normface easyensemble model1 Concart mirror</td>\n<td>FaceScrub Set1/LFW</td>\n<td>70.32%</td>\n</tr>\n<tr>\n<td>normface easyensemble 2models Concat mirror</td>\n<td>FaceScrub Set1</td>\n<td>70.49%</td>\n</tr>\n<tr>\n<td>balance concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>78.84%</td>\n</tr>\n<tr>\n<td>easyensemble concat addmirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>75.92%</td>\n</tr>\n<tr>\n<td>jitter_center_iter_190000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>77.69%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>83.13%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 only mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>82.08%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 add mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>82.87%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_184000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>83.16%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 concat mirror</td>\n<td>FaceScrub(python_mtcnn)</td>\n<td>78.33%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_184000 concat mirror(matlab align)</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>79.05%</td>\n</tr>\n<tr>\n<td>normface_jitter_iter_124000 concat mirror(python align)</td>\n<td>FaceScrub(python_mtcnn)</td>\n<td>76.50%</td>\n</tr>\n<tr>\n<td>normface_jitter_iter124000 cancat mirror(python align)</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>78.92%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step-10-MTCNN-matlab-人脸检测及对齐\"><a href=\"#step-10-MTCNN-matlab-人脸检测及对齐\" class=\"headerlink\" title=\"step 10:MTCNN(matlab)人脸检测及对齐\"></a>step 10:MTCNN(matlab)人脸检测及对齐</h1><h2 id=\"step-10-1：对齐Megaface和FaceScrub\"><a href=\"#step-10-1：对齐Megaface和FaceScrub\" class=\"headerlink\" title=\"step 10.1：对齐Megaface和FaceScrub\"></a>step 10.1：对齐Megaface和FaceScrub</h2><p>主要是Megaface数据集（1028062张）,FaceScrub数据集(91712张)，其中FaceScrub数据集中通过mtcnn（/home/yf/align/align_megaface.m）检测到的有89751张，剩余的1961张需要利用数据集中提供的3个关键点进行对齐，首先需要获取未检测到的图片的路径，然后利用python 脚本(/home/yf/megaface/devkit/templatelists/analysis/analysis_json.py)解析对应的存储该图片中人脸关键点的json文件，最后在利用matlab脚本(/home/yf/align/for_not_detect/align_megaface.m)进行批量对齐。Megaface数据集中未检测到的数据集同样处理。</p>\n<p><strong>问题</strong>：</p>\n<p>在进行了41万次对齐后，出现了imread的错误，然后将从目录读取路径改成了从存储图片路径的文件中(/home/yf/megaface/tests/MegaFace_align_list_image.txt)直接获取路径，并输出每次进行处理的文件名，重新进行对齐操作，然后重现了这个错误，最后比对MegaFace_align_list_image.txt的下一张图片，发现有张图片是输入为空的。</p>\n<h2 id=\"step-10-2-对齐msceleb数据\"><a href=\"#step-10-2-对齐msceleb数据\" class=\"headerlink\" title=\"step 10.2:对齐msceleb数据\"></a>step 10.2:对齐msceleb数据</h2><p>重新对齐msceleb数据集用于训练。</p>\n<h1 id=\"step-11-Normface训练\"><a href=\"#step-11-Normface训练\" class=\"headerlink\" title=\"step 11:Normface训练\"></a>step 11:Normface训练</h1><p>Normface(paper:<a href=\"https://arxiv.org/pdf/1704.06369.pdf\" target=\"_blank\" rel=\"external\">NormFace: L2 Hypersphere Embedding for Face Verification</a>)</p>\n<h2 id=\"step-11-1-训练EasyEnsemble模型\"><a href=\"#step-11-1-训练EasyEnsemble模型\" class=\"headerlink\" title=\"step 11.1:训练EasyEnsemble模型\"></a>step 11.1:训练EasyEnsemble模型</h2><p>model1在测试集上的准确率为92.88%，model2在测试集上的准确率为92.85%。</p>\n<p>暂时只测了单个的model1 concate mirror在Megaface(还是原始python版mtcnn对齐的)上的准确率只有70.32%。下周继续测试两个模型的效果。</p>\n<h2 id=\"step-11-2-训练Balance模型\"><a href=\"#step-11-2-训练Balance模型\" class=\"headerlink\" title=\"step 11.2:训练Balance模型\"></a>step 11.2:训练Balance模型</h2><p>刚生成完训练的数据集，下周开始训练。</p>\n<h1 id=\"step-12-Image-Jitter\"><a href=\"#step-12-Image-Jitter\" class=\"headerlink\" title=\"step 12:Image Jitter\"></a>step 12:Image Jitter</h1><p>对图片增加随机扰动，包括缩放、角度变换、镜像操作，主要还是msceleb数据集(/home/yf/data/msclean)上进行，由于该数据集类别不均衡，所以对于样本数较少的类别可以采用这种办法增加样本容量。最终将每个类别的样本数控制在80~160之间。</p>\n<p>10240892</p>\n<p>train:9257534</p>\n<p>val:983342</p>\n<p>正在生成训练的数据集lmdb。</p>\n<p>jitter_center_iter_190000 concat mirror  FaceScrub(matlab_mtcnn)  77.69%  </p>\n<p>jitter_softmax_iter_180000 concat mirror  FaceScrub(matlab_mtcnn)  83.18%</p>\n<p>jitter_softmax_iter_180000 only mirror  FaceScrub(matlab_mtcnn)  82.08%</p>\n<p>jitter_softmax_iter_180000 add mirror  FaceScrub(matlab_mtcnn)  82.87%</p>\n<p>jitter_softmax_iter_184000 concat mirror  FaceScrub(matlab_mtcnn)  83.16%</p>\n<h1 id=\"step-13-Gender-test\"><a href=\"#step-13-Gender-test\" class=\"headerlink\" title=\"step 13:Gender test\"></a>step 13:Gender test</h1><h3 id=\"1-0\"><a href=\"#1-0\" class=\"headerlink\" title=\"1.0\"></a>1.0</h3><p>VGG16在lfw上准确率90.04%，在imdb(15590测试样本)上准确率90.92%</p>\n<p>model training:female(69847),male(86061)</p>\n<p>train:124728</p>\n<p>val:15590</p>\n<p>test:15590</p>\n<h3 id=\"2-0\"><a href=\"#2-0\" class=\"headerlink\" title=\"2.0\"></a>2.0</h3><p>网络：AlexNet 在清理后的数据集上，迭代29500次后，训练准确率为98.16%，loss=0.55</p>\n<p>在测试集上达到98.11%（15127/15418)</p>\n<h1 id=\"Todo1\"><a href=\"#Todo1\" class=\"headerlink\" title=\"Todo1\"></a>Todo1</h1><ul>\n<li>[x] Create umdfaces–&gt;lmdb</li>\n<li>[x] EasyEnsemble train and test</li>\n<li>[x] Use matcaffe for metric learning</li>\n<li>[ ] Megaface test<pre><code>- - [x] center face\n  - [x] balance-cent-soft\n  - [x] reduced\n  - [x] mirror or concatenate\n  - [x] EasyEnsemble\n</code></pre></li>\n</ul>\n<ul>\n<li><p>[x] Paper reading:One-shot face recognition by promoting underrepresented classes</p>\n<pre><code>​\n</code></pre></li>\n</ul>\n<h1 id=\"Todo2\"><a href=\"#Todo2\" class=\"headerlink\" title=\"Todo2\"></a>Todo2</h1><ul>\n<li><p>[ ] jitter model training(softmax first)</p>\n</li>\n<li><p>[ ] balance model retrain on normface(include center loss)</p>\n</li>\n<li><p>[ ] aligned by matlab_mtcnn megaface(balance model 75.65% version)</p>\n</li>\n<li><p>[ ] gender classfication model training</p>\n<pre><code>- check the dataset (detect and crop by matlab_mtcnn)\n\n- generate lmdb\n\n- choose a model(ResNet?)\n\n  ​\n</code></pre></li>\n</ul>\n<h3 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h3><p><a href=\"https://github.com/happynear/FaceVerification\" target=\"_blank\" rel=\"external\">happynear-face-verification</a></p>\n<p><a href=\"https://github.com/davisking/dlib/blob/cbd187fb6109d21406f6a76bb0e9aa0689b1e54a/examples/dnn_face_recognition_ex.cpp\" target=\"_blank\" rel=\"external\">dlib-jitter</a></p>\n<p><a href=\"http://blog.dlib.net\" target=\"_blank\" rel=\"external\">dlib-face-verification-blog</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"step1-Mirror-face相关\"><a href=\"#step1-Mirror-face相关\" class=\"headerlink\" title=\"step1:Mirror face相关\"></a>step1:Mirror face相关</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>PCA Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Mirror</td>\n<td>192</td>\n<td>0.64</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Concat</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Add/Average</td>\n<td>184</td>\n<td>0.64</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>Mirror Max</td>\n<td>144</td>\n<td>0.65</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>Mirror Min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.48%</td>\n</tr>\n<tr>\n<td>Mirror Avg+min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.45%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step2-单眼Patch-model\"><a href=\"#step2-单眼Patch-model\" class=\"headerlink\" title=\"step2:单眼Patch model\"></a>step2:单眼Patch model</h1><table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>14</td>\n<td>90~100</td>\n<td>8,5674</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>3329</td>\n<td>100~110</td>\n<td>2,9481</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>21,3675</td>\n<td>110~120</td>\n<td>9051</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>45,1416</td>\n<td>120~130</td>\n<td>2894</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>49,1913</td>\n<td>130~140</td>\n<td>932</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>72,8911</td>\n<td>140~150</td>\n<td>279</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>137,9232</td>\n<td>150~160</td>\n<td>86</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>128,7442</td>\n<td>160~170</td>\n<td>14</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>30,9026</td>\n<td>170~180</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y…],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)<em>0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5</em>eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">if eye_width*0.5&gt;eye_left_x:</div><div class=\"line\">            eye_width=eye_left_x*2</div></pre></td></tr></table></figure></p>\n<p>导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张</p>\n<p>重新生成眼睛宽度估算文件，其分布如下</p>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>1,8975</td>\n<td>100~110</td>\n<td>98</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>71,3760</td>\n<td>110~120</td>\n<td>17</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>126,4102</td>\n<td>120~130</td>\n<td>3</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>226,4348</td>\n<td>130~140</td>\n<td>0</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>59,3351</td>\n<td>140~150</td>\n<td>0</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>10,6502</td>\n<td>150~160</td>\n<td>0</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>2,4588</td>\n<td>160~170</td>\n<td>1</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>5592</td>\n<td>170~180</td>\n<td>0</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>1588</td>\n<td>180~190</td>\n<td>8</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>421</td>\n<td>190~200</td>\n<td>21</td>\n</tr>\n</tbody>\n</table>\n<p>2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可<br>3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。<br>最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526</p>\n<p><u>17个小时，迭代12万次，26.5%的准确率，loss=5.5。</u></p>\n<h1 id=\"step3-双眼patch-model\"><a href=\"#step3-双眼patch-model\" class=\"headerlink\" title=\"step3:双眼patch model\"></a>step3:双眼patch model</h1><p>生成双眼宽度估算文件，其分布如下</p>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>30</td>\n<td>100~110</td>\n<td>18,8771</td>\n<td>200~210</td>\n<td>383</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>5888</td>\n<td>110~120</td>\n<td>8,8508</td>\n<td>210~220</td>\n<td>196</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>11,4595</td>\n<td>120~130</td>\n<td>4,6243</td>\n<td>220~230</td>\n<td>100</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>35,5451</td>\n<td>130~140</td>\n<td>2,4982</td>\n<td>230~240</td>\n<td>52</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>42,6729</td>\n<td>140~150</td>\n<td>1,2749</td>\n<td>240~250</td>\n<td>33</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>49,1312</td>\n<td>150~160</td>\n<td>6714</td>\n<td>250~260</td>\n<td>15</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>69,0394</td>\n<td>160~170</td>\n<td>3413</td>\n<td>260~270</td>\n<td>3</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>104,9353</td>\n<td>170~180</td>\n<td>1909</td>\n<td>270~280</td>\n<td>2</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>100,6618</td>\n<td>180~190</td>\n<td>1091</td>\n<td>280~290</td>\n<td>0</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>47,7219</td>\n<td>190~200</td>\n<td>622</td>\n<td>290~300</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<p>筛选crop后，宽度在20~130区间的图片，共90523个类别，4932655张。</p>\n<p>train.txt：3982004 </p>\n<p>val.txt：950661 </p>\n<p><u>18万次迭代之后，准确率只有66%左右。</u></p>\n<h1 id=\"step4-crop对齐后的图片的眼睛，训练单眼模型\"><a href=\"#step4-crop对齐后的图片的眼睛，训练单眼模型\" class=\"headerlink\" title=\"step4:crop对齐后的图片的眼睛，训练单眼模型\"></a>step4:crop对齐后的图片的眼睛，训练单眼模型</h1><p>数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”）</p>\n<pre><code>&quot;ref_points&quot;: [\n        30.2946, 51.6963, \n     65.5318, 51.5014, \n     48.0252, 71.7366,\n     33.5493, 92.3655, \n     62.7299, 92.2041\n ]\n\n\n  eye_width=(ref_points[2]-ref_points[0])*0.8\n     eye_height=eye_width\n     x1=ref_points[0]-0.5*eye_width=16\n     x2=ref_points[0]+0.5*eye_width=44\n     y1=ref_points[1]-0.5*eye_height=37\n     y2=ref_points[1]+0.5*eye_height=65\n</code></pre><p>train:4071324 张</p>\n<p>val:973183张</p>\n<h1 id=\"step5-crop对齐后的图片的眼睛，训练双眼模型\"><a href=\"#step5-crop对齐后的图片的眼睛，训练双眼模型\" class=\"headerlink\" title=\"step5:crop对齐后的图片的眼睛，训练双眼模型\"></a>step5:crop对齐后的图片的眼睛，训练双眼模型</h1><p>数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”）</p>\n<pre><code>   &quot;ref_points&quot;: [\n       30.2946, 51.6963, \n    65.5318, 51.5014, \n    48.0252, 71.7366,\n    33.5493, 92.3655, \n    62.7299, 92.2041\n]\n     eye_width=(ref_points[2]-ref_points[0])*0.8\n    eye_height=eye_width\n    x1=ref_points[0]-0.5*eye_width=16\n    x2=ref_points[2]+0.5*eye_width=79\n    y1=ref_points[1]-0.5*eye_height=37\n    y2=ref_points[1]+0.5*eye_height=65\n</code></pre><p><u>迭代16万次，精度为72.73%，loss=2.52</u></p>\n<p><u>在lfw上测试，精度最高达到77.04%</u></p>\n<h1 id=\"step6-Center-face-dropout-finetune-on-softmax\"><a href=\"#step6-Center-face-dropout-finetune-on-softmax\" class=\"headerlink\" title=\"step6:Center face+dropout+finetune on softmax\"></a>step6:Center face+dropout+finetune on softmax</h1><p>在msclean测试集上达到93.53% </p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Model</th>\n<th>PCA_Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">dropcenter</td>\n<td>168</td>\n<td>0.64</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter_mirror</td>\n<td>136</td>\n<td>0.64</td>\n<td>99.38%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter +dropcenter_mirror+Min</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter+dropcenter_mirror+ Add</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">center +dropcenter +Min</td>\n<td>400</td>\n<td>0.64</td>\n<td>99.40%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Min</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Add</td>\n<td>160</td>\n<td>0.64</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Max</td>\n<td>160</td>\n<td>0.64</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Concate</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.47%</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>PCA Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>center_min_mirror+dropcenter+Concate</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>center_min_mirror+dropcenter+Min</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>center_min_mirror+dropcenter+Add</td>\n<td>136</td>\n<td>0.64</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>eye_model</td>\n<td>160</td>\n<td>0.57</td>\n<td>77.04%</td>\n</tr>\n<tr>\n<td>eyemodel+center+Con</td>\n<td>208</td>\n<td>0.66</td>\n<td>74.80%</td>\n</tr>\n<tr>\n<td>三模型</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>center+center_min_mirror+dropoutcenter+Concate</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>center+softmax+dropoutcenter+Concate</td>\n<td>168</td>\n<td>0.66</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>center+softmax+dropoutcenter+Add</td>\n<td>496</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step7-balance\"><a href=\"#step7-balance\" class=\"headerlink\" title=\"step7:balance\"></a>step7:balance</h1><h2 id=\"step7-1-减小过采样的数量，防止过拟合\"><a href=\"#step7-1-减小过采样的数量，防止过拟合\" class=\"headerlink\" title=\"step7.1:减小过采样的数量，防止过拟合\"></a>step7.1:减小过采样的数量，防止过拟合</h2><p>对/home/yf/data/clean.txt中每种类别进行统计各有多少个数：</p>\n<table>\n<thead>\n<tr>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>&lt;10</td>\n<td>1213</td>\n<td>10~20</td>\n<td>11617</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>10868</td>\n<td>30~40</td>\n<td>9692</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>9020</td>\n<td>50~60</td>\n<td>8426</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>8443</td>\n<td>70~80</td>\n<td>8783</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>8762</td>\n<td>90~100</td>\n<td>7317</td>\n</tr>\n<tr>\n<td>100~110</td>\n<td>4277</td>\n<td>110~120</td>\n<td>1753</td>\n</tr>\n<tr>\n<td>120~130</td>\n<td>354</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>类别总数共90525。由上图可知，类别严重不均衡，之前处理类别不均衡的方法主要是欠抽样和过抽样结合，对于多数类样本丢弃一部分样本，对于少数类样本复制生成，最后的训练数据分布如下：</p>\n<table>\n<thead>\n<tr>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>70~80</td>\n<td>3673</td>\n<td>80~90</td>\n<td>19068</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>27069</td>\n<td>100~110</td>\n<td>40715</td>\n</tr>\n</tbody>\n</table>\n<p>由于少数类占了大多数，但是重复太多，可能导致过拟合问题，于是将每个类别的图片张数减去30，重新生成balance的训练数据，并训练模型。</p>\n<p><u>迭代17万次后，msdata测试集上准确率达到92.28%，loss=0.27</u></p>\n<p><u>lfw上精度为99.18%</u></p>\n<p><u>mirror:99.27%</u></p>\n<p><u>add:99.27%</u></p>\n<h2 id=\"step7-2-EasyEmsemble法均衡类别\"><a href=\"#step7-2-EasyEmsemble法均衡类别\" class=\"headerlink\" title=\"step7.2:EasyEmsemble法均衡类别\"></a>step7.2:EasyEmsemble法均衡类别</h2><p>step7.1的方法属于欠抽样和过抽样结合：</p>\n<ul>\n<li>对于欠抽样算法，将多数类样本删除有可能会导致分类器<strong>丢失有关多数类的重要信息</strong>。</li>\n<li>对于过抽样算法，虽然只是简单地将复制后的数据添加到原始数据集中，且某些样本的多个实例都是“<strong>并列的</strong>”，但这样也可能会导致分类器学习出现<strong>过拟合现象</strong>，对于同一个样本的多个复本产生多个规则条例，这就使得<strong>规则过于具体化</strong>；虽然在这种情况下，分类器的训练精度会很高，但在位置样本的分类性能就会非常不理想。</li>\n</ul>\n<p><strong>EasyEnsemble 核心思想是：</strong></p>\n<ul>\n<li><p>首先通过从多数类中<strong>独立随机</strong>抽取出若干子集</p>\n</li>\n<li><p>将每个子集与少数类数据<strong>联合</strong>起来<strong>训练</strong>生成多个基分类器</p>\n</li>\n<li><p>最终将这些基分类器<strong>组合形成</strong>一个集成学习系统</p>\n<p>设立一个阈值50，对于类别样本数超过50的，将其分写到两个不同的文件；对于类别样本数不超过50的，利用过采样进行增添，所以最终得到两个有交集的训练集A,B，两个训练集的样本数都是</p>\n<p>90525*50=4526250</p>\n</li>\n</ul>\n<p><u>训练两个model，然后提取特征，对特征进行融合。</u></p>\n<table>\n<thead>\n<tr>\n<th>Model(acc/loss)</th>\n<th>Pca Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>model1(90.75%/0.41)</td>\n<td>176</td>\n<td>0.64</td>\n<td>99.05%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26)</td>\n<td>200</td>\n<td>0.62</td>\n<td>99.28%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26) Mirror</td>\n<td>280</td>\n<td>0.63</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26) Add Mirror</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.30%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41)</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.27%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41) Mirror</td>\n<td>192</td>\n<td>0.63</td>\n<td>99.35%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41) Add Mirror</td>\n<td>192</td>\n<td>0.64</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1 add model2</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.33%</td>\n</tr>\n<tr>\n<td>model1 mirror add model2 mirror</td>\n<td>136</td>\n<td>0.65</td>\n<td>99.37%</td>\n</tr>\n<tr>\n<td>model1 add model2 mirror</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.37%</td>\n</tr>\n<tr>\n<td>model1_add_mirror add model2_add_mirror</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.38%</td>\n</tr>\n<tr>\n<td>model1_add_mirror concate model2_add_mirror</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1 mirror min model2 mirror</td>\n<td>168</td>\n<td>0.64</td>\n<td>99.37%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step8-UMDFaces\"><a href=\"#step8-UMDFaces\" class=\"headerlink\" title=\"step8:UMDFaces\"></a>step8:UMDFaces</h1><p>对UMDFaces数据集进行人脸对齐处理</p>\n<p>batch1:175,534(3554类)</p>\n<p>batch2:115,126(2590类)</p>\n<p>batch3:77,228(2133类)</p>\n<p>frames:3,735,475(3106类)</p>\n<p>提取4个数据集的类别名称，经过处理分析后发现frames的类别属于batch1类别的子集，将3个batch与frames的数据集整合到一个数据集下，因为当静态图片和视频帧进行结合后训练的模型往往既能兼顾个体之间的差异（静态图片特征）也能学习到同一个个体的姿态变化（视频帧特征），要注意的一点就是对于frames和batch1中同一个类别的要放在一个目录下，并重新生成类别标签。</p>\n<p>数据总量:4103363(8276个类别)</p>\n<p>数据整理已经完成，接下来是在这个数据集上进行metric learning的训练。</p>\n<p>train:3286012<br>val:817351</p>\n<h1 id=\"step9-Megaface测试\"><a href=\"#step9-Megaface测试\" class=\"headerlink\" title=\"step9:Megaface测试\"></a>step9:Megaface测试</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Dataset</th>\n<th>Score(Megaface/LFW)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>center-face</td>\n<td>FaceScrub Set1/LFW</td>\n<td>67.32%/99.42%</td>\n</tr>\n<tr>\n<td>balance-reduced</td>\n<td>FaceScrub Set1/LFW</td>\n<td>70.99%/99.18%</td>\n</tr>\n<tr>\n<td>easyensemble</td>\n<td>FaceScrub Set1</td>\n<td>73.91%/99.33%</td>\n</tr>\n<tr>\n<td>easyensemble  concat addmirror</td>\n<td>FaceScrub Set1</td>\n<td>74.21%/99.37%</td>\n</tr>\n<tr>\n<td>balance-cent-soft</td>\n<td>FaceScrub Set1/LFW</td>\n<td>74.47%/99.33%</td>\n</tr>\n<tr>\n<td>balance-cent-soft concat mirror</td>\n<td>FaceScrub Set1</td>\n<td>75.65%</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"step9-1-Megaface测试（续）\"><a href=\"#step9-1-Megaface测试（续）\" class=\"headerlink\" title=\"step9.1:Megaface测试（续）\"></a>step9.1:Megaface测试（续）</h2><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Dataset</th>\n<th>Score(Megaface/LFW)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Dropout_center Concat mirror</td>\n<td>FaceScrub Set1/LFW</td>\n<td>69.34%/99.47%</td>\n</tr>\n<tr>\n<td>normface easyensemble model1 Concart mirror</td>\n<td>FaceScrub Set1/LFW</td>\n<td>70.32%</td>\n</tr>\n<tr>\n<td>normface easyensemble 2models Concat mirror</td>\n<td>FaceScrub Set1</td>\n<td>70.49%</td>\n</tr>\n<tr>\n<td>balance concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>78.84%</td>\n</tr>\n<tr>\n<td>easyensemble concat addmirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>75.92%</td>\n</tr>\n<tr>\n<td>jitter_center_iter_190000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>77.69%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>83.13%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 only mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>82.08%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 add mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>82.87%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_184000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>83.16%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 concat mirror</td>\n<td>FaceScrub(python_mtcnn)</td>\n<td>78.33%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_184000 concat mirror(matlab align)</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>79.05%</td>\n</tr>\n<tr>\n<td>normface_jitter_iter_124000 concat mirror(python align)</td>\n<td>FaceScrub(python_mtcnn)</td>\n<td>76.50%</td>\n</tr>\n<tr>\n<td>normface_jitter_iter124000 cancat mirror(python align)</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>78.92%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step-10-MTCNN-matlab-人脸检测及对齐\"><a href=\"#step-10-MTCNN-matlab-人脸检测及对齐\" class=\"headerlink\" title=\"step 10:MTCNN(matlab)人脸检测及对齐\"></a>step 10:MTCNN(matlab)人脸检测及对齐</h1><h2 id=\"step-10-1：对齐Megaface和FaceScrub\"><a href=\"#step-10-1：对齐Megaface和FaceScrub\" class=\"headerlink\" title=\"step 10.1：对齐Megaface和FaceScrub\"></a>step 10.1：对齐Megaface和FaceScrub</h2><p>主要是Megaface数据集（1028062张）,FaceScrub数据集(91712张)，其中FaceScrub数据集中通过mtcnn（/home/yf/align/align_megaface.m）检测到的有89751张，剩余的1961张需要利用数据集中提供的3个关键点进行对齐，首先需要获取未检测到的图片的路径，然后利用python 脚本(/home/yf/megaface/devkit/templatelists/analysis/analysis_json.py)解析对应的存储该图片中人脸关键点的json文件，最后在利用matlab脚本(/home/yf/align/for_not_detect/align_megaface.m)进行批量对齐。Megaface数据集中未检测到的数据集同样处理。</p>\n<p><strong>问题</strong>：</p>\n<p>在进行了41万次对齐后，出现了imread的错误，然后将从目录读取路径改成了从存储图片路径的文件中(/home/yf/megaface/tests/MegaFace_align_list_image.txt)直接获取路径，并输出每次进行处理的文件名，重新进行对齐操作，然后重现了这个错误，最后比对MegaFace_align_list_image.txt的下一张图片，发现有张图片是输入为空的。</p>\n<h2 id=\"step-10-2-对齐msceleb数据\"><a href=\"#step-10-2-对齐msceleb数据\" class=\"headerlink\" title=\"step 10.2:对齐msceleb数据\"></a>step 10.2:对齐msceleb数据</h2><p>重新对齐msceleb数据集用于训练。</p>\n<h1 id=\"step-11-Normface训练\"><a href=\"#step-11-Normface训练\" class=\"headerlink\" title=\"step 11:Normface训练\"></a>step 11:Normface训练</h1><p>Normface(paper:<a href=\"https://arxiv.org/pdf/1704.06369.pdf\" target=\"_blank\" rel=\"external\">NormFace: L2 Hypersphere Embedding for Face Verification</a>)</p>\n<h2 id=\"step-11-1-训练EasyEnsemble模型\"><a href=\"#step-11-1-训练EasyEnsemble模型\" class=\"headerlink\" title=\"step 11.1:训练EasyEnsemble模型\"></a>step 11.1:训练EasyEnsemble模型</h2><p>model1在测试集上的准确率为92.88%，model2在测试集上的准确率为92.85%。</p>\n<p>暂时只测了单个的model1 concate mirror在Megaface(还是原始python版mtcnn对齐的)上的准确率只有70.32%。下周继续测试两个模型的效果。</p>\n<h2 id=\"step-11-2-训练Balance模型\"><a href=\"#step-11-2-训练Balance模型\" class=\"headerlink\" title=\"step 11.2:训练Balance模型\"></a>step 11.2:训练Balance模型</h2><p>刚生成完训练的数据集，下周开始训练。</p>\n<h1 id=\"step-12-Image-Jitter\"><a href=\"#step-12-Image-Jitter\" class=\"headerlink\" title=\"step 12:Image Jitter\"></a>step 12:Image Jitter</h1><p>对图片增加随机扰动，包括缩放、角度变换、镜像操作，主要还是msceleb数据集(/home/yf/data/msclean)上进行，由于该数据集类别不均衡，所以对于样本数较少的类别可以采用这种办法增加样本容量。最终将每个类别的样本数控制在80~160之间。</p>\n<p>10240892</p>\n<p>train:9257534</p>\n<p>val:983342</p>\n<p>正在生成训练的数据集lmdb。</p>\n<p>jitter_center_iter_190000 concat mirror  FaceScrub(matlab_mtcnn)  77.69%  </p>\n<p>jitter_softmax_iter_180000 concat mirror  FaceScrub(matlab_mtcnn)  83.18%</p>\n<p>jitter_softmax_iter_180000 only mirror  FaceScrub(matlab_mtcnn)  82.08%</p>\n<p>jitter_softmax_iter_180000 add mirror  FaceScrub(matlab_mtcnn)  82.87%</p>\n<p>jitter_softmax_iter_184000 concat mirror  FaceScrub(matlab_mtcnn)  83.16%</p>\n<h1 id=\"step-13-Gender-test\"><a href=\"#step-13-Gender-test\" class=\"headerlink\" title=\"step 13:Gender test\"></a>step 13:Gender test</h1><h3 id=\"1-0\"><a href=\"#1-0\" class=\"headerlink\" title=\"1.0\"></a>1.0</h3><p>VGG16在lfw上准确率90.04%，在imdb(15590测试样本)上准确率90.92%</p>\n<p>model training:female(69847),male(86061)</p>\n<p>train:124728</p>\n<p>val:15590</p>\n<p>test:15590</p>\n<h3 id=\"2-0\"><a href=\"#2-0\" class=\"headerlink\" title=\"2.0\"></a>2.0</h3><p>网络：AlexNet 在清理后的数据集上，迭代29500次后，训练准确率为98.16%，loss=0.55</p>\n<p>在测试集上达到98.11%（15127/15418)</p>\n<h1 id=\"Todo1\"><a href=\"#Todo1\" class=\"headerlink\" title=\"Todo1\"></a>Todo1</h1><ul>\n<li>[x] Create umdfaces–&gt;lmdb</li>\n<li>[x] EasyEnsemble train and test</li>\n<li>[x] Use matcaffe for metric learning</li>\n<li>[ ] Megaface test<pre><code>- - [x] center face\n  - [x] balance-cent-soft\n  - [x] reduced\n  - [x] mirror or concatenate\n  - [x] EasyEnsemble\n</code></pre></li>\n</ul>\n<ul>\n<li><p>[x] Paper reading:One-shot face recognition by promoting underrepresented classes</p>\n<pre><code>​\n</code></pre></li>\n</ul>\n<h1 id=\"Todo2\"><a href=\"#Todo2\" class=\"headerlink\" title=\"Todo2\"></a>Todo2</h1><ul>\n<li><p>[ ] jitter model training(softmax first)</p>\n</li>\n<li><p>[ ] balance model retrain on normface(include center loss)</p>\n</li>\n<li><p>[ ] aligned by matlab_mtcnn megaface(balance model 75.65% version)</p>\n</li>\n<li><p>[ ] gender classfication model training</p>\n<pre><code>- check the dataset (detect and crop by matlab_mtcnn)\n\n- generate lmdb\n\n- choose a model(ResNet?)\n\n  ​\n</code></pre></li>\n</ul>\n<h3 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h3><p><a href=\"https://github.com/happynear/FaceVerification\" target=\"_blank\" rel=\"external\">happynear-face-verification</a></p>\n<p><a href=\"https://github.com/davisking/dlib/blob/cbd187fb6109d21406f6a76bb0e9aa0689b1e54a/examples/dnn_face_recognition_ex.cpp\" target=\"_blank\" rel=\"external\">dlib-jitter</a></p>\n<p><a href=\"http://blog.dlib.net\" target=\"_blank\" rel=\"external\">dlib-face-verification-blog</a></p>\n"},{"title":"手册&指南","date":"2018-04-27T01:32:52.000Z","description":"关于linux/vim/matlab/python/git等的一些指令","mathjax":null,"_content":"\n# Linux操作相关\n##基本操作\n**打包**：\n​       `tar -cvf xx.tar xxx`\n​\t   `tar zcvf xx.tar.gz xx`\n**解压**：\n       `tar zxvf xx.tar.gz`\n\t   `tar xvf xx.tar`\n\t   `tar jxvf xx.tar.bz2`\n\t   `bzip2 -d xx.bz2`\n**创建目录xx**：`mkdir xx`\n**创建目录a及以下子目录b**:`mkdir -p a/b`\n**创建xx文件**:`touch xx`\n**查看xx文件**:`cat xx`\n**查看xx文件**:`less xx`(内容多于一屏时，j/k向下/上翻滚)\n**显示绝对路径**：`pwd`\n**删除空目录**：`rmdir`\n**删除xx文件**：`rm xx`\n**强制删除目录a**:`rm -rf a`\n**删除目录a**:`rm -r a`\n**删除父目录a中所有的空子目录**：`cd a; rmdir` \n**复制文件a到文件b**：`cp a b`\n**复制目录a为目录b**：`cp -r a/ b/`\n**从指定文件搜索指定文件**：`grep '指定内容' ./ -R`\n**下载文件到xx目录**：`axel http://... -o xx`\n**给文件a.c加执行权限**：`chmod +x a.c`\n**运行脚本xx.sh**:`sh xx.sh`\n**使.bashrc生效**：`source ~/.bashrc`\n**查看opencv版本**：`pkg-config --modversion opencv`\n**查看opencv安装位置**：`pkg-config -cflags opencv`\n**查找**：`grep -n -H -R \"you want to search \" *`\n**查看文件夹大小**：`du -sh`\n**查看文件夹大小并排序**：`du -h /home/* | sort`\n**查看文件夹大小**：`du -h --max-depth=1 ./`  \n**查看磁盘的使用情况**：`df -h`\n**查看权限**：`ls -l filaname` \n**查看GPU使用率**：`nvidia-smi`\n**查看CPU使用率**：`top`\n**查看文件夹内文件个数**：`ls -l | grep '^-' | wc -l`\n**查看文件夹内目录个数**：`ls -l | grep '^d' | wc -l`\n**查看文件行数**：`wc -l filename`\n**批量更改图片尺寸**：`mogrify -resize 224x224 -format jpg *`\n\n**查看网络设置**：`ifconfig`\n**查看操作系统**：`uname -a`\n**查看ubuntu版本**：`lsb_release -a`\n**查看操作系统位数**：getconf LONG_BIT\n**查看gcc版本**：`ls /usr/bin/gcc*或gcc -v`\n**查看磁盘使用情况**：`baobab`\n**添加用户**：`sudo adduser username`\n**删除用户**: `sudo userdel -r username`\n**添加sudo权限**：`sudo vi /etc/sudoers`\n\n\n**显示终端上所有进程**：`ps -a`\n**查看进程所有者及其他详细信息**：`ps -u`\n**杀掉某进程**：`kill -SIGKILL 进程号`\n**查看使用apt-get的进程**：`ps -aux | grep 'apt-get'`\n**进程暂停**：`kill -STOP pid`\n**进程重启**：`kill -CONT pid`\n\n\n**替换gcc版本**：\n`sudo apt-get install gcc-4.9 gcc-4.9-multilib g++-4.9 g++-4.9-multilib`\n`sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 40`\n`sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5.4 50`\n`sudo update-alternatives --config gcc`\n`sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 50` \n`sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 40`\n`sudo update-alternatives --remove gcc /usr/bin/gcc-4.9`\n\t\n\n**查找命令：**\n> - find . -name 'my*'：搜索当前目录（含子目录，以下同）中，所有文件名以my开头的文\n> - locate ~/m ：搜索用户主目录下，所有以m开头的文件\n> - locate -i ~/m：搜索用户主目录下，所有以m开头的文件，并且忽略大小写\n> - whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息\n> - which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令\n\n\n**Zip相关**：\n\n> - 把一个文件abc.txt和一个目录dir1压缩成为yasuo.zip：zip -r yasuo.zip abc.txt dir1\n> - 下载了一个yasuo.zip文件，想解压缩：unzip yasuo.zip\n> - 当前目录下有abc1.zip，abc2.zip和abc3.zip，一起解压缩它们：unzip abc\\?.zip(注释：?表示一个字符，如果用*表示任意多个字符。)\n> - 有一个很大的压缩文件large.zip，不想解压缩，只想看看它里面有什么：unzip -v large.zip\n> - 下载了一个压缩文件large.zip，想验证一下这个压缩文件是否下载完全了: unzip -t large.zip\n> - 用-v选项发现music.zip压缩文件里面有很多目录和子目录，并且子目录中其实都是歌曲mp3文件，把这些文件都下载到第一级目录，而不是一层一层建目录： unzip -j music.zip\n\n\n\n\n\n\n\n# Vim相关\n**跳转到指令行**：在命令行模式下输入  “:行号”\n**查找字符串**：在命令行模式下输入 “/字符串”，按“n\"键查找下一个\n**批量替换**：在命令行模式下输入 \":%s#abc#def#g\"   将def替换abc\n**批量注释**：ctrl+v进入列模式，大写I进入插入模式，输入注释符//或#,连按两次esc。\n**取消批量注释**：Ctrl + v 进入块选择模式，选中你要删除的行首的注释符号，注意// 要选中两个，选好之后按d即可删除注释\n\n\n\n\n\n\n# 安装驱动\n**查看显卡驱动信息**：cat /proc/driver/nvidia/version\n**查询合适的驱动版本xxx**: https://www.geforce.com/drivers\n```\nsudo apt-get remove --purge nvidia-*\nsudo add-apt-repository ppa:graphics-drivers/ppa\nsudo apt-get update\nsudo service lightdm stop\nsudo apt-get install nvidia-XXX\nsudo service lightdm start\nsudo reboot\nlsmod | grep nvidia   #查看驱动状态是否正常\n```\n\n\n\n\n\n\n# Matlab相关\n后台运行：nohup matlab -nojvm -nodisplay -nosplash < matlabscript.m 1>running.log 2>running.err &\n\n\n\n\n\n\n\n# Python相关\n##路径\nimport os\n先定义一个带路径的文件\nfilename = \"/home/mydir/test.txt\"\n将文件路径分割出来\nfile_dir = os.path.split(filename )[0]\n判断文件路径是否存在，如果不存在，则创建，此处是创建多级目录\n```\n    if not os.path.isdir(file_dir):\n        os.makedirs(file_dir)\n```\n然后再判断文件是否存在，如果不存在，则创建\n```\n    if not os.path.exists(filename ):\n        os.system(r'touch %s' % filename)\n```\n\n\n\n\n\n\n\n\n# git相关\n\n设置Git的user name和email\n\n```\ngit config --global user.name \"yourname\"\ngit config --global user.email \"youremail\"\n```\n\n生成SSH密钥\n\n```\n查看是否已经有了ssh密钥：cd ~/.ssh\n如果没有密钥则不会有此文件夹，有则备份删除\n生存密钥：\nssh-keygen -t rsa -C “haiyan.xu.vip@gmail.com”\n按3个回车，密码为空。\nYour identification has been saved in /home/tekkub/.ssh/id_rsa.\nYour public key has been saved in /home/tekkub/.ssh/id_rsa.pub.\nThe key fingerprint is:\n………………\n最后得到了两个文件：id_rsa和id_rsa.pub\n```\n\n添加密钥到ssh：ssh-add 文件名,需要之前输入密码.\n\n在github上添加ssh密钥，这要添加的是“id_rsa.pub”里面的公钥。\n\n\n\n从本地上传到github:\ngit init\ngit remote add origin https://github.com/nerddd/text.git  \n\n\n","source":"_posts/手册&指南.md","raw":"---\ntitle: 手册&指南\ndate: 2018-04-27 09:32:52\ncategories: 技能\ntags: [工具]\ndescription: 关于linux/vim/matlab/python/git等的一些指令\nmathjax:\n---\n\n# Linux操作相关\n##基本操作\n**打包**：\n​       `tar -cvf xx.tar xxx`\n​\t   `tar zcvf xx.tar.gz xx`\n**解压**：\n       `tar zxvf xx.tar.gz`\n\t   `tar xvf xx.tar`\n\t   `tar jxvf xx.tar.bz2`\n\t   `bzip2 -d xx.bz2`\n**创建目录xx**：`mkdir xx`\n**创建目录a及以下子目录b**:`mkdir -p a/b`\n**创建xx文件**:`touch xx`\n**查看xx文件**:`cat xx`\n**查看xx文件**:`less xx`(内容多于一屏时，j/k向下/上翻滚)\n**显示绝对路径**：`pwd`\n**删除空目录**：`rmdir`\n**删除xx文件**：`rm xx`\n**强制删除目录a**:`rm -rf a`\n**删除目录a**:`rm -r a`\n**删除父目录a中所有的空子目录**：`cd a; rmdir` \n**复制文件a到文件b**：`cp a b`\n**复制目录a为目录b**：`cp -r a/ b/`\n**从指定文件搜索指定文件**：`grep '指定内容' ./ -R`\n**下载文件到xx目录**：`axel http://... -o xx`\n**给文件a.c加执行权限**：`chmod +x a.c`\n**运行脚本xx.sh**:`sh xx.sh`\n**使.bashrc生效**：`source ~/.bashrc`\n**查看opencv版本**：`pkg-config --modversion opencv`\n**查看opencv安装位置**：`pkg-config -cflags opencv`\n**查找**：`grep -n -H -R \"you want to search \" *`\n**查看文件夹大小**：`du -sh`\n**查看文件夹大小并排序**：`du -h /home/* | sort`\n**查看文件夹大小**：`du -h --max-depth=1 ./`  \n**查看磁盘的使用情况**：`df -h`\n**查看权限**：`ls -l filaname` \n**查看GPU使用率**：`nvidia-smi`\n**查看CPU使用率**：`top`\n**查看文件夹内文件个数**：`ls -l | grep '^-' | wc -l`\n**查看文件夹内目录个数**：`ls -l | grep '^d' | wc -l`\n**查看文件行数**：`wc -l filename`\n**批量更改图片尺寸**：`mogrify -resize 224x224 -format jpg *`\n\n**查看网络设置**：`ifconfig`\n**查看操作系统**：`uname -a`\n**查看ubuntu版本**：`lsb_release -a`\n**查看操作系统位数**：getconf LONG_BIT\n**查看gcc版本**：`ls /usr/bin/gcc*或gcc -v`\n**查看磁盘使用情况**：`baobab`\n**添加用户**：`sudo adduser username`\n**删除用户**: `sudo userdel -r username`\n**添加sudo权限**：`sudo vi /etc/sudoers`\n\n\n**显示终端上所有进程**：`ps -a`\n**查看进程所有者及其他详细信息**：`ps -u`\n**杀掉某进程**：`kill -SIGKILL 进程号`\n**查看使用apt-get的进程**：`ps -aux | grep 'apt-get'`\n**进程暂停**：`kill -STOP pid`\n**进程重启**：`kill -CONT pid`\n\n\n**替换gcc版本**：\n`sudo apt-get install gcc-4.9 gcc-4.9-multilib g++-4.9 g++-4.9-multilib`\n`sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 40`\n`sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5.4 50`\n`sudo update-alternatives --config gcc`\n`sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 50` \n`sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 40`\n`sudo update-alternatives --remove gcc /usr/bin/gcc-4.9`\n\t\n\n**查找命令：**\n> - find . -name 'my*'：搜索当前目录（含子目录，以下同）中，所有文件名以my开头的文\n> - locate ~/m ：搜索用户主目录下，所有以m开头的文件\n> - locate -i ~/m：搜索用户主目录下，所有以m开头的文件，并且忽略大小写\n> - whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息\n> - which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令\n\n\n**Zip相关**：\n\n> - 把一个文件abc.txt和一个目录dir1压缩成为yasuo.zip：zip -r yasuo.zip abc.txt dir1\n> - 下载了一个yasuo.zip文件，想解压缩：unzip yasuo.zip\n> - 当前目录下有abc1.zip，abc2.zip和abc3.zip，一起解压缩它们：unzip abc\\?.zip(注释：?表示一个字符，如果用*表示任意多个字符。)\n> - 有一个很大的压缩文件large.zip，不想解压缩，只想看看它里面有什么：unzip -v large.zip\n> - 下载了一个压缩文件large.zip，想验证一下这个压缩文件是否下载完全了: unzip -t large.zip\n> - 用-v选项发现music.zip压缩文件里面有很多目录和子目录，并且子目录中其实都是歌曲mp3文件，把这些文件都下载到第一级目录，而不是一层一层建目录： unzip -j music.zip\n\n\n\n\n\n\n\n# Vim相关\n**跳转到指令行**：在命令行模式下输入  “:行号”\n**查找字符串**：在命令行模式下输入 “/字符串”，按“n\"键查找下一个\n**批量替换**：在命令行模式下输入 \":%s#abc#def#g\"   将def替换abc\n**批量注释**：ctrl+v进入列模式，大写I进入插入模式，输入注释符//或#,连按两次esc。\n**取消批量注释**：Ctrl + v 进入块选择模式，选中你要删除的行首的注释符号，注意// 要选中两个，选好之后按d即可删除注释\n\n\n\n\n\n\n# 安装驱动\n**查看显卡驱动信息**：cat /proc/driver/nvidia/version\n**查询合适的驱动版本xxx**: https://www.geforce.com/drivers\n```\nsudo apt-get remove --purge nvidia-*\nsudo add-apt-repository ppa:graphics-drivers/ppa\nsudo apt-get update\nsudo service lightdm stop\nsudo apt-get install nvidia-XXX\nsudo service lightdm start\nsudo reboot\nlsmod | grep nvidia   #查看驱动状态是否正常\n```\n\n\n\n\n\n\n# Matlab相关\n后台运行：nohup matlab -nojvm -nodisplay -nosplash < matlabscript.m 1>running.log 2>running.err &\n\n\n\n\n\n\n\n# Python相关\n##路径\nimport os\n先定义一个带路径的文件\nfilename = \"/home/mydir/test.txt\"\n将文件路径分割出来\nfile_dir = os.path.split(filename )[0]\n判断文件路径是否存在，如果不存在，则创建，此处是创建多级目录\n```\n    if not os.path.isdir(file_dir):\n        os.makedirs(file_dir)\n```\n然后再判断文件是否存在，如果不存在，则创建\n```\n    if not os.path.exists(filename ):\n        os.system(r'touch %s' % filename)\n```\n\n\n\n\n\n\n\n\n# git相关\n\n设置Git的user name和email\n\n```\ngit config --global user.name \"yourname\"\ngit config --global user.email \"youremail\"\n```\n\n生成SSH密钥\n\n```\n查看是否已经有了ssh密钥：cd ~/.ssh\n如果没有密钥则不会有此文件夹，有则备份删除\n生存密钥：\nssh-keygen -t rsa -C “haiyan.xu.vip@gmail.com”\n按3个回车，密码为空。\nYour identification has been saved in /home/tekkub/.ssh/id_rsa.\nYour public key has been saved in /home/tekkub/.ssh/id_rsa.pub.\nThe key fingerprint is:\n………………\n最后得到了两个文件：id_rsa和id_rsa.pub\n```\n\n添加密钥到ssh：ssh-add 文件名,需要之前输入密码.\n\n在github上添加ssh密钥，这要添加的是“id_rsa.pub”里面的公钥。\n\n\n\n从本地上传到github:\ngit init\ngit remote add origin https://github.com/nerddd/text.git  \n\n\n","slug":"手册&指南","published":1,"updated":"2018-12-05T09:25:16.952Z","_id":"cjpaxs0mc000os0vwchju6a5e","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Linux操作相关\"><a href=\"#Linux操作相关\" class=\"headerlink\" title=\"Linux操作相关\"></a>Linux操作相关</h1><p>##基本操作<br><strong>打包</strong>：<br>​       <code>tar -cvf xx.tar xxx</code><br>​       <code>tar zcvf xx.tar.gz xx</code><br><strong>解压</strong>：<br>       <code>tar zxvf xx.tar.gz</code><br>       <code>tar xvf xx.tar</code><br>       <code>tar jxvf xx.tar.bz2</code><br>       <code>bzip2 -d xx.bz2</code><br><strong>创建目录xx</strong>：<code>mkdir xx</code><br><strong>创建目录a及以下子目录b</strong>:<code>mkdir -p a/b</code><br><strong>创建xx文件</strong>:<code>touch xx</code><br><strong>查看xx文件</strong>:<code>cat xx</code><br><strong>查看xx文件</strong>:<code>less xx</code>(内容多于一屏时，j/k向下/上翻滚)<br><strong>显示绝对路径</strong>：<code>pwd</code><br><strong>删除空目录</strong>：<code>rmdir</code><br><strong>删除xx文件</strong>：<code>rm xx</code><br><strong>强制删除目录a</strong>:<code>rm -rf a</code><br><strong>删除目录a</strong>:<code>rm -r a</code><br><strong>删除父目录a中所有的空子目录</strong>：<code>cd a; rmdir</code><br><strong>复制文件a到文件b</strong>：<code>cp a b</code><br><strong>复制目录a为目录b</strong>：<code>cp -r a/ b/</code><br><strong>从指定文件搜索指定文件</strong>：<code>grep &#39;指定内容&#39; ./ -R</code><br><strong>下载文件到xx目录</strong>：<code>axel http://... -o xx</code><br><strong>给文件a.c加执行权限</strong>：<code>chmod +x a.c</code><br><strong>运行脚本xx.sh</strong>:<code>sh xx.sh</code><br><strong>使.bashrc生效</strong>：<code>source ~/.bashrc</code><br><strong>查看opencv版本</strong>：<code>pkg-config --modversion opencv</code><br><strong>查看opencv安装位置</strong>：<code>pkg-config -cflags opencv</code><br><strong>查找</strong>：<code>grep -n -H -R &quot;you want to search &quot; *</code><br><strong>查看文件夹大小</strong>：<code>du -sh</code><br><strong>查看文件夹大小并排序</strong>：<code>du -h /home/* | sort</code><br><strong>查看文件夹大小</strong>：<code>du -h --max-depth=1 ./</code><br><strong>查看磁盘的使用情况</strong>：<code>df -h</code><br><strong>查看权限</strong>：<code>ls -l filaname</code><br><strong>查看GPU使用率</strong>：<code>nvidia-smi</code><br><strong>查看CPU使用率</strong>：<code>top</code><br><strong>查看文件夹内文件个数</strong>：<code>ls -l | grep &#39;^-&#39; | wc -l</code><br><strong>查看文件夹内目录个数</strong>：<code>ls -l | grep &#39;^d&#39; | wc -l</code><br><strong>查看文件行数</strong>：<code>wc -l filename</code><br><strong>批量更改图片尺寸</strong>：<code>mogrify -resize 224x224 -format jpg *</code></p>\n<p><strong>查看网络设置</strong>：<code>ifconfig</code><br><strong>查看操作系统</strong>：<code>uname -a</code><br><strong>查看ubuntu版本</strong>：<code>lsb_release -a</code><br><strong>查看操作系统位数</strong>：getconf LONG_BIT<br><strong>查看gcc版本</strong>：<code>ls /usr/bin/gcc*或gcc -v</code><br><strong>查看磁盘使用情况</strong>：<code>baobab</code><br><strong>添加用户</strong>：<code>sudo adduser username</code><br><strong>删除用户</strong>: <code>sudo userdel -r username</code><br><strong>添加sudo权限</strong>：<code>sudo vi /etc/sudoers</code></p>\n<p><strong>显示终端上所有进程</strong>：<code>ps -a</code><br><strong>查看进程所有者及其他详细信息</strong>：<code>ps -u</code><br><strong>杀掉某进程</strong>：<code>kill -SIGKILL 进程号</code><br><strong>查看使用apt-get的进程</strong>：<code>ps -aux | grep &#39;apt-get&#39;</code><br><strong>进程暂停</strong>：<code>kill -STOP pid</code><br><strong>进程重启</strong>：<code>kill -CONT pid</code></p>\n<p><strong>替换gcc版本</strong>：<br><code>sudo apt-get install gcc-4.9 gcc-4.9-multilib g++-4.9 g++-4.9-multilib</code><br><code>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 40</code><br><code>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5.4 50</code><br><code>sudo update-alternatives --config gcc</code><br><code>sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 50</code><br><code>sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 40</code><br><code>sudo update-alternatives --remove gcc /usr/bin/gcc-4.9</code></p>\n<p><strong>查找命令：</strong></p>\n<blockquote>\n<ul>\n<li>find . -name ‘my*’：搜索当前目录（含子目录，以下同）中，所有文件名以my开头的文</li>\n<li>locate ~/m ：搜索用户主目录下，所有以m开头的文件</li>\n<li>locate -i ~/m：搜索用户主目录下，所有以m开头的文件，并且忽略大小写</li>\n<li>whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息</li>\n<li>which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令</li>\n</ul>\n</blockquote>\n<p><strong>Zip相关</strong>：</p>\n<blockquote>\n<ul>\n<li>把一个文件abc.txt和一个目录dir1压缩成为yasuo.zip：zip -r yasuo.zip abc.txt dir1</li>\n<li>下载了一个yasuo.zip文件，想解压缩：unzip yasuo.zip</li>\n<li>当前目录下有abc1.zip，abc2.zip和abc3.zip，一起解压缩它们：unzip abc\\?.zip(注释：?表示一个字符，如果用*表示任意多个字符。)</li>\n<li>有一个很大的压缩文件large.zip，不想解压缩，只想看看它里面有什么：unzip -v large.zip</li>\n<li>下载了一个压缩文件large.zip，想验证一下这个压缩文件是否下载完全了: unzip -t large.zip</li>\n<li>用-v选项发现music.zip压缩文件里面有很多目录和子目录，并且子目录中其实都是歌曲mp3文件，把这些文件都下载到第一级目录，而不是一层一层建目录： unzip -j music.zip</li>\n</ul>\n</blockquote>\n<h1 id=\"Vim相关\"><a href=\"#Vim相关\" class=\"headerlink\" title=\"Vim相关\"></a>Vim相关</h1><p><strong>跳转到指令行</strong>：在命令行模式下输入  “:行号”<br><strong>查找字符串</strong>：在命令行模式下输入 “/字符串”，按“n”键查找下一个<br><strong>批量替换</strong>：在命令行模式下输入 “:%s#abc#def#g”   将def替换abc<br><strong>批量注释</strong>：ctrl+v进入列模式，大写I进入插入模式，输入注释符//或#,连按两次esc。<br><strong>取消批量注释</strong>：Ctrl + v 进入块选择模式，选中你要删除的行首的注释符号，注意// 要选中两个，选好之后按d即可删除注释</p>\n<h1 id=\"安装驱动\"><a href=\"#安装驱动\" class=\"headerlink\" title=\"安装驱动\"></a>安装驱动</h1><p><strong>查看显卡驱动信息</strong>：cat /proc/driver/nvidia/version<br><strong>查询合适的驱动版本xxx</strong>: <a href=\"https://www.geforce.com/drivers\" target=\"_blank\" rel=\"external\">https://www.geforce.com/drivers</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo apt-get remove --purge nvidia-*</div><div class=\"line\">sudo add-apt-repository ppa:graphics-drivers/ppa</div><div class=\"line\">sudo apt-get update</div><div class=\"line\">sudo service lightdm stop</div><div class=\"line\">sudo apt-get install nvidia-XXX</div><div class=\"line\">sudo service lightdm start</div><div class=\"line\">sudo reboot</div><div class=\"line\">lsmod | grep nvidia   #查看驱动状态是否正常</div></pre></td></tr></table></figure></p>\n<h1 id=\"Matlab相关\"><a href=\"#Matlab相关\" class=\"headerlink\" title=\"Matlab相关\"></a>Matlab相关</h1><p>后台运行：nohup matlab -nojvm -nodisplay -nosplash &lt; matlabscript.m 1&gt;running.log 2&gt;running.err &amp;</p>\n<h1 id=\"Python相关\"><a href=\"#Python相关\" class=\"headerlink\" title=\"Python相关\"></a>Python相关</h1><p>##路径<br>import os<br>先定义一个带路径的文件<br>filename = “/home/mydir/test.txt”<br>将文件路径分割出来<br>file_dir = os.path.split(filename )[0]<br>判断文件路径是否存在，如果不存在，则创建，此处是创建多级目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">if not os.path.isdir(file_dir):</div><div class=\"line\">    os.makedirs(file_dir)</div></pre></td></tr></table></figure></p>\n<p>然后再判断文件是否存在，如果不存在，则创建<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">if not os.path.exists(filename ):</div><div class=\"line\">    os.system(r&apos;touch %s&apos; % filename)</div></pre></td></tr></table></figure></p>\n<h1 id=\"git相关\"><a href=\"#git相关\" class=\"headerlink\" title=\"git相关\"></a>git相关</h1><p>设置Git的user name和email</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">git config --global user.name &quot;yourname&quot;</div><div class=\"line\">git config --global user.email &quot;youremail&quot;</div></pre></td></tr></table></figure>\n<p>生成SSH密钥</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">查看是否已经有了ssh密钥：cd ~/.ssh</div><div class=\"line\">如果没有密钥则不会有此文件夹，有则备份删除</div><div class=\"line\">生存密钥：</div><div class=\"line\">ssh-keygen -t rsa -C “haiyan.xu.vip@gmail.com”</div><div class=\"line\">按3个回车，密码为空。</div><div class=\"line\">Your identification has been saved in /home/tekkub/.ssh/id_rsa.</div><div class=\"line\">Your public key has been saved in /home/tekkub/.ssh/id_rsa.pub.</div><div class=\"line\">The key fingerprint is:</div><div class=\"line\">………………</div><div class=\"line\">最后得到了两个文件：id_rsa和id_rsa.pub</div></pre></td></tr></table></figure>\n<p>添加密钥到ssh：ssh-add 文件名,需要之前输入密码.</p>\n<p>在github上添加ssh密钥，这要添加的是“id_rsa.pub”里面的公钥。</p>\n<p>从本地上传到github:<br>git init<br>git remote add origin <a href=\"https://github.com/nerddd/text.git\" target=\"_blank\" rel=\"external\">https://github.com/nerddd/text.git</a>  </p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Linux操作相关\"><a href=\"#Linux操作相关\" class=\"headerlink\" title=\"Linux操作相关\"></a>Linux操作相关</h1><p>##基本操作<br><strong>打包</strong>：<br>​       <code>tar -cvf xx.tar xxx</code><br>​       <code>tar zcvf xx.tar.gz xx</code><br><strong>解压</strong>：<br>       <code>tar zxvf xx.tar.gz</code><br>       <code>tar xvf xx.tar</code><br>       <code>tar jxvf xx.tar.bz2</code><br>       <code>bzip2 -d xx.bz2</code><br><strong>创建目录xx</strong>：<code>mkdir xx</code><br><strong>创建目录a及以下子目录b</strong>:<code>mkdir -p a/b</code><br><strong>创建xx文件</strong>:<code>touch xx</code><br><strong>查看xx文件</strong>:<code>cat xx</code><br><strong>查看xx文件</strong>:<code>less xx</code>(内容多于一屏时，j/k向下/上翻滚)<br><strong>显示绝对路径</strong>：<code>pwd</code><br><strong>删除空目录</strong>：<code>rmdir</code><br><strong>删除xx文件</strong>：<code>rm xx</code><br><strong>强制删除目录a</strong>:<code>rm -rf a</code><br><strong>删除目录a</strong>:<code>rm -r a</code><br><strong>删除父目录a中所有的空子目录</strong>：<code>cd a; rmdir</code><br><strong>复制文件a到文件b</strong>：<code>cp a b</code><br><strong>复制目录a为目录b</strong>：<code>cp -r a/ b/</code><br><strong>从指定文件搜索指定文件</strong>：<code>grep &#39;指定内容&#39; ./ -R</code><br><strong>下载文件到xx目录</strong>：<code>axel http://... -o xx</code><br><strong>给文件a.c加执行权限</strong>：<code>chmod +x a.c</code><br><strong>运行脚本xx.sh</strong>:<code>sh xx.sh</code><br><strong>使.bashrc生效</strong>：<code>source ~/.bashrc</code><br><strong>查看opencv版本</strong>：<code>pkg-config --modversion opencv</code><br><strong>查看opencv安装位置</strong>：<code>pkg-config -cflags opencv</code><br><strong>查找</strong>：<code>grep -n -H -R &quot;you want to search &quot; *</code><br><strong>查看文件夹大小</strong>：<code>du -sh</code><br><strong>查看文件夹大小并排序</strong>：<code>du -h /home/* | sort</code><br><strong>查看文件夹大小</strong>：<code>du -h --max-depth=1 ./</code><br><strong>查看磁盘的使用情况</strong>：<code>df -h</code><br><strong>查看权限</strong>：<code>ls -l filaname</code><br><strong>查看GPU使用率</strong>：<code>nvidia-smi</code><br><strong>查看CPU使用率</strong>：<code>top</code><br><strong>查看文件夹内文件个数</strong>：<code>ls -l | grep &#39;^-&#39; | wc -l</code><br><strong>查看文件夹内目录个数</strong>：<code>ls -l | grep &#39;^d&#39; | wc -l</code><br><strong>查看文件行数</strong>：<code>wc -l filename</code><br><strong>批量更改图片尺寸</strong>：<code>mogrify -resize 224x224 -format jpg *</code></p>\n<p><strong>查看网络设置</strong>：<code>ifconfig</code><br><strong>查看操作系统</strong>：<code>uname -a</code><br><strong>查看ubuntu版本</strong>：<code>lsb_release -a</code><br><strong>查看操作系统位数</strong>：getconf LONG_BIT<br><strong>查看gcc版本</strong>：<code>ls /usr/bin/gcc*或gcc -v</code><br><strong>查看磁盘使用情况</strong>：<code>baobab</code><br><strong>添加用户</strong>：<code>sudo adduser username</code><br><strong>删除用户</strong>: <code>sudo userdel -r username</code><br><strong>添加sudo权限</strong>：<code>sudo vi /etc/sudoers</code></p>\n<p><strong>显示终端上所有进程</strong>：<code>ps -a</code><br><strong>查看进程所有者及其他详细信息</strong>：<code>ps -u</code><br><strong>杀掉某进程</strong>：<code>kill -SIGKILL 进程号</code><br><strong>查看使用apt-get的进程</strong>：<code>ps -aux | grep &#39;apt-get&#39;</code><br><strong>进程暂停</strong>：<code>kill -STOP pid</code><br><strong>进程重启</strong>：<code>kill -CONT pid</code></p>\n<p><strong>替换gcc版本</strong>：<br><code>sudo apt-get install gcc-4.9 gcc-4.9-multilib g++-4.9 g++-4.9-multilib</code><br><code>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.9 40</code><br><code>sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5.4 50</code><br><code>sudo update-alternatives --config gcc</code><br><code>sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 50</code><br><code>sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-4.9 40</code><br><code>sudo update-alternatives --remove gcc /usr/bin/gcc-4.9</code></p>\n<p><strong>查找命令：</strong></p>\n<blockquote>\n<ul>\n<li>find . -name ‘my*’：搜索当前目录（含子目录，以下同）中，所有文件名以my开头的文</li>\n<li>locate ~/m ：搜索用户主目录下，所有以m开头的文件</li>\n<li>locate -i ~/m：搜索用户主目录下，所有以m开头的文件，并且忽略大小写</li>\n<li>whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息</li>\n<li>which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令</li>\n</ul>\n</blockquote>\n<p><strong>Zip相关</strong>：</p>\n<blockquote>\n<ul>\n<li>把一个文件abc.txt和一个目录dir1压缩成为yasuo.zip：zip -r yasuo.zip abc.txt dir1</li>\n<li>下载了一个yasuo.zip文件，想解压缩：unzip yasuo.zip</li>\n<li>当前目录下有abc1.zip，abc2.zip和abc3.zip，一起解压缩它们：unzip abc\\?.zip(注释：?表示一个字符，如果用*表示任意多个字符。)</li>\n<li>有一个很大的压缩文件large.zip，不想解压缩，只想看看它里面有什么：unzip -v large.zip</li>\n<li>下载了一个压缩文件large.zip，想验证一下这个压缩文件是否下载完全了: unzip -t large.zip</li>\n<li>用-v选项发现music.zip压缩文件里面有很多目录和子目录，并且子目录中其实都是歌曲mp3文件，把这些文件都下载到第一级目录，而不是一层一层建目录： unzip -j music.zip</li>\n</ul>\n</blockquote>\n<h1 id=\"Vim相关\"><a href=\"#Vim相关\" class=\"headerlink\" title=\"Vim相关\"></a>Vim相关</h1><p><strong>跳转到指令行</strong>：在命令行模式下输入  “:行号”<br><strong>查找字符串</strong>：在命令行模式下输入 “/字符串”，按“n”键查找下一个<br><strong>批量替换</strong>：在命令行模式下输入 “:%s#abc#def#g”   将def替换abc<br><strong>批量注释</strong>：ctrl+v进入列模式，大写I进入插入模式，输入注释符//或#,连按两次esc。<br><strong>取消批量注释</strong>：Ctrl + v 进入块选择模式，选中你要删除的行首的注释符号，注意// 要选中两个，选好之后按d即可删除注释</p>\n<h1 id=\"安装驱动\"><a href=\"#安装驱动\" class=\"headerlink\" title=\"安装驱动\"></a>安装驱动</h1><p><strong>查看显卡驱动信息</strong>：cat /proc/driver/nvidia/version<br><strong>查询合适的驱动版本xxx</strong>: <a href=\"https://www.geforce.com/drivers\" target=\"_blank\" rel=\"external\">https://www.geforce.com/drivers</a><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo apt-get remove --purge nvidia-*</div><div class=\"line\">sudo add-apt-repository ppa:graphics-drivers/ppa</div><div class=\"line\">sudo apt-get update</div><div class=\"line\">sudo service lightdm stop</div><div class=\"line\">sudo apt-get install nvidia-XXX</div><div class=\"line\">sudo service lightdm start</div><div class=\"line\">sudo reboot</div><div class=\"line\">lsmod | grep nvidia   #查看驱动状态是否正常</div></pre></td></tr></table></figure></p>\n<h1 id=\"Matlab相关\"><a href=\"#Matlab相关\" class=\"headerlink\" title=\"Matlab相关\"></a>Matlab相关</h1><p>后台运行：nohup matlab -nojvm -nodisplay -nosplash &lt; matlabscript.m 1&gt;running.log 2&gt;running.err &amp;</p>\n<h1 id=\"Python相关\"><a href=\"#Python相关\" class=\"headerlink\" title=\"Python相关\"></a>Python相关</h1><p>##路径<br>import os<br>先定义一个带路径的文件<br>filename = “/home/mydir/test.txt”<br>将文件路径分割出来<br>file_dir = os.path.split(filename )[0]<br>判断文件路径是否存在，如果不存在，则创建，此处是创建多级目录<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">if not os.path.isdir(file_dir):</div><div class=\"line\">    os.makedirs(file_dir)</div></pre></td></tr></table></figure></p>\n<p>然后再判断文件是否存在，如果不存在，则创建<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">if not os.path.exists(filename ):</div><div class=\"line\">    os.system(r&apos;touch %s&apos; % filename)</div></pre></td></tr></table></figure></p>\n<h1 id=\"git相关\"><a href=\"#git相关\" class=\"headerlink\" title=\"git相关\"></a>git相关</h1><p>设置Git的user name和email</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">git config --global user.name &quot;yourname&quot;</div><div class=\"line\">git config --global user.email &quot;youremail&quot;</div></pre></td></tr></table></figure>\n<p>生成SSH密钥</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div></pre></td><td class=\"code\"><pre><div class=\"line\">查看是否已经有了ssh密钥：cd ~/.ssh</div><div class=\"line\">如果没有密钥则不会有此文件夹，有则备份删除</div><div class=\"line\">生存密钥：</div><div class=\"line\">ssh-keygen -t rsa -C “haiyan.xu.vip@gmail.com”</div><div class=\"line\">按3个回车，密码为空。</div><div class=\"line\">Your identification has been saved in /home/tekkub/.ssh/id_rsa.</div><div class=\"line\">Your public key has been saved in /home/tekkub/.ssh/id_rsa.pub.</div><div class=\"line\">The key fingerprint is:</div><div class=\"line\">………………</div><div class=\"line\">最后得到了两个文件：id_rsa和id_rsa.pub</div></pre></td></tr></table></figure>\n<p>添加密钥到ssh：ssh-add 文件名,需要之前输入密码.</p>\n<p>在github上添加ssh密钥，这要添加的是“id_rsa.pub”里面的公钥。</p>\n<p>从本地上传到github:<br>git init<br>git remote add origin <a href=\"https://github.com/nerddd/text.git\" target=\"_blank\" rel=\"external\">https://github.com/nerddd/text.git</a>  </p>\n"},{"title":"杂知识点","date":"2017-06-01T01:42:05.000Z","description":"深度学习的一些基础知识点","mathjax":true,"_content":"\n# 分类与回归\n\n**本节部分转载于穆文发表于知乎的[分类与回归区别是什么](https://www.zhihu.com/question/21329754/answer/151216012)下面的回答，获得原作者授权**\n\n*分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。*\n\n1. Logistic Regression&Linear Regression:\n\n   + Linear Regression:输出一个标量**wx+b**，这个值是连续值，用以回归问题\n   + Logistic Regression:将上面的**wx+b**通过**sigmoid**函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题\n   + 对于N分类问题，可以先计算N组w值不同的**wx+b** ，然后归一化，比如**softmax**函数变成N个类上的概率，用以多分类\n\n2. SVR &SVM\n\n   + SVR:输出**wx+b**，即某个样本点到分类面的距离，是连续值，属于回归问题\n   + SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类\n\n3. Naive Bayes用来分类和回归\n\n4. 前馈神经网络（CNN系列）用于分类和回归\n\n   + 回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出看做向量v，现全部连接到一个神经元上，这个神经元的输出**wv+b**，是一个连续值，处理回归问题，和Linear Regression的思想一样\n   + 分类：将m个神经元最后连接到N个神经元，有N组不同的**wv+b**，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题\n\n5. 循环神经网络（RNN系列）用于分类和回归\n\n   + 回归和分类与CNN类似，输出层的值**y=wx+b**，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列\n\n6. Logistic回归&SVM\n\n    两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。[^1]\n\n    线性模型的表达式为\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\n$$\n\n​\t将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。\n\n​\t用n表示Feature数量,m表示训练集个数。下面分情况讨论[^2]：\n\n- n很大，m很小\n  n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择`线性kernel的SVM`，也可以选择`Logistic回归`。\n- n很小，m中等 \n  n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择`非线性kernel的SVM`，比如`高斯核kernel的SVM`。\n- n很小，m很大\n  n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，`带核函数的SVM`计算也非常慢。所以此时应该选`线性kernel的SVM`，也可以选择`Logistic回归`。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。\n\n\n\n\n\n\n\n# 一些概念\n\n## 迁移学习[^3]：\n\n有监督预训练(*supervised pre-training*)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。\n\n+ **NMS(非极大值抑制)：**\n\n在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列[算法](http://lib.csdn.net/base/datastructure)中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。\n\n定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。\n(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;\n(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。\n(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。\n就这样一直重复，找到所有被保留下来的矩形框。\n\n+ **IoU(交并比)：**\n\n物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们[算法](http://lib.csdn.net/base/datastructure)不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式\n$$\nIoU=\\frac{A\\cap B}{A\\cup B}\n$$\n\n## 准确率&精确率&召回率:\n\n​\t_准确率是正确预测的样本占总的预测样本比例_\n​\t*精确率是预测为正的样本中有多少是真的正类*\n​\t*召回率是样本中有多少正例被正确的预测*\n​\t_F值=准确率\\*召回率\\*2/(准确率+召回率)，是准确率和召回率的调和平均值_\n\n​*TP*：正类被预测为正类\n​*FN*：正类被预测为负类\n​*FP*：负类被预测为正类\n​*TN*：负类被预测为负类\n\n$$\n准确率=\\frac{TP+TN}{TP+TF+FN+FP}\n$$\n\n$$\n精确率=\\frac{TP}{TP+FP}\n$$\n\n$$\n召回率=\\frac{TP}{TP+FN}\n$$\n\n## 卷积计算后的图片尺寸：\n\n$$\noutputsize=\\frac{imagesize+2*padding-kernelsize}{stride}+1\n$$\n\n# RankBoost:\n\n​\tRankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类\t器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1>r2>r3>r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题\n\n\n\n# 真阳率，假阳率，AUC，ROC\n\n![pic1](http://img.blog.csdn.net/20150919111349931)\n\n$真阳率=\\frac{a}{a+c}$:含义是检测出来的真阳性样本数除以所有真实阳性样本数。\n\n$假阳率=\\frac{b}{b+d}$:含义是检测出来的假阳性样本数除以所有真实阴性样本数\n\nROC曲线就是把假阳率当x轴，真阳率当y轴画一个二维平面直角坐标系。然后不断调整检测方法（或机器学习中的分类器）的阈值，即最终得分高于某个值就是阳性，反之就是阴性，得到不同的真阳率和假阳率数值，然后描点。就可以得到一条ROC曲线。 \n需要注意的是，ROC曲线必定起于（0，0），止于（1，1）。因为，当全都判断为阴性(-)时，就是（0，0）；全部判断为阳性(+)时就是（1，1）。这两点间斜率为1的线段表示随机分类器（对真实的正负样本没有区分能力）。所以一般分类器需要在这条线上方\n\n![pic2](http://img.blog.csdn.net/20150919114145488)\n\nAUC就是ROC曲线下方的面积，越接近1表示分类器越好。\n\n# 参考文献\n\n[^1]: [SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n[^2]: [SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n[^3]: [[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n\n[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n\n[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n[机器学习算法常用指标总结](http://www.cnblogs.com/maybe2030/p/5375175.html)\n\n\n\n","source":"_posts/杂知识点.md","raw":"---\ntitle: 杂知识点\ndate: 2017-06-01 09:42:05\ncategories: 深度学习\ntags: [深度学习,神经网络]\ndescription: 深度学习的一些基础知识点\nmathjax: true\n---\n\n# 分类与回归\n\n**本节部分转载于穆文发表于知乎的[分类与回归区别是什么](https://www.zhihu.com/question/21329754/answer/151216012)下面的回答，获得原作者授权**\n\n*分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。*\n\n1. Logistic Regression&Linear Regression:\n\n   + Linear Regression:输出一个标量**wx+b**，这个值是连续值，用以回归问题\n   + Logistic Regression:将上面的**wx+b**通过**sigmoid**函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题\n   + 对于N分类问题，可以先计算N组w值不同的**wx+b** ，然后归一化，比如**softmax**函数变成N个类上的概率，用以多分类\n\n2. SVR &SVM\n\n   + SVR:输出**wx+b**，即某个样本点到分类面的距离，是连续值，属于回归问题\n   + SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类\n\n3. Naive Bayes用来分类和回归\n\n4. 前馈神经网络（CNN系列）用于分类和回归\n\n   + 回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出看做向量v，现全部连接到一个神经元上，这个神经元的输出**wv+b**，是一个连续值，处理回归问题，和Linear Regression的思想一样\n   + 分类：将m个神经元最后连接到N个神经元，有N组不同的**wv+b**，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题\n\n5. 循环神经网络（RNN系列）用于分类和回归\n\n   + 回归和分类与CNN类似，输出层的值**y=wx+b**，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列\n\n6. Logistic回归&SVM\n\n    两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。[^1]\n\n    线性模型的表达式为\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\n$$\n\n​\t将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。\n\n​\t用n表示Feature数量,m表示训练集个数。下面分情况讨论[^2]：\n\n- n很大，m很小\n  n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择`线性kernel的SVM`，也可以选择`Logistic回归`。\n- n很小，m中等 \n  n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择`非线性kernel的SVM`，比如`高斯核kernel的SVM`。\n- n很小，m很大\n  n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，`带核函数的SVM`计算也非常慢。所以此时应该选`线性kernel的SVM`，也可以选择`Logistic回归`。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。\n\n\n\n\n\n\n\n# 一些概念\n\n## 迁移学习[^3]：\n\n有监督预训练(*supervised pre-training*)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。\n\n+ **NMS(非极大值抑制)：**\n\n在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列[算法](http://lib.csdn.net/base/datastructure)中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。\n\n定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。\n(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;\n(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。\n(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。\n就这样一直重复，找到所有被保留下来的矩形框。\n\n+ **IoU(交并比)：**\n\n物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们[算法](http://lib.csdn.net/base/datastructure)不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式\n$$\nIoU=\\frac{A\\cap B}{A\\cup B}\n$$\n\n## 准确率&精确率&召回率:\n\n​\t_准确率是正确预测的样本占总的预测样本比例_\n​\t*精确率是预测为正的样本中有多少是真的正类*\n​\t*召回率是样本中有多少正例被正确的预测*\n​\t_F值=准确率\\*召回率\\*2/(准确率+召回率)，是准确率和召回率的调和平均值_\n\n​*TP*：正类被预测为正类\n​*FN*：正类被预测为负类\n​*FP*：负类被预测为正类\n​*TN*：负类被预测为负类\n\n$$\n准确率=\\frac{TP+TN}{TP+TF+FN+FP}\n$$\n\n$$\n精确率=\\frac{TP}{TP+FP}\n$$\n\n$$\n召回率=\\frac{TP}{TP+FN}\n$$\n\n## 卷积计算后的图片尺寸：\n\n$$\noutputsize=\\frac{imagesize+2*padding-kernelsize}{stride}+1\n$$\n\n# RankBoost:\n\n​\tRankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类\t器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1>r2>r3>r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题\n\n\n\n# 真阳率，假阳率，AUC，ROC\n\n![pic1](http://img.blog.csdn.net/20150919111349931)\n\n$真阳率=\\frac{a}{a+c}$:含义是检测出来的真阳性样本数除以所有真实阳性样本数。\n\n$假阳率=\\frac{b}{b+d}$:含义是检测出来的假阳性样本数除以所有真实阴性样本数\n\nROC曲线就是把假阳率当x轴，真阳率当y轴画一个二维平面直角坐标系。然后不断调整检测方法（或机器学习中的分类器）的阈值，即最终得分高于某个值就是阳性，反之就是阴性，得到不同的真阳率和假阳率数值，然后描点。就可以得到一条ROC曲线。 \n需要注意的是，ROC曲线必定起于（0，0），止于（1，1）。因为，当全都判断为阴性(-)时，就是（0，0）；全部判断为阳性(+)时就是（1，1）。这两点间斜率为1的线段表示随机分类器（对真实的正负样本没有区分能力）。所以一般分类器需要在这条线上方\n\n![pic2](http://img.blog.csdn.net/20150919114145488)\n\nAUC就是ROC曲线下方的面积，越接近1表示分类器越好。\n\n# 参考文献\n\n[^1]: [SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n[^2]: [SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n[^3]: [[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n\n[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n\n[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n[机器学习算法常用指标总结](http://www.cnblogs.com/maybe2030/p/5375175.html)\n\n\n\n","slug":"杂知识点","published":1,"updated":"2018-12-05T09:25:47.318Z","_id":"cjpaxs0ms000rs0vwcp17ll2y","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"分类与回归\"><a href=\"#分类与回归\" class=\"headerlink\" title=\"分类与回归\"></a>分类与回归</h1><p><strong>本节部分转载于穆文发表于知乎的<a href=\"https://www.zhihu.com/question/21329754/answer/151216012\" target=\"_blank\" rel=\"external\">分类与回归区别是什么</a>下面的回答，获得原作者授权</strong></p>\n<p><em>分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。</em></p>\n<ol>\n<li><p>Logistic Regression&amp;Linear Regression:</p>\n<ul>\n<li>Linear Regression:输出一个标量<strong>wx+b</strong>，这个值是连续值，用以回归问题</li>\n<li>Logistic Regression:将上面的<strong>wx+b</strong>通过<strong>sigmoid</strong>函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题</li>\n<li>对于N分类问题，可以先计算N组w值不同的<strong>wx+b</strong> ，然后归一化，比如<strong>softmax</strong>函数变成N个类上的概率，用以多分类</li>\n</ul>\n</li>\n<li><p>SVR &amp;SVM</p>\n<ul>\n<li>SVR:输出<strong>wx+b</strong>，即某个样本点到分类面的距离，是连续值，属于回归问题</li>\n<li>SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类</li>\n</ul>\n</li>\n<li><p>Naive Bayes用来分类和回归</p>\n</li>\n<li><p>前馈神经网络（CNN系列）用于分类和回归</p>\n<ul>\n<li>回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出看做向量v，现全部连接到一个神经元上，这个神经元的输出<strong>wv+b</strong>，是一个连续值，处理回归问题，和Linear Regression的思想一样</li>\n<li>分类：将m个神经元最后连接到N个神经元，有N组不同的<strong>wv+b</strong>，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题</li>\n</ul>\n</li>\n<li><p>循环神经网络（RNN系列）用于分类和回归</p>\n<ul>\n<li>回归和分类与CNN类似，输出层的值<strong>y=wx+b</strong>，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列</li>\n</ul>\n</li>\n<li><p>Logistic回归&amp;SVM</p>\n<p> 两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。<a href=\"[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\">^1</a></p>\n<p> 线性模型的表达式为</p>\n</li>\n</ol>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n<br>$$</p>\n<p>​    将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。</p>\n<p>​    用n表示Feature数量,m表示训练集个数。下面分情况讨论<a href=\"[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\">^2</a>：</p>\n<ul>\n<li>n很大，m很小<br>n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。</li>\n<li>n很小，m中等<br>n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择<code>非线性kernel的SVM</code>，比如<code>高斯核kernel的SVM</code>。</li>\n<li>n很小，m很大<br>n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，<code>带核函数的SVM</code>计算也非常慢。所以此时应该选<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。</li>\n</ul>\n<h1 id=\"一些概念\"><a href=\"#一些概念\" class=\"headerlink\" title=\"一些概念\"></a>一些概念</h1><h2 id=\"迁移学习-3：\"><a href=\"#迁移学习-3：\" class=\"headerlink\" title=\"迁移学习^3：\"></a>迁移学习<a href=\"[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\">^3</a>：</h2><p>有监督预训练(<em>supervised pre-training</em>)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。</p>\n<ul>\n<li><strong>NMS(非极大值抑制)：</strong></li>\n</ul>\n<p>在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。</p>\n<p>定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。<br>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;<br>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。<br>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。<br>就这样一直重复，找到所有被保留下来的矩形框。</p>\n<ul>\n<li><strong>IoU(交并比)：</strong></li>\n</ul>\n<p>物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式<br>$$<br>IoU=\\frac{A\\cap B}{A\\cup B}<br>$$</p>\n<h2 id=\"准确率-amp-精确率-amp-召回率\"><a href=\"#准确率-amp-精确率-amp-召回率\" class=\"headerlink\" title=\"准确率&amp;精确率&amp;召回率:\"></a>准确率&amp;精确率&amp;召回率:</h2><p>​    <em>准确率是正确预测的样本占总的预测样本比例</em><br>​    <em>精确率是预测为正的样本中有多少是真的正类</em><br>​    <em>召回率是样本中有多少正例被正确的预测</em><br>​    <em>F值=准确率*召回率*2/(准确率+召回率)，是准确率和召回率的调和平均值</em></p>\n<p>​<em>TP</em>：正类被预测为正类<br>​<em>FN</em>：正类被预测为负类<br>​<em>FP</em>：负类被预测为正类<br>​<em>TN</em>：负类被预测为负类</p>\n<p>$$<br>准确率=\\frac{TP+TN}{TP+TF+FN+FP}<br>$$</p>\n<p>$$<br>精确率=\\frac{TP}{TP+FP}<br>$$</p>\n<p>$$<br>召回率=\\frac{TP}{TP+FN}<br>$$</p>\n<h2 id=\"卷积计算后的图片尺寸：\"><a href=\"#卷积计算后的图片尺寸：\" class=\"headerlink\" title=\"卷积计算后的图片尺寸：\"></a>卷积计算后的图片尺寸：</h2><p>$$<br>outputsize=\\frac{imagesize+2*padding-kernelsize}{stride}+1<br>$$</p>\n<h1 id=\"RankBoost\"><a href=\"#RankBoost\" class=\"headerlink\" title=\"RankBoost:\"></a>RankBoost:</h1><p>​    RankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类    器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1&gt;r2&gt;r3&gt;r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题</p>\n<h1 id=\"真阳率，假阳率，AUC，ROC\"><a href=\"#真阳率，假阳率，AUC，ROC\" class=\"headerlink\" title=\"真阳率，假阳率，AUC，ROC\"></a>真阳率，假阳率，AUC，ROC</h1><p><img src=\"http://img.blog.csdn.net/20150919111349931\" alt=\"pic1\"></p>\n<p>$真阳率=\\frac{a}{a+c}$:含义是检测出来的真阳性样本数除以所有真实阳性样本数。</p>\n<p>$假阳率=\\frac{b}{b+d}$:含义是检测出来的假阳性样本数除以所有真实阴性样本数</p>\n<p>ROC曲线就是把假阳率当x轴，真阳率当y轴画一个二维平面直角坐标系。然后不断调整检测方法（或机器学习中的分类器）的阈值，即最终得分高于某个值就是阳性，反之就是阴性，得到不同的真阳率和假阳率数值，然后描点。就可以得到一条ROC曲线。<br>需要注意的是，ROC曲线必定起于（0，0），止于（1，1）。因为，当全都判断为阴性(-)时，就是（0，0）；全部判断为阳性(+)时就是（1，1）。这两点间斜率为1的线段表示随机分类器（对真实的正负样本没有区分能力）。所以一般分类器需要在这条线上方</p>\n<p><img src=\"http://img.blog.csdn.net/20150919114145488\" alt=\"pic2\"></p>\n<p>AUC就是ROC曲线下方的面积，越接近1表示分类器越好。</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p><a href=\"https://www.zhihu.com/question/21704547/answer/20293255\" target=\"_blank\" rel=\"external\">SVM和logistic回归分别在什么情况下使用</a></p>\n<p><a href=\"http://blog.csdn.net/ybdesire/article/details/54143481\" target=\"_blank\" rel=\"external\">SVM和Logistic的区别</a></p>\n<p>[<a href=\"http://blog.csdn.net/zhang_shuai12/article/details/52716952\" target=\"_blank\" rel=\"external\">物体检测中常用的几个概念迁移学习、IOU、NMS理解</a>]</p>\n<p><a href=\"http://www.cnblogs.com/maybe2030/p/5375175.html\" target=\"_blank\" rel=\"external\">机器学习算法常用指标总结</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"分类与回归\"><a href=\"#分类与回归\" class=\"headerlink\" title=\"分类与回归\"></a>分类与回归</h1><p><strong>本节部分转载于穆文发表于知乎的<a href=\"https://www.zhihu.com/question/21329754/answer/151216012\" target=\"_blank\" rel=\"external\">分类与回归区别是什么</a>下面的回答，获得原作者授权</strong></p>\n<p><em>分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。</em></p>\n<ol>\n<li><p>Logistic Regression&amp;Linear Regression:</p>\n<ul>\n<li>Linear Regression:输出一个标量<strong>wx+b</strong>，这个值是连续值，用以回归问题</li>\n<li>Logistic Regression:将上面的<strong>wx+b</strong>通过<strong>sigmoid</strong>函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题</li>\n<li>对于N分类问题，可以先计算N组w值不同的<strong>wx+b</strong> ，然后归一化，比如<strong>softmax</strong>函数变成N个类上的概率，用以多分类</li>\n</ul>\n</li>\n<li><p>SVR &amp;SVM</p>\n<ul>\n<li>SVR:输出<strong>wx+b</strong>，即某个样本点到分类面的距离，是连续值，属于回归问题</li>\n<li>SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类</li>\n</ul>\n</li>\n<li><p>Naive Bayes用来分类和回归</p>\n</li>\n<li><p>前馈神经网络（CNN系列）用于分类和回归</p>\n<ul>\n<li>回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出看做向量v，现全部连接到一个神经元上，这个神经元的输出<strong>wv+b</strong>，是一个连续值，处理回归问题，和Linear Regression的思想一样</li>\n<li>分类：将m个神经元最后连接到N个神经元，有N组不同的<strong>wv+b</strong>，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题</li>\n</ul>\n</li>\n<li><p>循环神经网络（RNN系列）用于分类和回归</p>\n<ul>\n<li>回归和分类与CNN类似，输出层的值<strong>y=wx+b</strong>，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列</li>\n</ul>\n</li>\n<li><p>Logistic回归&amp;SVM</p>\n<p> 两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。<a href=\"[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\">^1</a></p>\n<p> 线性模型的表达式为</p>\n</li>\n</ol>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n<br>$$</p>\n<p>​    将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。</p>\n<p>​    用n表示Feature数量,m表示训练集个数。下面分情况讨论<a href=\"[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\">^2</a>：</p>\n<ul>\n<li>n很大，m很小<br>n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。</li>\n<li>n很小，m中等<br>n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择<code>非线性kernel的SVM</code>，比如<code>高斯核kernel的SVM</code>。</li>\n<li>n很小，m很大<br>n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，<code>带核函数的SVM</code>计算也非常慢。所以此时应该选<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。</li>\n</ul>\n<h1 id=\"一些概念\"><a href=\"#一些概念\" class=\"headerlink\" title=\"一些概念\"></a>一些概念</h1><h2 id=\"迁移学习-3：\"><a href=\"#迁移学习-3：\" class=\"headerlink\" title=\"迁移学习^3：\"></a>迁移学习<a href=\"[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\">^3</a>：</h2><p>有监督预训练(<em>supervised pre-training</em>)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。</p>\n<ul>\n<li><strong>NMS(非极大值抑制)：</strong></li>\n</ul>\n<p>在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。</p>\n<p>定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。<br>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;<br>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。<br>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。<br>就这样一直重复，找到所有被保留下来的矩形框。</p>\n<ul>\n<li><strong>IoU(交并比)：</strong></li>\n</ul>\n<p>物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式<br>$$<br>IoU=\\frac{A\\cap B}{A\\cup B}<br>$$</p>\n<h2 id=\"准确率-amp-精确率-amp-召回率\"><a href=\"#准确率-amp-精确率-amp-召回率\" class=\"headerlink\" title=\"准确率&amp;精确率&amp;召回率:\"></a>准确率&amp;精确率&amp;召回率:</h2><p>​    <em>准确率是正确预测的样本占总的预测样本比例</em><br>​    <em>精确率是预测为正的样本中有多少是真的正类</em><br>​    <em>召回率是样本中有多少正例被正确的预测</em><br>​    <em>F值=准确率*召回率*2/(准确率+召回率)，是准确率和召回率的调和平均值</em></p>\n<p>​<em>TP</em>：正类被预测为正类<br>​<em>FN</em>：正类被预测为负类<br>​<em>FP</em>：负类被预测为正类<br>​<em>TN</em>：负类被预测为负类</p>\n<p>$$<br>准确率=\\frac{TP+TN}{TP+TF+FN+FP}<br>$$</p>\n<p>$$<br>精确率=\\frac{TP}{TP+FP}<br>$$</p>\n<p>$$<br>召回率=\\frac{TP}{TP+FN}<br>$$</p>\n<h2 id=\"卷积计算后的图片尺寸：\"><a href=\"#卷积计算后的图片尺寸：\" class=\"headerlink\" title=\"卷积计算后的图片尺寸：\"></a>卷积计算后的图片尺寸：</h2><p>$$<br>outputsize=\\frac{imagesize+2*padding-kernelsize}{stride}+1<br>$$</p>\n<h1 id=\"RankBoost\"><a href=\"#RankBoost\" class=\"headerlink\" title=\"RankBoost:\"></a>RankBoost:</h1><p>​    RankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类    器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1&gt;r2&gt;r3&gt;r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题</p>\n<h1 id=\"真阳率，假阳率，AUC，ROC\"><a href=\"#真阳率，假阳率，AUC，ROC\" class=\"headerlink\" title=\"真阳率，假阳率，AUC，ROC\"></a>真阳率，假阳率，AUC，ROC</h1><p><img src=\"http://img.blog.csdn.net/20150919111349931\" alt=\"pic1\"></p>\n<p>$真阳率=\\frac{a}{a+c}$:含义是检测出来的真阳性样本数除以所有真实阳性样本数。</p>\n<p>$假阳率=\\frac{b}{b+d}$:含义是检测出来的假阳性样本数除以所有真实阴性样本数</p>\n<p>ROC曲线就是把假阳率当x轴，真阳率当y轴画一个二维平面直角坐标系。然后不断调整检测方法（或机器学习中的分类器）的阈值，即最终得分高于某个值就是阳性，反之就是阴性，得到不同的真阳率和假阳率数值，然后描点。就可以得到一条ROC曲线。<br>需要注意的是，ROC曲线必定起于（0，0），止于（1，1）。因为，当全都判断为阴性(-)时，就是（0，0）；全部判断为阳性(+)时就是（1，1）。这两点间斜率为1的线段表示随机分类器（对真实的正负样本没有区分能力）。所以一般分类器需要在这条线上方</p>\n<p><img src=\"http://img.blog.csdn.net/20150919114145488\" alt=\"pic2\"></p>\n<p>AUC就是ROC曲线下方的面积，越接近1表示分类器越好。</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p><a href=\"https://www.zhihu.com/question/21704547/answer/20293255\" target=\"_blank\" rel=\"external\">SVM和logistic回归分别在什么情况下使用</a></p>\n<p><a href=\"http://blog.csdn.net/ybdesire/article/details/54143481\" target=\"_blank\" rel=\"external\">SVM和Logistic的区别</a></p>\n<p>[<a href=\"http://blog.csdn.net/zhang_shuai12/article/details/52716952\" target=\"_blank\" rel=\"external\">物体检测中常用的几个概念迁移学习、IOU、NMS理解</a>]</p>\n<p><a href=\"http://www.cnblogs.com/maybe2030/p/5375175.html\" target=\"_blank\" rel=\"external\">机器学习算法常用指标总结</a></p>\n"},{"title":"每日阅读","date":"2018-12-05T08:52:09.000Z","description":"每天阅读论文的笔记","mathjax":null,"_content":"\n# Rethinking ImageNet Pre-training\n\n**Paper**:[Rethinking ImageNet Pre-training](http://arxiv.org/pdf/1811.10104v1)\n\n**code**:-\n\n**Author**:Kaiming He、 Ross Girshick 、Piotr Dollar 、Facebook AI Research (FAIR) \n\n**time**:2018\n\n## Abstract\n\nWe report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained **from random initialization**.The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine -tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge.Training from random initialization is surprisingly robust; our results hold even when: (i) Using only 10% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics.Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy.To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data--a result on par with the top COCO 2017 competition results that used ImageNet pre-training.These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of ' pre-training and fine -tuning' in computer vision.\n\n**翻译**：我们使用随机初始化训练的标准模型报告在COCO数据集上的物体检测和物体分割的竞争结果。即使使用baseline model的超参数，结果也不比它们的ImageNet预训练对应物差（Mask R-CNN）针对精细调整预训练模型进行了优化，唯一的例外是增加训练迭代次数，因此随机初始化模型可以收敛。随机初始化训练非常强大;我们的结果即使在以下情况下仍然存在：（i）仅使用10％的训练数据，（ii）用于更深和更宽的模型，以及（iii）用于多个任务和指标。实验表明在ImageNet预训练速度能加速训练时候的收敛速度，但不一定提供正则化或提高最终在目标任务上的准确性。为了突破这个范围，我们在不使用任何外部数据的情况下在COCO对象检测上达到了50.9 AP  - 这个结果与2017年COCO相同使用ImageNet预训练的最好的竞赛结果差不多。这些结论挑战了ImageNet与训练对于依赖性任务的传统智慧，我们期望这些发现将鼓励人们重新思考计算机视觉中的“pre-training and fine-tuneing”范式。\n\n![543926824(1](/images/154392682.jpg)\n\n## Contribution\n\n- 采用基于ImageNet的预训练模型参数可以加快模型的收敛速度，尤其是在训练初期，而基于随机初始化网络结构参数训练模型时在采用合适的归一化方式且迭代足够多的次数后也能够达到相同的效果\n\n![img](/images/154392682.jpg)\n\n- 采用基于ImageNet的预训练模型参数训练得到的模型泛化能力不如基于随机初始化网络结构训练得到的模型，前者更容易出现过拟合，因此需要选择合适的超参数训练模型，从而尽可能减少过拟合风险（前提是数据量足够大，比如10K以上的COCO数据集）\n\n  ![Q图片2018120420442](/images/QQ%E5%9B%BE%E7%89%8720181204204421.png)\n\n- 当迁移任务的目标对空间位置信息比较敏感时（比如目标检测、人体关键点检测任务），采用基于ImageNet的预训练模型并不会有太大的帮助\n\n## Summary\n\n\n\n# Deep Neural Decision  Forests\n\n**paper**:[Deep neural decision forests](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf)\n\n**code**:\n\n**Author**:\n\n**time:**ICCV2015\n\n## Abstract\n\nWe present Deep Neural Decision Forests – a novel approach that unifies classification trees with the representation learning  known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find onpar or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84%/6.38% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67% error obtained by the best GoogLeNet architecture (7 models, 144 crops). \n\n**翻译**:我们提出了深度神经决策森林--一个新的方法可以通过端到端的训练将分类树和深度卷积卷积网络中的表征学习功能统一起来。为了把这两个方法结合起来，我们引入了一个随机可微分的决策树模型，控制一般在（深度）卷积网络的初始层进行的表征学习。我们的模型不同于传统的深度网络，因为决策森林进行最后的预测，它与传统的决策森林不同，因为我们提出了分裂节点和叶子节点参数的联合全局优化原则。我们展示了在类似MNIST和ImageNet的benchmark上的实验结果，并与最先进的深度模型进行比较，发现了优于它们的结果。最值得注意的是，当我们在ImageNet验证数据上获得的Top5误差仅为7.84%/6.38%。因此即使没有任何形式的训练数据增强，我们也在最佳的GoogleNet架构(7种模型，144 crops）上得到6.67%的误差。\n\n![Q图片2018120420311](/images/QQ%E5%9B%BE%E7%89%8720181204203113.png)\n\n## Contribution\n\n## Summary\n\n","source":"_posts/每日阅读.md","raw":"---\ntitle: 每日阅读\ndate: 2018-12-05 16:52:09\ncategories: 论文笔记\ntags: [杂]\ndescription: 每天阅读论文的笔记\nmathjax:\n---\n\n# Rethinking ImageNet Pre-training\n\n**Paper**:[Rethinking ImageNet Pre-training](http://arxiv.org/pdf/1811.10104v1)\n\n**code**:-\n\n**Author**:Kaiming He、 Ross Girshick 、Piotr Dollar 、Facebook AI Research (FAIR) \n\n**time**:2018\n\n## Abstract\n\nWe report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained **from random initialization**.The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine -tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge.Training from random initialization is surprisingly robust; our results hold even when: (i) Using only 10% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics.Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy.To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data--a result on par with the top COCO 2017 competition results that used ImageNet pre-training.These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of ' pre-training and fine -tuning' in computer vision.\n\n**翻译**：我们使用随机初始化训练的标准模型报告在COCO数据集上的物体检测和物体分割的竞争结果。即使使用baseline model的超参数，结果也不比它们的ImageNet预训练对应物差（Mask R-CNN）针对精细调整预训练模型进行了优化，唯一的例外是增加训练迭代次数，因此随机初始化模型可以收敛。随机初始化训练非常强大;我们的结果即使在以下情况下仍然存在：（i）仅使用10％的训练数据，（ii）用于更深和更宽的模型，以及（iii）用于多个任务和指标。实验表明在ImageNet预训练速度能加速训练时候的收敛速度，但不一定提供正则化或提高最终在目标任务上的准确性。为了突破这个范围，我们在不使用任何外部数据的情况下在COCO对象检测上达到了50.9 AP  - 这个结果与2017年COCO相同使用ImageNet预训练的最好的竞赛结果差不多。这些结论挑战了ImageNet与训练对于依赖性任务的传统智慧，我们期望这些发现将鼓励人们重新思考计算机视觉中的“pre-training and fine-tuneing”范式。\n\n![543926824(1](/images/154392682.jpg)\n\n## Contribution\n\n- 采用基于ImageNet的预训练模型参数可以加快模型的收敛速度，尤其是在训练初期，而基于随机初始化网络结构参数训练模型时在采用合适的归一化方式且迭代足够多的次数后也能够达到相同的效果\n\n![img](/images/154392682.jpg)\n\n- 采用基于ImageNet的预训练模型参数训练得到的模型泛化能力不如基于随机初始化网络结构训练得到的模型，前者更容易出现过拟合，因此需要选择合适的超参数训练模型，从而尽可能减少过拟合风险（前提是数据量足够大，比如10K以上的COCO数据集）\n\n  ![Q图片2018120420442](/images/QQ%E5%9B%BE%E7%89%8720181204204421.png)\n\n- 当迁移任务的目标对空间位置信息比较敏感时（比如目标检测、人体关键点检测任务），采用基于ImageNet的预训练模型并不会有太大的帮助\n\n## Summary\n\n\n\n# Deep Neural Decision  Forests\n\n**paper**:[Deep neural decision forests](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf)\n\n**code**:\n\n**Author**:\n\n**time:**ICCV2015\n\n## Abstract\n\nWe present Deep Neural Decision Forests – a novel approach that unifies classification trees with the representation learning  known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find onpar or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84%/6.38% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67% error obtained by the best GoogLeNet architecture (7 models, 144 crops). \n\n**翻译**:我们提出了深度神经决策森林--一个新的方法可以通过端到端的训练将分类树和深度卷积卷积网络中的表征学习功能统一起来。为了把这两个方法结合起来，我们引入了一个随机可微分的决策树模型，控制一般在（深度）卷积网络的初始层进行的表征学习。我们的模型不同于传统的深度网络，因为决策森林进行最后的预测，它与传统的决策森林不同，因为我们提出了分裂节点和叶子节点参数的联合全局优化原则。我们展示了在类似MNIST和ImageNet的benchmark上的实验结果，并与最先进的深度模型进行比较，发现了优于它们的结果。最值得注意的是，当我们在ImageNet验证数据上获得的Top5误差仅为7.84%/6.38%。因此即使没有任何形式的训练数据增强，我们也在最佳的GoogleNet架构(7种模型，144 crops）上得到6.67%的误差。\n\n![Q图片2018120420311](/images/QQ%E5%9B%BE%E7%89%8720181204203113.png)\n\n## Contribution\n\n## Summary\n\n","slug":"每日阅读","published":1,"updated":"2018-12-05T09:04:05.513Z","_id":"cjpaxs0n6000ws0vwqvsg038r","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Rethinking-ImageNet-Pre-training\"><a href=\"#Rethinking-ImageNet-Pre-training\" class=\"headerlink\" title=\"Rethinking ImageNet Pre-training\"></a>Rethinking ImageNet Pre-training</h1><p><strong>Paper</strong>:<a href=\"http://arxiv.org/pdf/1811.10104v1\" target=\"_blank\" rel=\"external\">Rethinking ImageNet Pre-training</a></p>\n<p><strong>code</strong>:-</p>\n<p><strong>Author</strong>:Kaiming He、 Ross Girshick 、Piotr Dollar 、Facebook AI Research (FAIR) </p>\n<p><strong>time</strong>:2018</p>\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained <strong>from random initialization</strong>.The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine -tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge.Training from random initialization is surprisingly robust; our results hold even when: (i) Using only 10% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics.Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy.To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data–a result on par with the top COCO 2017 competition results that used ImageNet pre-training.These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of ‘ pre-training and fine -tuning’ in computer vision.</p>\n<p><strong>翻译</strong>：我们使用随机初始化训练的标准模型报告在COCO数据集上的物体检测和物体分割的竞争结果。即使使用baseline model的超参数，结果也不比它们的ImageNet预训练对应物差（Mask R-CNN）针对精细调整预训练模型进行了优化，唯一的例外是增加训练迭代次数，因此随机初始化模型可以收敛。随机初始化训练非常强大;我们的结果即使在以下情况下仍然存在：（i）仅使用10％的训练数据，（ii）用于更深和更宽的模型，以及（iii）用于多个任务和指标。实验表明在ImageNet预训练速度能加速训练时候的收敛速度，但不一定提供正则化或提高最终在目标任务上的准确性。为了突破这个范围，我们在不使用任何外部数据的情况下在COCO对象检测上达到了50.9 AP  - 这个结果与2017年COCO相同使用ImageNet预训练的最好的竞赛结果差不多。这些结论挑战了ImageNet与训练对于依赖性任务的传统智慧，我们期望这些发现将鼓励人们重新思考计算机视觉中的“pre-training and fine-tuneing”范式。</p>\n<p><img src=\"/images/154392682.jpg\" alt=\"543926824(1\"></p>\n<h2 id=\"Contribution\"><a href=\"#Contribution\" class=\"headerlink\" title=\"Contribution\"></a>Contribution</h2><ul>\n<li>采用基于ImageNet的预训练模型参数可以加快模型的收敛速度，尤其是在训练初期，而基于随机初始化网络结构参数训练模型时在采用合适的归一化方式且迭代足够多的次数后也能够达到相同的效果</li>\n</ul>\n<p><img src=\"/images/154392682.jpg\" alt=\"img\"></p>\n<ul>\n<li><p>采用基于ImageNet的预训练模型参数训练得到的模型泛化能力不如基于随机初始化网络结构训练得到的模型，前者更容易出现过拟合，因此需要选择合适的超参数训练模型，从而尽可能减少过拟合风险（前提是数据量足够大，比如10K以上的COCO数据集）</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181204204421.png\" alt=\"Q图片2018120420442\"></p>\n</li>\n<li><p>当迁移任务的目标对空间位置信息比较敏感时（比如目标检测、人体关键点检测任务），采用基于ImageNet的预训练模型并不会有太大的帮助</p>\n</li>\n</ul>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><h1 id=\"Deep-Neural-Decision-Forests\"><a href=\"#Deep-Neural-Decision-Forests\" class=\"headerlink\" title=\"Deep Neural Decision  Forests\"></a>Deep Neural Decision  Forests</h1><p><strong>paper</strong>:<a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf\" target=\"_blank\" rel=\"external\">Deep neural decision forests</a></p>\n<p><strong>code</strong>:</p>\n<p><strong>Author</strong>:</p>\n<p><strong>time:</strong>ICCV2015</p>\n<h2 id=\"Abstract-1\"><a href=\"#Abstract-1\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>We present Deep Neural Decision Forests – a novel approach that unifies classification trees with the representation learning  known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find onpar or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84%/6.38% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67% error obtained by the best GoogLeNet architecture (7 models, 144 crops). </p>\n<p><strong>翻译</strong>:我们提出了深度神经决策森林–一个新的方法可以通过端到端的训练将分类树和深度卷积卷积网络中的表征学习功能统一起来。为了把这两个方法结合起来，我们引入了一个随机可微分的决策树模型，控制一般在（深度）卷积网络的初始层进行的表征学习。我们的模型不同于传统的深度网络，因为决策森林进行最后的预测，它与传统的决策森林不同，因为我们提出了分裂节点和叶子节点参数的联合全局优化原则。我们展示了在类似MNIST和ImageNet的benchmark上的实验结果，并与最先进的深度模型进行比较，发现了优于它们的结果。最值得注意的是，当我们在ImageNet验证数据上获得的Top5误差仅为7.84%/6.38%。因此即使没有任何形式的训练数据增强，我们也在最佳的GoogleNet架构(7种模型，144 crops）上得到6.67%的误差。</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181204203113.png\" alt=\"Q图片2018120420311\"></p>\n<h2 id=\"Contribution-1\"><a href=\"#Contribution-1\" class=\"headerlink\" title=\"Contribution\"></a>Contribution</h2><h2 id=\"Summary-1\"><a href=\"#Summary-1\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Rethinking-ImageNet-Pre-training\"><a href=\"#Rethinking-ImageNet-Pre-training\" class=\"headerlink\" title=\"Rethinking ImageNet Pre-training\"></a>Rethinking ImageNet Pre-training</h1><p><strong>Paper</strong>:<a href=\"http://arxiv.org/pdf/1811.10104v1\" target=\"_blank\" rel=\"external\">Rethinking ImageNet Pre-training</a></p>\n<p><strong>code</strong>:-</p>\n<p><strong>Author</strong>:Kaiming He、 Ross Girshick 、Piotr Dollar 、Facebook AI Research (FAIR) </p>\n<p><strong>time</strong>:2018</p>\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>We report competitive results on object detection and instance segmentation on the COCO dataset using standard models trained <strong>from random initialization</strong>.The results are no worse than their ImageNet pre-training counterparts even when using the hyper-parameters of the baseline system (Mask R-CNN) that were optimized for fine -tuning pre-trained models, with the sole exception of increasing the number of training iterations so the randomly initialized models may converge.Training from random initialization is surprisingly robust; our results hold even when: (i) Using only 10% of the training data, (ii) for deeper and wider models, and (iii) for multiple tasks and metrics.Experiments show that ImageNet pre-training speeds up convergence early in training, but does not necessarily provide regularization or improve final target task accuracy.To push the envelope we demonstrate 50.9 AP on COCO object detection without using any external data–a result on par with the top COCO 2017 competition results that used ImageNet pre-training.These observations challenge the conventional wisdom of ImageNet pre-training for dependent tasks and we expect these discoveries will encourage people to rethink the current de facto paradigm of ‘ pre-training and fine -tuning’ in computer vision.</p>\n<p><strong>翻译</strong>：我们使用随机初始化训练的标准模型报告在COCO数据集上的物体检测和物体分割的竞争结果。即使使用baseline model的超参数，结果也不比它们的ImageNet预训练对应物差（Mask R-CNN）针对精细调整预训练模型进行了优化，唯一的例外是增加训练迭代次数，因此随机初始化模型可以收敛。随机初始化训练非常强大;我们的结果即使在以下情况下仍然存在：（i）仅使用10％的训练数据，（ii）用于更深和更宽的模型，以及（iii）用于多个任务和指标。实验表明在ImageNet预训练速度能加速训练时候的收敛速度，但不一定提供正则化或提高最终在目标任务上的准确性。为了突破这个范围，我们在不使用任何外部数据的情况下在COCO对象检测上达到了50.9 AP  - 这个结果与2017年COCO相同使用ImageNet预训练的最好的竞赛结果差不多。这些结论挑战了ImageNet与训练对于依赖性任务的传统智慧，我们期望这些发现将鼓励人们重新思考计算机视觉中的“pre-training and fine-tuneing”范式。</p>\n<p><img src=\"/images/154392682.jpg\" alt=\"543926824(1\"></p>\n<h2 id=\"Contribution\"><a href=\"#Contribution\" class=\"headerlink\" title=\"Contribution\"></a>Contribution</h2><ul>\n<li>采用基于ImageNet的预训练模型参数可以加快模型的收敛速度，尤其是在训练初期，而基于随机初始化网络结构参数训练模型时在采用合适的归一化方式且迭代足够多的次数后也能够达到相同的效果</li>\n</ul>\n<p><img src=\"/images/154392682.jpg\" alt=\"img\"></p>\n<ul>\n<li><p>采用基于ImageNet的预训练模型参数训练得到的模型泛化能力不如基于随机初始化网络结构训练得到的模型，前者更容易出现过拟合，因此需要选择合适的超参数训练模型，从而尽可能减少过拟合风险（前提是数据量足够大，比如10K以上的COCO数据集）</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181204204421.png\" alt=\"Q图片2018120420442\"></p>\n</li>\n<li><p>当迁移任务的目标对空间位置信息比较敏感时（比如目标检测、人体关键点检测任务），采用基于ImageNet的预训练模型并不会有太大的帮助</p>\n</li>\n</ul>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><h1 id=\"Deep-Neural-Decision-Forests\"><a href=\"#Deep-Neural-Decision-Forests\" class=\"headerlink\" title=\"Deep Neural Decision  Forests\"></a>Deep Neural Decision  Forests</h1><p><strong>paper</strong>:<a href=\"https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf\" target=\"_blank\" rel=\"external\">Deep neural decision forests</a></p>\n<p><strong>code</strong>:</p>\n<p><strong>Author</strong>:</p>\n<p><strong>time:</strong>ICCV2015</p>\n<h2 id=\"Abstract-1\"><a href=\"#Abstract-1\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>We present Deep Neural Decision Forests – a novel approach that unifies classification trees with the representation learning  known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find onpar or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84%/6.38% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67% error obtained by the best GoogLeNet architecture (7 models, 144 crops). </p>\n<p><strong>翻译</strong>:我们提出了深度神经决策森林–一个新的方法可以通过端到端的训练将分类树和深度卷积卷积网络中的表征学习功能统一起来。为了把这两个方法结合起来，我们引入了一个随机可微分的决策树模型，控制一般在（深度）卷积网络的初始层进行的表征学习。我们的模型不同于传统的深度网络，因为决策森林进行最后的预测，它与传统的决策森林不同，因为我们提出了分裂节点和叶子节点参数的联合全局优化原则。我们展示了在类似MNIST和ImageNet的benchmark上的实验结果，并与最先进的深度模型进行比较，发现了优于它们的结果。最值得注意的是，当我们在ImageNet验证数据上获得的Top5误差仅为7.84%/6.38%。因此即使没有任何形式的训练数据增强，我们也在最佳的GoogleNet架构(7种模型，144 crops）上得到6.67%的误差。</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181204203113.png\" alt=\"Q图片2018120420311\"></p>\n<h2 id=\"Contribution-1\"><a href=\"#Contribution-1\" class=\"headerlink\" title=\"Contribution\"></a>Contribution</h2><h2 id=\"Summary-1\"><a href=\"#Summary-1\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2>"},{"title":"年龄估计","date":"2018-12-05T08:48:48.000Z","description":"年龄估计的系列论文阅读","mathjax":null,"_content":"\n# Dataset\n\n- **FG-Net**:1002 images,0到69岁，82个人\n- **MORPH1**:1690 images\n- **MORPH2**:55608 images、unbalanced ethnic(96% African and European ,less than 1% Asian)，16到77岁\n- **AFAD**:160K Asian facial images(unrelease)\n\n# Ordinal Regression\n\n**paper：**[Ordinal Regression With Multiple Output CNN for Age Estimation](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Niu_Ordinal_Regression_With_CVPR_2016_paper.pdf)`CVPR2016`\n\n**code:**[Ordinal Regression](https://github.com/luoyetx/OrdinalRegression)\n\n## Abstract\n\nTo address the non-stationary property of aging patterns, age estimation can be cast as an ordinal regression problem.However, the processes of extracting features and learning a regression model are often separated and optimized independently in previous work. In this paper, we propose an End-to-End learning approach to address ordinal regression problems using deep Convolutional Neural Network, which could simultaneously conduct feature learning andregression modeling. In particular, an ordinal regression problem is transformed into a series of binary classification sub-problems. And we propose a multiple output CNN learning algorithm to collectively solve these classification sub-problems, so that the correlation between these tasks could be explored. In addition, we publish an Asian Face Age Dataset (AFAD) containing more than 160K facial images with precise age ground-truths, which is the largest public age dataset to date. To the best of our knowledge, this is the first work to address ordinal regression problems by using CNN, and achieves the state-of-the-art performance on both the MORPH and AFAD datasets \n\n**翻译**：为了解决衰老模式的非平稳特性，可以将年龄估计作为序数回归问题。然而，在以前的工作中，提取特征和学习回归模型的过程通常是独立分离和优化的。在本文中，我们提出了一种端到端学习方法，使用深度卷积神经网络解决序数回归问题，可以同时进行特征学习和回归建模。特别是，序数回归问题转化为一系列二元分类子问题。我们提出了一种多输出CNN学习算法来集体解决这些分类子问题，从而可以探索这些任务之间的相关性。此外，我们发布了一个亚洲面部年龄数据集（AFAD），其中包含超过**160K**的面部图像，具有精确的年龄真实性，这是迄今为止最大的公共年龄数据集。据我们所知，这是通过使用CNN解决序数回归问题的第一项工作，并在MORPH和AFAD数据集上实现了最先进的性能。\n\n![Q图片2018120212082](/images/QQ%E5%9B%BE%E7%89%8720181202120825.png)\n\n\n\n## Contributions\n\n- 利用端到端的深度学习方法解决序数回归问题\n- released a dataset named AFAD , the largest public dataset to date\n\n## Summary\n\nRank的思想比较好，针对有序的问题，将序数回归转化为一系列的二分类问题，但是这篇论文并不是第一个使用ranking思想的论文，和以前的方法（提取手工特征或者深度学习特征，然后用SVM/SVR的分类器作为二分类器）不同的是，将ranking和CNN结合起来，可以端到端的训练。\n\n# RankingCNN\n\n**paper**：[Using Ranking-CNN for Age Estimation](http://openaccess.thecvf.com/content_cvpr_2017/poster/2148_POSTER.pdf)`CVPR2017`\n\n**code:** [Using-Ranking-CNN-for-Age-Estimation](https://github.com/RankingCNN/Using-Ranking-CNN-for-Age-Estimation)\n\n## Abstract\n\nHuman age is considered an important biometric trait for human identification or search. Recent research shows that the aging features deeply learned from large-scale data lead to significant performance improvement on facial image-based age estimation. However, age-related ordinal information is totally ignored in these approaches. In this paper, we propose a novel Convolutional Neural Network (CNN)-based framework, ranking-CNN, for age estimation. Ranking-CNN contains a series of basic CNNs, each of\nwhich is trained with ordinal age labels. Then, their binary outputs are aggregated for the final age prediction. We theoretically obtain a much tighter error bound for rankingbased age estimation. Moreover, we rigorously prove that ranking-CNN is more likely to get smaller estimation errors when compared with multi-class classification approaches. Through extensive experiments, we show that statistically, ranking-CNN significantly outperforms other state-of-theart age estimation models on benchmark datasets .\n\n**翻译**:\n\n人类年龄被认为是一个重要的生物特征，对人类的识别或搜索。最近的研究表明，从大规模数据中深入学习到的年龄特征使得基于人脸图像的年龄估计的性能有了显着的提高。然而，在这些方法中，与年龄相关的顺序信息被完全忽略。本文提出了一种新的基于卷积神经网络(CNN)的年龄估计框架-Ranking-CNN。Ranjing-CNN包含一系列基本CNN，每个CNN都是用顺序年龄标签训练的。然后，将它们的二进制输出进行聚合，以进行最终的年龄预测。我们从理论上得到了一个更严格的基于秩的年龄估计的误差界。此外，我们严格地证明了，与多类分类方法相比，Ranking-CNN更容易获得较小的估计误差。通过大量的实验，我们表明，在统计上，Ranking-CNN在基准数据集上的性能明显优于其他最先进的年龄估计模型。\n\n![Q图片2018120212115](/images/QQ%E5%9B%BE%E7%89%8720181202121159.png)![Q图片2018120212122](/images/QQ%E5%9B%BE%E7%89%8720181202121223.png)\n\n## Contributions\n\n- To the best of our knowledge, ranking-CNN is the first work that uses a deep ranking model for age estimation, in which binary ordinal age labels are used to train a series of basic CNNs, one for each age group.Each basic CNN in ranking-CNN can be trained using all the labeled data, leading to better performance of feature learning and also preventing overfitting.\n- We provide a much tighter error bound for age ranking than that introduced in [2], which claimed that the final ranking error is bounded by the sum of errors generated by all the classifiers. We obtain the approximation for the final ranking error that is controlled by the maximum error produced among sub-problems. From a technical perspective, the tighter error bound provides several advantages for the training of ranking-CNN.\n- We prove that ranking-CNN, by taking the ordinal relation between ages into consideration, is more likely to get smaller estimation errors when compared with multi-class classification approaches (i.e., CNNs using the softmax function). Moreover, through extensive experiments, we show that statistically, ranking-CNN significantly outperforms other state-of-the-art age estimation methods \n\n## Summary\n\n这篇论文和上篇oridnal regression的思想类似，但是效果要好很多，不同的是每个二分类器都有单独的模型，所有二分类器都不共享中间层，然后聚合所有二分类器的结果得到最终的年龄估计，另外还会得到最终排序误差的近似值，该误差由子问题中产生的最大误差控制，比上一篇论文的方法误差控制更严格。\n\n# SSR-Net\n\n**paper**：[SSR-Net: A Compact Soft Stagewise Regression Network for Age Estimation](https://www.csie.ntu.edu.tw/~cyy/publications/papers/Yang2018SSR.pdf)`IJCAI2018`\n\n**code:** [SSRNet](https://github.com/shamangary/SSR-Net)\n\n## Abstract\n\nThis paper presents a novel CNN model called Soft Stagewise Regression Network (SSR-Net) for age estimation from a single image with a compact model size. Inspired by DEX, we address age estimation by performing multi-class classification andthen turning classification results into regression by calculating the expected values. SSR-Net takes a coarse -to-fine strategy and performs multi-class classification with multiple stages.Each stage is only responsible for refining the decision of its previous stage for more accurate age estimation. Thus, each stage performs a task with few classes and requires few neurons, greatly reducing the model size. For addressing the quantization issue introduced by grouping ages into classes, SSR-Net assigns a dynamic range to each age class by allowing it to be shifted and scaled according to the input face image. Both the multi-stage strategy and the dynamic range are incorporated into the formulation of soft stagewise regression. A novel network architecture is proposed for carrying out soft stagewise regression. The resultant SSR-Net model is very compact and takes only 0.32 MB. Despite its compact size, SSR-Net' s performance approaches those of the state -of-the -art methods whose model sizes are often more than 1500x larger.\n\n**翻译**：本文提出了一种新的CNN模型，称为Soft stagewise regression net(SSR-Net)，使用一个很小的compact模型对单个图像进行年龄估计。受DEX的启发，我们通过执行多类分类来解决年龄估计，然后通过计算预期值将分类结果转化为回归。 SSR-Net采用粗略到精细的策略并执行多阶段的多级分类。每个阶段仅负责完善其前一阶段的决策以进行更准确的年龄估计。因此，每个阶段执行具有少量类的任务并且需要很少的神经元，从而大大减小了模型尺寸。为了解决通过将年龄分组到类中引入的量化问题，SSR-Net通过允许根据输入面部图像移位和缩放来为每个年龄类别分配动态范围。多阶段策略和动态范围都包含在软阶段回归的公式中。提出了一种新颖的网络架构，用于进行软分段回归。由此产生的SSR-Net模型非常紧凑，仅需0.32MB。尽管尺寸紧凑，但SSR-Net的性能接近于那些模型尺寸通常超过1500倍的方法。\n\n\n\n![Q图片2018120212095](/images/QQ%E5%9B%BE%E7%89%8720181202120956.png)\n\n## Contributions\n\n新的年龄估计模型，平均误差绝对值为3.16，与最好的模型（当时）差距0.5岁，但参数不到其1/1000，整个模型参数仅0.3M，非常适合用于嵌入式系统。\n\n## Summary\n\n采用多分段的多级分类的思想，大大减少了模型的参数，在准确性和模型大小方面做了一个均衡，另外，这么小的模型做到这种准确率很厉害。\n\n# DeepRegressionForest\n\n**paper:**[Deep Regression Forests for Age Estimation](https://arxiv.org/abs/1712.07195)`CVPR2018`\n\n**code**:[caffe-DeepRegressionForests](https://github.com/shenwei1231/caffe-DeepRegressionForests)\n\n## Abstract\n\n摘要\n\nAge estimation from facial images is typically cast as a nonlinear regression problem. The main challenge of this problem is the facial feature space w.r.t. ages is heterogeneous, due to the large variation in facial appearance across different persons of the same age and the nonstationary property of aging patterns. In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation. DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with heterogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes. This joint learning follows an alternating strategy: First, by ﬁxing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by ﬁxing the split nodes, the leaf nodes are optimized by iterating a step-size free and fastconverging update rule derived from Variational Bounding. We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them.\n\n**翻译**：\n\n来自面部图像的年龄估计通常被视为非线性回归问题。这个问题的主要挑战是面部特征空间w.r.t.年龄是不均匀的，这是由于同一年龄段不同人的面部外观差异很大，以及老龄化模式的非平稳性。在本文中，我们提出了深度回归森林（DRFs），一种端到端模型，用于年龄估计。DRF将分裂节点连接到卷积神经网络（CNN）的完全连接层，并通过联合学习分裂节点处的输入相关数据分区和叶节点处的数据抽象来处理异构数据。这种联合学习遵循交替策略：首先，通过固定叶节点，分裂节点以及CNN参数通过反向传播进行优化;然后，通过修剪拆分节点，通过迭代从变分边界派生的步长自由和快速收敛更新规则来优化叶节点。我们在三个标准年龄估算基准上验证了提议的DRF，并在所有这些基准上取得了最新的成果。\n\n![83KC{3A7Y}PVX~YU`383R](/images/Q83KC%7B3A7Y%7DPVX~YU%60383RN.png)\n\n## Contributions\n\n- We propose Deep Regression Forests (DRFs), an endto-end model, to deal with heterogeneous data by jointly learning input-dependant data partition at split nodes and data abstraction at leaf nodes .\n- Based on Variational Bounding, the convergence of our update rule for leaf nodes in DRFs is mathematically guaranteed.\n- We apply DRFs on three standard age estimation\n  benchmarks, and achieve state-of-the-art results. \n\n## Summary\n\n用深度回归森林的思想来进行年龄估计的任务，属于一种distribution learning的办法，论文中涉及到大量的概率论的知识和大量的推导公式，将年龄分布分布到多个叶子节点，最终采用随机森林和决策树的思想，对模型训练结果做融合，模型很好训练，且准确率也不错。\n\n# Mean-Variance Loss\n\n**paper**:[Mean-Variance Loss for Deep Age Estimation From a Face](http://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Mean-Variance_Loss_for_CVPR_2018_paper.pdf)`CVPR2018`\n\n**code:**-\n\n## Abstract\n\nAge estimation has wide applications in video surveillance, social networking, and human-computer interaction. Many of the published approaches simply treat age estimation as an exact age regression problem, and thus do not leverage a distribution’s robustness in representing labels with ambiguity such as ages. In this paper, we propose a new loss function, called mean-variance loss, for robust age estimation via distribution learning. Speciﬁcally, the mean-variance loss consists of a mean loss, which penalizes difference between the mean of the estimated age distribution and the ground-truth age, and a variance loss, which penalizes the variance of the estimated age distribution to ensure a concentrated distribution. The proposed meanvariance loss and softmax loss are jointly embedded into Convolutional Neural Networks (CNNs) for age estimation. Experimental results on the FG-NET, MORPH Album II, CLAP2016, and AADB databases show that the proposed approach outperforms the state-of-the-art age estimation methods by a large margin, and generalizes well to image aesthetics assessment.1\n\n**翻译**：\n\n年龄估计在视频监控，社交网络和人机交互中具有广泛的应用。许多已发表的方法仅将年龄估计视为精确的年龄回归问题，因此不会利用分布的稳健性来表示具有模糊性的标签，例如年龄。在本文中，我们提出了一种新的损失函数，称为均值 - 方差损失，用于通过分布学习进行稳健的年龄估计。具体而言，均值 - 方差损失包括平均损失，其惩罚估计年龄分布的平均值与地面实际年龄之间的差异，以及方差损失，其惩罚估计年龄分布的方差以确保集中分布。 。所提出的均值方差损失和softmax损失共同嵌入到卷积神经网络（CNN）中用于年龄估计。在FG-NET，MORPH Album II，CLAP2016和AADB数据库上的实验结果表明，所提出的方法大大超过了现有技术的年龄估计方法，并且很好地概括了图像美学评估。\n\n![Q图片2018120213002](/images/QQ%E5%9B%BE%E7%89%8720181202130028.png)\n\n\n\n## Contributions\n\n1. propose a new loss:**mean-variance loss**,aiming at the estimate of an age distribution with its mean as close to the groud-truth age as possible , and its variance as small as possible\n2. diffferent from the age disturibution learning methods,the proposed approach **doesn't require that each training image must have a mean age and a variance** (neither real nor assumed) labels during model training, but it can still give a distribution estimate for a face image\n3. the proposed loss can be **easily embedded into different CNNs**, and the network can be optimized via SGD **end-to-end**\n\n## Summary\n\n思想简单且新颖，有用且有效。","source":"_posts/年龄估计.md","raw":"---\ntitle: 年龄估计\ndate: 2018-12-05 16:48:48\ncategories: 论文笔记\ntags: 年龄估计\ndescription: 年龄估计的系列论文阅读\nmathjax:\n---\n\n# Dataset\n\n- **FG-Net**:1002 images,0到69岁，82个人\n- **MORPH1**:1690 images\n- **MORPH2**:55608 images、unbalanced ethnic(96% African and European ,less than 1% Asian)，16到77岁\n- **AFAD**:160K Asian facial images(unrelease)\n\n# Ordinal Regression\n\n**paper：**[Ordinal Regression With Multiple Output CNN for Age Estimation](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Niu_Ordinal_Regression_With_CVPR_2016_paper.pdf)`CVPR2016`\n\n**code:**[Ordinal Regression](https://github.com/luoyetx/OrdinalRegression)\n\n## Abstract\n\nTo address the non-stationary property of aging patterns, age estimation can be cast as an ordinal regression problem.However, the processes of extracting features and learning a regression model are often separated and optimized independently in previous work. In this paper, we propose an End-to-End learning approach to address ordinal regression problems using deep Convolutional Neural Network, which could simultaneously conduct feature learning andregression modeling. In particular, an ordinal regression problem is transformed into a series of binary classification sub-problems. And we propose a multiple output CNN learning algorithm to collectively solve these classification sub-problems, so that the correlation between these tasks could be explored. In addition, we publish an Asian Face Age Dataset (AFAD) containing more than 160K facial images with precise age ground-truths, which is the largest public age dataset to date. To the best of our knowledge, this is the first work to address ordinal regression problems by using CNN, and achieves the state-of-the-art performance on both the MORPH and AFAD datasets \n\n**翻译**：为了解决衰老模式的非平稳特性，可以将年龄估计作为序数回归问题。然而，在以前的工作中，提取特征和学习回归模型的过程通常是独立分离和优化的。在本文中，我们提出了一种端到端学习方法，使用深度卷积神经网络解决序数回归问题，可以同时进行特征学习和回归建模。特别是，序数回归问题转化为一系列二元分类子问题。我们提出了一种多输出CNN学习算法来集体解决这些分类子问题，从而可以探索这些任务之间的相关性。此外，我们发布了一个亚洲面部年龄数据集（AFAD），其中包含超过**160K**的面部图像，具有精确的年龄真实性，这是迄今为止最大的公共年龄数据集。据我们所知，这是通过使用CNN解决序数回归问题的第一项工作，并在MORPH和AFAD数据集上实现了最先进的性能。\n\n![Q图片2018120212082](/images/QQ%E5%9B%BE%E7%89%8720181202120825.png)\n\n\n\n## Contributions\n\n- 利用端到端的深度学习方法解决序数回归问题\n- released a dataset named AFAD , the largest public dataset to date\n\n## Summary\n\nRank的思想比较好，针对有序的问题，将序数回归转化为一系列的二分类问题，但是这篇论文并不是第一个使用ranking思想的论文，和以前的方法（提取手工特征或者深度学习特征，然后用SVM/SVR的分类器作为二分类器）不同的是，将ranking和CNN结合起来，可以端到端的训练。\n\n# RankingCNN\n\n**paper**：[Using Ranking-CNN for Age Estimation](http://openaccess.thecvf.com/content_cvpr_2017/poster/2148_POSTER.pdf)`CVPR2017`\n\n**code:** [Using-Ranking-CNN-for-Age-Estimation](https://github.com/RankingCNN/Using-Ranking-CNN-for-Age-Estimation)\n\n## Abstract\n\nHuman age is considered an important biometric trait for human identification or search. Recent research shows that the aging features deeply learned from large-scale data lead to significant performance improvement on facial image-based age estimation. However, age-related ordinal information is totally ignored in these approaches. In this paper, we propose a novel Convolutional Neural Network (CNN)-based framework, ranking-CNN, for age estimation. Ranking-CNN contains a series of basic CNNs, each of\nwhich is trained with ordinal age labels. Then, their binary outputs are aggregated for the final age prediction. We theoretically obtain a much tighter error bound for rankingbased age estimation. Moreover, we rigorously prove that ranking-CNN is more likely to get smaller estimation errors when compared with multi-class classification approaches. Through extensive experiments, we show that statistically, ranking-CNN significantly outperforms other state-of-theart age estimation models on benchmark datasets .\n\n**翻译**:\n\n人类年龄被认为是一个重要的生物特征，对人类的识别或搜索。最近的研究表明，从大规模数据中深入学习到的年龄特征使得基于人脸图像的年龄估计的性能有了显着的提高。然而，在这些方法中，与年龄相关的顺序信息被完全忽略。本文提出了一种新的基于卷积神经网络(CNN)的年龄估计框架-Ranking-CNN。Ranjing-CNN包含一系列基本CNN，每个CNN都是用顺序年龄标签训练的。然后，将它们的二进制输出进行聚合，以进行最终的年龄预测。我们从理论上得到了一个更严格的基于秩的年龄估计的误差界。此外，我们严格地证明了，与多类分类方法相比，Ranking-CNN更容易获得较小的估计误差。通过大量的实验，我们表明，在统计上，Ranking-CNN在基准数据集上的性能明显优于其他最先进的年龄估计模型。\n\n![Q图片2018120212115](/images/QQ%E5%9B%BE%E7%89%8720181202121159.png)![Q图片2018120212122](/images/QQ%E5%9B%BE%E7%89%8720181202121223.png)\n\n## Contributions\n\n- To the best of our knowledge, ranking-CNN is the first work that uses a deep ranking model for age estimation, in which binary ordinal age labels are used to train a series of basic CNNs, one for each age group.Each basic CNN in ranking-CNN can be trained using all the labeled data, leading to better performance of feature learning and also preventing overfitting.\n- We provide a much tighter error bound for age ranking than that introduced in [2], which claimed that the final ranking error is bounded by the sum of errors generated by all the classifiers. We obtain the approximation for the final ranking error that is controlled by the maximum error produced among sub-problems. From a technical perspective, the tighter error bound provides several advantages for the training of ranking-CNN.\n- We prove that ranking-CNN, by taking the ordinal relation between ages into consideration, is more likely to get smaller estimation errors when compared with multi-class classification approaches (i.e., CNNs using the softmax function). Moreover, through extensive experiments, we show that statistically, ranking-CNN significantly outperforms other state-of-the-art age estimation methods \n\n## Summary\n\n这篇论文和上篇oridnal regression的思想类似，但是效果要好很多，不同的是每个二分类器都有单独的模型，所有二分类器都不共享中间层，然后聚合所有二分类器的结果得到最终的年龄估计，另外还会得到最终排序误差的近似值，该误差由子问题中产生的最大误差控制，比上一篇论文的方法误差控制更严格。\n\n# SSR-Net\n\n**paper**：[SSR-Net: A Compact Soft Stagewise Regression Network for Age Estimation](https://www.csie.ntu.edu.tw/~cyy/publications/papers/Yang2018SSR.pdf)`IJCAI2018`\n\n**code:** [SSRNet](https://github.com/shamangary/SSR-Net)\n\n## Abstract\n\nThis paper presents a novel CNN model called Soft Stagewise Regression Network (SSR-Net) for age estimation from a single image with a compact model size. Inspired by DEX, we address age estimation by performing multi-class classification andthen turning classification results into regression by calculating the expected values. SSR-Net takes a coarse -to-fine strategy and performs multi-class classification with multiple stages.Each stage is only responsible for refining the decision of its previous stage for more accurate age estimation. Thus, each stage performs a task with few classes and requires few neurons, greatly reducing the model size. For addressing the quantization issue introduced by grouping ages into classes, SSR-Net assigns a dynamic range to each age class by allowing it to be shifted and scaled according to the input face image. Both the multi-stage strategy and the dynamic range are incorporated into the formulation of soft stagewise regression. A novel network architecture is proposed for carrying out soft stagewise regression. The resultant SSR-Net model is very compact and takes only 0.32 MB. Despite its compact size, SSR-Net' s performance approaches those of the state -of-the -art methods whose model sizes are often more than 1500x larger.\n\n**翻译**：本文提出了一种新的CNN模型，称为Soft stagewise regression net(SSR-Net)，使用一个很小的compact模型对单个图像进行年龄估计。受DEX的启发，我们通过执行多类分类来解决年龄估计，然后通过计算预期值将分类结果转化为回归。 SSR-Net采用粗略到精细的策略并执行多阶段的多级分类。每个阶段仅负责完善其前一阶段的决策以进行更准确的年龄估计。因此，每个阶段执行具有少量类的任务并且需要很少的神经元，从而大大减小了模型尺寸。为了解决通过将年龄分组到类中引入的量化问题，SSR-Net通过允许根据输入面部图像移位和缩放来为每个年龄类别分配动态范围。多阶段策略和动态范围都包含在软阶段回归的公式中。提出了一种新颖的网络架构，用于进行软分段回归。由此产生的SSR-Net模型非常紧凑，仅需0.32MB。尽管尺寸紧凑，但SSR-Net的性能接近于那些模型尺寸通常超过1500倍的方法。\n\n\n\n![Q图片2018120212095](/images/QQ%E5%9B%BE%E7%89%8720181202120956.png)\n\n## Contributions\n\n新的年龄估计模型，平均误差绝对值为3.16，与最好的模型（当时）差距0.5岁，但参数不到其1/1000，整个模型参数仅0.3M，非常适合用于嵌入式系统。\n\n## Summary\n\n采用多分段的多级分类的思想，大大减少了模型的参数，在准确性和模型大小方面做了一个均衡，另外，这么小的模型做到这种准确率很厉害。\n\n# DeepRegressionForest\n\n**paper:**[Deep Regression Forests for Age Estimation](https://arxiv.org/abs/1712.07195)`CVPR2018`\n\n**code**:[caffe-DeepRegressionForests](https://github.com/shenwei1231/caffe-DeepRegressionForests)\n\n## Abstract\n\n摘要\n\nAge estimation from facial images is typically cast as a nonlinear regression problem. The main challenge of this problem is the facial feature space w.r.t. ages is heterogeneous, due to the large variation in facial appearance across different persons of the same age and the nonstationary property of aging patterns. In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation. DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with heterogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes. This joint learning follows an alternating strategy: First, by ﬁxing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by ﬁxing the split nodes, the leaf nodes are optimized by iterating a step-size free and fastconverging update rule derived from Variational Bounding. We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them.\n\n**翻译**：\n\n来自面部图像的年龄估计通常被视为非线性回归问题。这个问题的主要挑战是面部特征空间w.r.t.年龄是不均匀的，这是由于同一年龄段不同人的面部外观差异很大，以及老龄化模式的非平稳性。在本文中，我们提出了深度回归森林（DRFs），一种端到端模型，用于年龄估计。DRF将分裂节点连接到卷积神经网络（CNN）的完全连接层，并通过联合学习分裂节点处的输入相关数据分区和叶节点处的数据抽象来处理异构数据。这种联合学习遵循交替策略：首先，通过固定叶节点，分裂节点以及CNN参数通过反向传播进行优化;然后，通过修剪拆分节点，通过迭代从变分边界派生的步长自由和快速收敛更新规则来优化叶节点。我们在三个标准年龄估算基准上验证了提议的DRF，并在所有这些基准上取得了最新的成果。\n\n![83KC{3A7Y}PVX~YU`383R](/images/Q83KC%7B3A7Y%7DPVX~YU%60383RN.png)\n\n## Contributions\n\n- We propose Deep Regression Forests (DRFs), an endto-end model, to deal with heterogeneous data by jointly learning input-dependant data partition at split nodes and data abstraction at leaf nodes .\n- Based on Variational Bounding, the convergence of our update rule for leaf nodes in DRFs is mathematically guaranteed.\n- We apply DRFs on three standard age estimation\n  benchmarks, and achieve state-of-the-art results. \n\n## Summary\n\n用深度回归森林的思想来进行年龄估计的任务，属于一种distribution learning的办法，论文中涉及到大量的概率论的知识和大量的推导公式，将年龄分布分布到多个叶子节点，最终采用随机森林和决策树的思想，对模型训练结果做融合，模型很好训练，且准确率也不错。\n\n# Mean-Variance Loss\n\n**paper**:[Mean-Variance Loss for Deep Age Estimation From a Face](http://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Mean-Variance_Loss_for_CVPR_2018_paper.pdf)`CVPR2018`\n\n**code:**-\n\n## Abstract\n\nAge estimation has wide applications in video surveillance, social networking, and human-computer interaction. Many of the published approaches simply treat age estimation as an exact age regression problem, and thus do not leverage a distribution’s robustness in representing labels with ambiguity such as ages. In this paper, we propose a new loss function, called mean-variance loss, for robust age estimation via distribution learning. Speciﬁcally, the mean-variance loss consists of a mean loss, which penalizes difference between the mean of the estimated age distribution and the ground-truth age, and a variance loss, which penalizes the variance of the estimated age distribution to ensure a concentrated distribution. The proposed meanvariance loss and softmax loss are jointly embedded into Convolutional Neural Networks (CNNs) for age estimation. Experimental results on the FG-NET, MORPH Album II, CLAP2016, and AADB databases show that the proposed approach outperforms the state-of-the-art age estimation methods by a large margin, and generalizes well to image aesthetics assessment.1\n\n**翻译**：\n\n年龄估计在视频监控，社交网络和人机交互中具有广泛的应用。许多已发表的方法仅将年龄估计视为精确的年龄回归问题，因此不会利用分布的稳健性来表示具有模糊性的标签，例如年龄。在本文中，我们提出了一种新的损失函数，称为均值 - 方差损失，用于通过分布学习进行稳健的年龄估计。具体而言，均值 - 方差损失包括平均损失，其惩罚估计年龄分布的平均值与地面实际年龄之间的差异，以及方差损失，其惩罚估计年龄分布的方差以确保集中分布。 。所提出的均值方差损失和softmax损失共同嵌入到卷积神经网络（CNN）中用于年龄估计。在FG-NET，MORPH Album II，CLAP2016和AADB数据库上的实验结果表明，所提出的方法大大超过了现有技术的年龄估计方法，并且很好地概括了图像美学评估。\n\n![Q图片2018120213002](/images/QQ%E5%9B%BE%E7%89%8720181202130028.png)\n\n\n\n## Contributions\n\n1. propose a new loss:**mean-variance loss**,aiming at the estimate of an age distribution with its mean as close to the groud-truth age as possible , and its variance as small as possible\n2. diffferent from the age disturibution learning methods,the proposed approach **doesn't require that each training image must have a mean age and a variance** (neither real nor assumed) labels during model training, but it can still give a distribution estimate for a face image\n3. the proposed loss can be **easily embedded into different CNNs**, and the network can be optimized via SGD **end-to-end**\n\n## Summary\n\n思想简单且新颖，有用且有效。","slug":"年龄估计","published":1,"updated":"2018-12-05T08:58:33.030Z","_id":"cjpaxs0nb000ys0vwgak6btz9","comments":1,"layout":"post","photos":[],"link":"","content":"<h1 id=\"Dataset\"><a href=\"#Dataset\" class=\"headerlink\" title=\"Dataset\"></a>Dataset</h1><ul>\n<li><strong>FG-Net</strong>:1002 images,0到69岁，82个人</li>\n<li><strong>MORPH1</strong>:1690 images</li>\n<li><strong>MORPH2</strong>:55608 images、unbalanced ethnic(96% African and European ,less than 1% Asian)，16到77岁</li>\n<li><strong>AFAD</strong>:160K Asian facial images(unrelease)</li>\n</ul>\n<h1 id=\"Ordinal-Regression\"><a href=\"#Ordinal-Regression\" class=\"headerlink\" title=\"Ordinal Regression\"></a>Ordinal Regression</h1><p><strong>paper：</strong><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Niu_Ordinal_Regression_With_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"external\">Ordinal Regression With Multiple Output CNN for Age Estimation</a><code>CVPR2016</code></p>\n<p><strong>code:</strong><a href=\"https://github.com/luoyetx/OrdinalRegression\" target=\"_blank\" rel=\"external\">Ordinal Regression</a></p>\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>To address the non-stationary property of aging patterns, age estimation can be cast as an ordinal regression problem.However, the processes of extracting features and learning a regression model are often separated and optimized independently in previous work. In this paper, we propose an End-to-End learning approach to address ordinal regression problems using deep Convolutional Neural Network, which could simultaneously conduct feature learning andregression modeling. In particular, an ordinal regression problem is transformed into a series of binary classification sub-problems. And we propose a multiple output CNN learning algorithm to collectively solve these classification sub-problems, so that the correlation between these tasks could be explored. In addition, we publish an Asian Face Age Dataset (AFAD) containing more than 160K facial images with precise age ground-truths, which is the largest public age dataset to date. To the best of our knowledge, this is the first work to address ordinal regression problems by using CNN, and achieves the state-of-the-art performance on both the MORPH and AFAD datasets </p>\n<p><strong>翻译</strong>：为了解决衰老模式的非平稳特性，可以将年龄估计作为序数回归问题。然而，在以前的工作中，提取特征和学习回归模型的过程通常是独立分离和优化的。在本文中，我们提出了一种端到端学习方法，使用深度卷积神经网络解决序数回归问题，可以同时进行特征学习和回归建模。特别是，序数回归问题转化为一系列二元分类子问题。我们提出了一种多输出CNN学习算法来集体解决这些分类子问题，从而可以探索这些任务之间的相关性。此外，我们发布了一个亚洲面部年龄数据集（AFAD），其中包含超过<strong>160K</strong>的面部图像，具有精确的年龄真实性，这是迄今为止最大的公共年龄数据集。据我们所知，这是通过使用CNN解决序数回归问题的第一项工作，并在MORPH和AFAD数据集上实现了最先进的性能。</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181202120825.png\" alt=\"Q图片2018120212082\"></p>\n<h2 id=\"Contributions\"><a href=\"#Contributions\" class=\"headerlink\" title=\"Contributions\"></a>Contributions</h2><ul>\n<li>利用端到端的深度学习方法解决序数回归问题</li>\n<li>released a dataset named AFAD , the largest public dataset to date</li>\n</ul>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>Rank的思想比较好，针对有序的问题，将序数回归转化为一系列的二分类问题，但是这篇论文并不是第一个使用ranking思想的论文，和以前的方法（提取手工特征或者深度学习特征，然后用SVM/SVR的分类器作为二分类器）不同的是，将ranking和CNN结合起来，可以端到端的训练。</p>\n<h1 id=\"RankingCNN\"><a href=\"#RankingCNN\" class=\"headerlink\" title=\"RankingCNN\"></a>RankingCNN</h1><p><strong>paper</strong>：<a href=\"http://openaccess.thecvf.com/content_cvpr_2017/poster/2148_POSTER.pdf\" target=\"_blank\" rel=\"external\">Using Ranking-CNN for Age Estimation</a><code>CVPR2017</code></p>\n<p><strong>code:</strong> <a href=\"https://github.com/RankingCNN/Using-Ranking-CNN-for-Age-Estimation\" target=\"_blank\" rel=\"external\">Using-Ranking-CNN-for-Age-Estimation</a></p>\n<h2 id=\"Abstract-1\"><a href=\"#Abstract-1\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>Human age is considered an important biometric trait for human identification or search. Recent research shows that the aging features deeply learned from large-scale data lead to significant performance improvement on facial image-based age estimation. However, age-related ordinal information is totally ignored in these approaches. In this paper, we propose a novel Convolutional Neural Network (CNN)-based framework, ranking-CNN, for age estimation. Ranking-CNN contains a series of basic CNNs, each of<br>which is trained with ordinal age labels. Then, their binary outputs are aggregated for the final age prediction. We theoretically obtain a much tighter error bound for rankingbased age estimation. Moreover, we rigorously prove that ranking-CNN is more likely to get smaller estimation errors when compared with multi-class classification approaches. Through extensive experiments, we show that statistically, ranking-CNN significantly outperforms other state-of-theart age estimation models on benchmark datasets .</p>\n<p><strong>翻译</strong>:</p>\n<p>人类年龄被认为是一个重要的生物特征，对人类的识别或搜索。最近的研究表明，从大规模数据中深入学习到的年龄特征使得基于人脸图像的年龄估计的性能有了显着的提高。然而，在这些方法中，与年龄相关的顺序信息被完全忽略。本文提出了一种新的基于卷积神经网络(CNN)的年龄估计框架-Ranking-CNN。Ranjing-CNN包含一系列基本CNN，每个CNN都是用顺序年龄标签训练的。然后，将它们的二进制输出进行聚合，以进行最终的年龄预测。我们从理论上得到了一个更严格的基于秩的年龄估计的误差界。此外，我们严格地证明了，与多类分类方法相比，Ranking-CNN更容易获得较小的估计误差。通过大量的实验，我们表明，在统计上，Ranking-CNN在基准数据集上的性能明显优于其他最先进的年龄估计模型。</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181202121159.png\" alt=\"Q图片2018120212115\"><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181202121223.png\" alt=\"Q图片2018120212122\"></p>\n<h2 id=\"Contributions-1\"><a href=\"#Contributions-1\" class=\"headerlink\" title=\"Contributions\"></a>Contributions</h2><ul>\n<li>To the best of our knowledge, ranking-CNN is the first work that uses a deep ranking model for age estimation, in which binary ordinal age labels are used to train a series of basic CNNs, one for each age group.Each basic CNN in ranking-CNN can be trained using all the labeled data, leading to better performance of feature learning and also preventing overfitting.</li>\n<li>We provide a much tighter error bound for age ranking than that introduced in [2], which claimed that the final ranking error is bounded by the sum of errors generated by all the classifiers. We obtain the approximation for the final ranking error that is controlled by the maximum error produced among sub-problems. From a technical perspective, the tighter error bound provides several advantages for the training of ranking-CNN.</li>\n<li>We prove that ranking-CNN, by taking the ordinal relation between ages into consideration, is more likely to get smaller estimation errors when compared with multi-class classification approaches (i.e., CNNs using the softmax function). Moreover, through extensive experiments, we show that statistically, ranking-CNN significantly outperforms other state-of-the-art age estimation methods </li>\n</ul>\n<h2 id=\"Summary-1\"><a href=\"#Summary-1\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>这篇论文和上篇oridnal regression的思想类似，但是效果要好很多，不同的是每个二分类器都有单独的模型，所有二分类器都不共享中间层，然后聚合所有二分类器的结果得到最终的年龄估计，另外还会得到最终排序误差的近似值，该误差由子问题中产生的最大误差控制，比上一篇论文的方法误差控制更严格。</p>\n<h1 id=\"SSR-Net\"><a href=\"#SSR-Net\" class=\"headerlink\" title=\"SSR-Net\"></a>SSR-Net</h1><p><strong>paper</strong>：<a href=\"https://www.csie.ntu.edu.tw/~cyy/publications/papers/Yang2018SSR.pdf\" target=\"_blank\" rel=\"external\">SSR-Net: A Compact Soft Stagewise Regression Network for Age Estimation</a><code>IJCAI2018</code></p>\n<p><strong>code:</strong> <a href=\"https://github.com/shamangary/SSR-Net\" target=\"_blank\" rel=\"external\">SSRNet</a></p>\n<h2 id=\"Abstract-2\"><a href=\"#Abstract-2\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>This paper presents a novel CNN model called Soft Stagewise Regression Network (SSR-Net) for age estimation from a single image with a compact model size. Inspired by DEX, we address age estimation by performing multi-class classification andthen turning classification results into regression by calculating the expected values. SSR-Net takes a coarse -to-fine strategy and performs multi-class classification with multiple stages.Each stage is only responsible for refining the decision of its previous stage for more accurate age estimation. Thus, each stage performs a task with few classes and requires few neurons, greatly reducing the model size. For addressing the quantization issue introduced by grouping ages into classes, SSR-Net assigns a dynamic range to each age class by allowing it to be shifted and scaled according to the input face image. Both the multi-stage strategy and the dynamic range are incorporated into the formulation of soft stagewise regression. A novel network architecture is proposed for carrying out soft stagewise regression. The resultant SSR-Net model is very compact and takes only 0.32 MB. Despite its compact size, SSR-Net’ s performance approaches those of the state -of-the -art methods whose model sizes are often more than 1500x larger.</p>\n<p><strong>翻译</strong>：本文提出了一种新的CNN模型，称为Soft stagewise regression net(SSR-Net)，使用一个很小的compact模型对单个图像进行年龄估计。受DEX的启发，我们通过执行多类分类来解决年龄估计，然后通过计算预期值将分类结果转化为回归。 SSR-Net采用粗略到精细的策略并执行多阶段的多级分类。每个阶段仅负责完善其前一阶段的决策以进行更准确的年龄估计。因此，每个阶段执行具有少量类的任务并且需要很少的神经元，从而大大减小了模型尺寸。为了解决通过将年龄分组到类中引入的量化问题，SSR-Net通过允许根据输入面部图像移位和缩放来为每个年龄类别分配动态范围。多阶段策略和动态范围都包含在软阶段回归的公式中。提出了一种新颖的网络架构，用于进行软分段回归。由此产生的SSR-Net模型非常紧凑，仅需0.32MB。尽管尺寸紧凑，但SSR-Net的性能接近于那些模型尺寸通常超过1500倍的方法。</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181202120956.png\" alt=\"Q图片2018120212095\"></p>\n<h2 id=\"Contributions-2\"><a href=\"#Contributions-2\" class=\"headerlink\" title=\"Contributions\"></a>Contributions</h2><p>新的年龄估计模型，平均误差绝对值为3.16，与最好的模型（当时）差距0.5岁，但参数不到其1/1000，整个模型参数仅0.3M，非常适合用于嵌入式系统。</p>\n<h2 id=\"Summary-2\"><a href=\"#Summary-2\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>采用多分段的多级分类的思想，大大减少了模型的参数，在准确性和模型大小方面做了一个均衡，另外，这么小的模型做到这种准确率很厉害。</p>\n<h1 id=\"DeepRegressionForest\"><a href=\"#DeepRegressionForest\" class=\"headerlink\" title=\"DeepRegressionForest\"></a>DeepRegressionForest</h1><p><strong>paper:</strong><a href=\"https://arxiv.org/abs/1712.07195\" target=\"_blank\" rel=\"external\">Deep Regression Forests for Age Estimation</a><code>CVPR2018</code></p>\n<p><strong>code</strong>:<a href=\"https://github.com/shenwei1231/caffe-DeepRegressionForests\" target=\"_blank\" rel=\"external\">caffe-DeepRegressionForests</a></p>\n<h2 id=\"Abstract-3\"><a href=\"#Abstract-3\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>摘要</p>\n<p>Age estimation from facial images is typically cast as a nonlinear regression problem. The main challenge of this problem is the facial feature space w.r.t. ages is heterogeneous, due to the large variation in facial appearance across different persons of the same age and the nonstationary property of aging patterns. In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation. DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with heterogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes. This joint learning follows an alternating strategy: First, by ﬁxing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by ﬁxing the split nodes, the leaf nodes are optimized by iterating a step-size free and fastconverging update rule derived from Variational Bounding. We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them.</p>\n<p><strong>翻译</strong>：</p>\n<p>来自面部图像的年龄估计通常被视为非线性回归问题。这个问题的主要挑战是面部特征空间w.r.t.年龄是不均匀的，这是由于同一年龄段不同人的面部外观差异很大，以及老龄化模式的非平稳性。在本文中，我们提出了深度回归森林（DRFs），一种端到端模型，用于年龄估计。DRF将分裂节点连接到卷积神经网络（CNN）的完全连接层，并通过联合学习分裂节点处的输入相关数据分区和叶节点处的数据抽象来处理异构数据。这种联合学习遵循交替策略：首先，通过固定叶节点，分裂节点以及CNN参数通过反向传播进行优化;然后，通过修剪拆分节点，通过迭代从变分边界派生的步长自由和快速收敛更新规则来优化叶节点。我们在三个标准年龄估算基准上验证了提议的DRF，并在所有这些基准上取得了最新的成果。</p>\n<p><img src=\"/images/Q83KC%7B3A7Y%7DPVX~YU%60383RN.png\" alt=\"83KC{3A7Y}PVX~YU`383R\"></p>\n<h2 id=\"Contributions-3\"><a href=\"#Contributions-3\" class=\"headerlink\" title=\"Contributions\"></a>Contributions</h2><ul>\n<li>We propose Deep Regression Forests (DRFs), an endto-end model, to deal with heterogeneous data by jointly learning input-dependant data partition at split nodes and data abstraction at leaf nodes .</li>\n<li>Based on Variational Bounding, the convergence of our update rule for leaf nodes in DRFs is mathematically guaranteed.</li>\n<li>We apply DRFs on three standard age estimation<br>benchmarks, and achieve state-of-the-art results. </li>\n</ul>\n<h2 id=\"Summary-3\"><a href=\"#Summary-3\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>用深度回归森林的思想来进行年龄估计的任务，属于一种distribution learning的办法，论文中涉及到大量的概率论的知识和大量的推导公式，将年龄分布分布到多个叶子节点，最终采用随机森林和决策树的思想，对模型训练结果做融合，模型很好训练，且准确率也不错。</p>\n<h1 id=\"Mean-Variance-Loss\"><a href=\"#Mean-Variance-Loss\" class=\"headerlink\" title=\"Mean-Variance Loss\"></a>Mean-Variance Loss</h1><p><strong>paper</strong>:<a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Mean-Variance_Loss_for_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"external\">Mean-Variance Loss for Deep Age Estimation From a Face</a><code>CVPR2018</code></p>\n<p><strong>code:</strong>-</p>\n<h2 id=\"Abstract-4\"><a href=\"#Abstract-4\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>Age estimation has wide applications in video surveillance, social networking, and human-computer interaction. Many of the published approaches simply treat age estimation as an exact age regression problem, and thus do not leverage a distribution’s robustness in representing labels with ambiguity such as ages. In this paper, we propose a new loss function, called mean-variance loss, for robust age estimation via distribution learning. Speciﬁcally, the mean-variance loss consists of a mean loss, which penalizes difference between the mean of the estimated age distribution and the ground-truth age, and a variance loss, which penalizes the variance of the estimated age distribution to ensure a concentrated distribution. The proposed meanvariance loss and softmax loss are jointly embedded into Convolutional Neural Networks (CNNs) for age estimation. Experimental results on the FG-NET, MORPH Album II, CLAP2016, and AADB databases show that the proposed approach outperforms the state-of-the-art age estimation methods by a large margin, and generalizes well to image aesthetics assessment.1</p>\n<p><strong>翻译</strong>：</p>\n<p>年龄估计在视频监控，社交网络和人机交互中具有广泛的应用。许多已发表的方法仅将年龄估计视为精确的年龄回归问题，因此不会利用分布的稳健性来表示具有模糊性的标签，例如年龄。在本文中，我们提出了一种新的损失函数，称为均值 - 方差损失，用于通过分布学习进行稳健的年龄估计。具体而言，均值 - 方差损失包括平均损失，其惩罚估计年龄分布的平均值与地面实际年龄之间的差异，以及方差损失，其惩罚估计年龄分布的方差以确保集中分布。 。所提出的均值方差损失和softmax损失共同嵌入到卷积神经网络（CNN）中用于年龄估计。在FG-NET，MORPH Album II，CLAP2016和AADB数据库上的实验结果表明，所提出的方法大大超过了现有技术的年龄估计方法，并且很好地概括了图像美学评估。</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181202130028.png\" alt=\"Q图片2018120213002\"></p>\n<h2 id=\"Contributions-4\"><a href=\"#Contributions-4\" class=\"headerlink\" title=\"Contributions\"></a>Contributions</h2><ol>\n<li>propose a new loss:<strong>mean-variance loss</strong>,aiming at the estimate of an age distribution with its mean as close to the groud-truth age as possible , and its variance as small as possible</li>\n<li>diffferent from the age disturibution learning methods,the proposed approach <strong>doesn’t require that each training image must have a mean age and a variance</strong> (neither real nor assumed) labels during model training, but it can still give a distribution estimate for a face image</li>\n<li>the proposed loss can be <strong>easily embedded into different CNNs</strong>, and the network can be optimized via SGD <strong>end-to-end</strong></li>\n</ol>\n<h2 id=\"Summary-4\"><a href=\"#Summary-4\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>思想简单且新颖，有用且有效。</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Dataset\"><a href=\"#Dataset\" class=\"headerlink\" title=\"Dataset\"></a>Dataset</h1><ul>\n<li><strong>FG-Net</strong>:1002 images,0到69岁，82个人</li>\n<li><strong>MORPH1</strong>:1690 images</li>\n<li><strong>MORPH2</strong>:55608 images、unbalanced ethnic(96% African and European ,less than 1% Asian)，16到77岁</li>\n<li><strong>AFAD</strong>:160K Asian facial images(unrelease)</li>\n</ul>\n<h1 id=\"Ordinal-Regression\"><a href=\"#Ordinal-Regression\" class=\"headerlink\" title=\"Ordinal Regression\"></a>Ordinal Regression</h1><p><strong>paper：</strong><a href=\"https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Niu_Ordinal_Regression_With_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"external\">Ordinal Regression With Multiple Output CNN for Age Estimation</a><code>CVPR2016</code></p>\n<p><strong>code:</strong><a href=\"https://github.com/luoyetx/OrdinalRegression\" target=\"_blank\" rel=\"external\">Ordinal Regression</a></p>\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>To address the non-stationary property of aging patterns, age estimation can be cast as an ordinal regression problem.However, the processes of extracting features and learning a regression model are often separated and optimized independently in previous work. In this paper, we propose an End-to-End learning approach to address ordinal regression problems using deep Convolutional Neural Network, which could simultaneously conduct feature learning andregression modeling. In particular, an ordinal regression problem is transformed into a series of binary classification sub-problems. And we propose a multiple output CNN learning algorithm to collectively solve these classification sub-problems, so that the correlation between these tasks could be explored. In addition, we publish an Asian Face Age Dataset (AFAD) containing more than 160K facial images with precise age ground-truths, which is the largest public age dataset to date. To the best of our knowledge, this is the first work to address ordinal regression problems by using CNN, and achieves the state-of-the-art performance on both the MORPH and AFAD datasets </p>\n<p><strong>翻译</strong>：为了解决衰老模式的非平稳特性，可以将年龄估计作为序数回归问题。然而，在以前的工作中，提取特征和学习回归模型的过程通常是独立分离和优化的。在本文中，我们提出了一种端到端学习方法，使用深度卷积神经网络解决序数回归问题，可以同时进行特征学习和回归建模。特别是，序数回归问题转化为一系列二元分类子问题。我们提出了一种多输出CNN学习算法来集体解决这些分类子问题，从而可以探索这些任务之间的相关性。此外，我们发布了一个亚洲面部年龄数据集（AFAD），其中包含超过<strong>160K</strong>的面部图像，具有精确的年龄真实性，这是迄今为止最大的公共年龄数据集。据我们所知，这是通过使用CNN解决序数回归问题的第一项工作，并在MORPH和AFAD数据集上实现了最先进的性能。</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181202120825.png\" alt=\"Q图片2018120212082\"></p>\n<h2 id=\"Contributions\"><a href=\"#Contributions\" class=\"headerlink\" title=\"Contributions\"></a>Contributions</h2><ul>\n<li>利用端到端的深度学习方法解决序数回归问题</li>\n<li>released a dataset named AFAD , the largest public dataset to date</li>\n</ul>\n<h2 id=\"Summary\"><a href=\"#Summary\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>Rank的思想比较好，针对有序的问题，将序数回归转化为一系列的二分类问题，但是这篇论文并不是第一个使用ranking思想的论文，和以前的方法（提取手工特征或者深度学习特征，然后用SVM/SVR的分类器作为二分类器）不同的是，将ranking和CNN结合起来，可以端到端的训练。</p>\n<h1 id=\"RankingCNN\"><a href=\"#RankingCNN\" class=\"headerlink\" title=\"RankingCNN\"></a>RankingCNN</h1><p><strong>paper</strong>：<a href=\"http://openaccess.thecvf.com/content_cvpr_2017/poster/2148_POSTER.pdf\" target=\"_blank\" rel=\"external\">Using Ranking-CNN for Age Estimation</a><code>CVPR2017</code></p>\n<p><strong>code:</strong> <a href=\"https://github.com/RankingCNN/Using-Ranking-CNN-for-Age-Estimation\" target=\"_blank\" rel=\"external\">Using-Ranking-CNN-for-Age-Estimation</a></p>\n<h2 id=\"Abstract-1\"><a href=\"#Abstract-1\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>Human age is considered an important biometric trait for human identification or search. Recent research shows that the aging features deeply learned from large-scale data lead to significant performance improvement on facial image-based age estimation. However, age-related ordinal information is totally ignored in these approaches. In this paper, we propose a novel Convolutional Neural Network (CNN)-based framework, ranking-CNN, for age estimation. Ranking-CNN contains a series of basic CNNs, each of<br>which is trained with ordinal age labels. Then, their binary outputs are aggregated for the final age prediction. We theoretically obtain a much tighter error bound for rankingbased age estimation. Moreover, we rigorously prove that ranking-CNN is more likely to get smaller estimation errors when compared with multi-class classification approaches. Through extensive experiments, we show that statistically, ranking-CNN significantly outperforms other state-of-theart age estimation models on benchmark datasets .</p>\n<p><strong>翻译</strong>:</p>\n<p>人类年龄被认为是一个重要的生物特征，对人类的识别或搜索。最近的研究表明，从大规模数据中深入学习到的年龄特征使得基于人脸图像的年龄估计的性能有了显着的提高。然而，在这些方法中，与年龄相关的顺序信息被完全忽略。本文提出了一种新的基于卷积神经网络(CNN)的年龄估计框架-Ranking-CNN。Ranjing-CNN包含一系列基本CNN，每个CNN都是用顺序年龄标签训练的。然后，将它们的二进制输出进行聚合，以进行最终的年龄预测。我们从理论上得到了一个更严格的基于秩的年龄估计的误差界。此外，我们严格地证明了，与多类分类方法相比，Ranking-CNN更容易获得较小的估计误差。通过大量的实验，我们表明，在统计上，Ranking-CNN在基准数据集上的性能明显优于其他最先进的年龄估计模型。</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181202121159.png\" alt=\"Q图片2018120212115\"><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181202121223.png\" alt=\"Q图片2018120212122\"></p>\n<h2 id=\"Contributions-1\"><a href=\"#Contributions-1\" class=\"headerlink\" title=\"Contributions\"></a>Contributions</h2><ul>\n<li>To the best of our knowledge, ranking-CNN is the first work that uses a deep ranking model for age estimation, in which binary ordinal age labels are used to train a series of basic CNNs, one for each age group.Each basic CNN in ranking-CNN can be trained using all the labeled data, leading to better performance of feature learning and also preventing overfitting.</li>\n<li>We provide a much tighter error bound for age ranking than that introduced in [2], which claimed that the final ranking error is bounded by the sum of errors generated by all the classifiers. We obtain the approximation for the final ranking error that is controlled by the maximum error produced among sub-problems. From a technical perspective, the tighter error bound provides several advantages for the training of ranking-CNN.</li>\n<li>We prove that ranking-CNN, by taking the ordinal relation between ages into consideration, is more likely to get smaller estimation errors when compared with multi-class classification approaches (i.e., CNNs using the softmax function). Moreover, through extensive experiments, we show that statistically, ranking-CNN significantly outperforms other state-of-the-art age estimation methods </li>\n</ul>\n<h2 id=\"Summary-1\"><a href=\"#Summary-1\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>这篇论文和上篇oridnal regression的思想类似，但是效果要好很多，不同的是每个二分类器都有单独的模型，所有二分类器都不共享中间层，然后聚合所有二分类器的结果得到最终的年龄估计，另外还会得到最终排序误差的近似值，该误差由子问题中产生的最大误差控制，比上一篇论文的方法误差控制更严格。</p>\n<h1 id=\"SSR-Net\"><a href=\"#SSR-Net\" class=\"headerlink\" title=\"SSR-Net\"></a>SSR-Net</h1><p><strong>paper</strong>：<a href=\"https://www.csie.ntu.edu.tw/~cyy/publications/papers/Yang2018SSR.pdf\" target=\"_blank\" rel=\"external\">SSR-Net: A Compact Soft Stagewise Regression Network for Age Estimation</a><code>IJCAI2018</code></p>\n<p><strong>code:</strong> <a href=\"https://github.com/shamangary/SSR-Net\" target=\"_blank\" rel=\"external\">SSRNet</a></p>\n<h2 id=\"Abstract-2\"><a href=\"#Abstract-2\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>This paper presents a novel CNN model called Soft Stagewise Regression Network (SSR-Net) for age estimation from a single image with a compact model size. Inspired by DEX, we address age estimation by performing multi-class classification andthen turning classification results into regression by calculating the expected values. SSR-Net takes a coarse -to-fine strategy and performs multi-class classification with multiple stages.Each stage is only responsible for refining the decision of its previous stage for more accurate age estimation. Thus, each stage performs a task with few classes and requires few neurons, greatly reducing the model size. For addressing the quantization issue introduced by grouping ages into classes, SSR-Net assigns a dynamic range to each age class by allowing it to be shifted and scaled according to the input face image. Both the multi-stage strategy and the dynamic range are incorporated into the formulation of soft stagewise regression. A novel network architecture is proposed for carrying out soft stagewise regression. The resultant SSR-Net model is very compact and takes only 0.32 MB. Despite its compact size, SSR-Net’ s performance approaches those of the state -of-the -art methods whose model sizes are often more than 1500x larger.</p>\n<p><strong>翻译</strong>：本文提出了一种新的CNN模型，称为Soft stagewise regression net(SSR-Net)，使用一个很小的compact模型对单个图像进行年龄估计。受DEX的启发，我们通过执行多类分类来解决年龄估计，然后通过计算预期值将分类结果转化为回归。 SSR-Net采用粗略到精细的策略并执行多阶段的多级分类。每个阶段仅负责完善其前一阶段的决策以进行更准确的年龄估计。因此，每个阶段执行具有少量类的任务并且需要很少的神经元，从而大大减小了模型尺寸。为了解决通过将年龄分组到类中引入的量化问题，SSR-Net通过允许根据输入面部图像移位和缩放来为每个年龄类别分配动态范围。多阶段策略和动态范围都包含在软阶段回归的公式中。提出了一种新颖的网络架构，用于进行软分段回归。由此产生的SSR-Net模型非常紧凑，仅需0.32MB。尽管尺寸紧凑，但SSR-Net的性能接近于那些模型尺寸通常超过1500倍的方法。</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181202120956.png\" alt=\"Q图片2018120212095\"></p>\n<h2 id=\"Contributions-2\"><a href=\"#Contributions-2\" class=\"headerlink\" title=\"Contributions\"></a>Contributions</h2><p>新的年龄估计模型，平均误差绝对值为3.16，与最好的模型（当时）差距0.5岁，但参数不到其1/1000，整个模型参数仅0.3M，非常适合用于嵌入式系统。</p>\n<h2 id=\"Summary-2\"><a href=\"#Summary-2\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>采用多分段的多级分类的思想，大大减少了模型的参数，在准确性和模型大小方面做了一个均衡，另外，这么小的模型做到这种准确率很厉害。</p>\n<h1 id=\"DeepRegressionForest\"><a href=\"#DeepRegressionForest\" class=\"headerlink\" title=\"DeepRegressionForest\"></a>DeepRegressionForest</h1><p><strong>paper:</strong><a href=\"https://arxiv.org/abs/1712.07195\" target=\"_blank\" rel=\"external\">Deep Regression Forests for Age Estimation</a><code>CVPR2018</code></p>\n<p><strong>code</strong>:<a href=\"https://github.com/shenwei1231/caffe-DeepRegressionForests\" target=\"_blank\" rel=\"external\">caffe-DeepRegressionForests</a></p>\n<h2 id=\"Abstract-3\"><a href=\"#Abstract-3\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>摘要</p>\n<p>Age estimation from facial images is typically cast as a nonlinear regression problem. The main challenge of this problem is the facial feature space w.r.t. ages is heterogeneous, due to the large variation in facial appearance across different persons of the same age and the nonstationary property of aging patterns. In this paper, we propose Deep Regression Forests (DRFs), an end-to-end model, for age estimation. DRFs connect the split nodes to a fully connected layer of a convolutional neural network (CNN) and deal with heterogeneous data by jointly learning input-dependant data partitions at the split nodes and data abstractions at the leaf nodes. This joint learning follows an alternating strategy: First, by ﬁxing the leaf nodes, the split nodes as well as the CNN parameters are optimized by Back-propagation; Then, by ﬁxing the split nodes, the leaf nodes are optimized by iterating a step-size free and fastconverging update rule derived from Variational Bounding. We verify the proposed DRFs on three standard age estimation benchmarks and achieve state-of-the-art results on all of them.</p>\n<p><strong>翻译</strong>：</p>\n<p>来自面部图像的年龄估计通常被视为非线性回归问题。这个问题的主要挑战是面部特征空间w.r.t.年龄是不均匀的，这是由于同一年龄段不同人的面部外观差异很大，以及老龄化模式的非平稳性。在本文中，我们提出了深度回归森林（DRFs），一种端到端模型，用于年龄估计。DRF将分裂节点连接到卷积神经网络（CNN）的完全连接层，并通过联合学习分裂节点处的输入相关数据分区和叶节点处的数据抽象来处理异构数据。这种联合学习遵循交替策略：首先，通过固定叶节点，分裂节点以及CNN参数通过反向传播进行优化;然后，通过修剪拆分节点，通过迭代从变分边界派生的步长自由和快速收敛更新规则来优化叶节点。我们在三个标准年龄估算基准上验证了提议的DRF，并在所有这些基准上取得了最新的成果。</p>\n<p><img src=\"/images/Q83KC%7B3A7Y%7DPVX~YU%60383RN.png\" alt=\"83KC{3A7Y}PVX~YU`383R\"></p>\n<h2 id=\"Contributions-3\"><a href=\"#Contributions-3\" class=\"headerlink\" title=\"Contributions\"></a>Contributions</h2><ul>\n<li>We propose Deep Regression Forests (DRFs), an endto-end model, to deal with heterogeneous data by jointly learning input-dependant data partition at split nodes and data abstraction at leaf nodes .</li>\n<li>Based on Variational Bounding, the convergence of our update rule for leaf nodes in DRFs is mathematically guaranteed.</li>\n<li>We apply DRFs on three standard age estimation<br>benchmarks, and achieve state-of-the-art results. </li>\n</ul>\n<h2 id=\"Summary-3\"><a href=\"#Summary-3\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>用深度回归森林的思想来进行年龄估计的任务，属于一种distribution learning的办法，论文中涉及到大量的概率论的知识和大量的推导公式，将年龄分布分布到多个叶子节点，最终采用随机森林和决策树的思想，对模型训练结果做融合，模型很好训练，且准确率也不错。</p>\n<h1 id=\"Mean-Variance-Loss\"><a href=\"#Mean-Variance-Loss\" class=\"headerlink\" title=\"Mean-Variance Loss\"></a>Mean-Variance Loss</h1><p><strong>paper</strong>:<a href=\"http://openaccess.thecvf.com/content_cvpr_2018/papers/Pan_Mean-Variance_Loss_for_CVPR_2018_paper.pdf\" target=\"_blank\" rel=\"external\">Mean-Variance Loss for Deep Age Estimation From a Face</a><code>CVPR2018</code></p>\n<p><strong>code:</strong>-</p>\n<h2 id=\"Abstract-4\"><a href=\"#Abstract-4\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>Age estimation has wide applications in video surveillance, social networking, and human-computer interaction. Many of the published approaches simply treat age estimation as an exact age regression problem, and thus do not leverage a distribution’s robustness in representing labels with ambiguity such as ages. In this paper, we propose a new loss function, called mean-variance loss, for robust age estimation via distribution learning. Speciﬁcally, the mean-variance loss consists of a mean loss, which penalizes difference between the mean of the estimated age distribution and the ground-truth age, and a variance loss, which penalizes the variance of the estimated age distribution to ensure a concentrated distribution. The proposed meanvariance loss and softmax loss are jointly embedded into Convolutional Neural Networks (CNNs) for age estimation. Experimental results on the FG-NET, MORPH Album II, CLAP2016, and AADB databases show that the proposed approach outperforms the state-of-the-art age estimation methods by a large margin, and generalizes well to image aesthetics assessment.1</p>\n<p><strong>翻译</strong>：</p>\n<p>年龄估计在视频监控，社交网络和人机交互中具有广泛的应用。许多已发表的方法仅将年龄估计视为精确的年龄回归问题，因此不会利用分布的稳健性来表示具有模糊性的标签，例如年龄。在本文中，我们提出了一种新的损失函数，称为均值 - 方差损失，用于通过分布学习进行稳健的年龄估计。具体而言，均值 - 方差损失包括平均损失，其惩罚估计年龄分布的平均值与地面实际年龄之间的差异，以及方差损失，其惩罚估计年龄分布的方差以确保集中分布。 。所提出的均值方差损失和softmax损失共同嵌入到卷积神经网络（CNN）中用于年龄估计。在FG-NET，MORPH Album II，CLAP2016和AADB数据库上的实验结果表明，所提出的方法大大超过了现有技术的年龄估计方法，并且很好地概括了图像美学评估。</p>\n<p><img src=\"/images/QQ%E5%9B%BE%E7%89%8720181202130028.png\" alt=\"Q图片2018120213002\"></p>\n<h2 id=\"Contributions-4\"><a href=\"#Contributions-4\" class=\"headerlink\" title=\"Contributions\"></a>Contributions</h2><ol>\n<li>propose a new loss:<strong>mean-variance loss</strong>,aiming at the estimate of an age distribution with its mean as close to the groud-truth age as possible , and its variance as small as possible</li>\n<li>diffferent from the age disturibution learning methods,the proposed approach <strong>doesn’t require that each training image must have a mean age and a variance</strong> (neither real nor assumed) labels during model training, but it can still give a distribution estimate for a face image</li>\n<li>the proposed loss can be <strong>easily embedded into different CNNs</strong>, and the network can be optimized via SGD <strong>end-to-end</strong></li>\n</ol>\n<h2 id=\"Summary-4\"><a href=\"#Summary-4\" class=\"headerlink\" title=\"Summary\"></a>Summary</h2><p>思想简单且新颖，有用且有效。</p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjpaxs0hu0002s0vw03tnnzqa","category_id":"cjpaxs0jp000as0vw6h6g55ue","_id":"cjpaxs0lh000ls0vwxk8086qh"},{"post_id":"cjpaxs0la000js0vwdvuuk6zm","category_id":"cjpaxs0jp000as0vw6h6g55ue","_id":"cjpaxs0mw000ss0vwy93kea3m"},{"post_id":"cjpaxs0ii0006s0vwibpejvrb","category_id":"cjpaxs0kn000gs0vw3c1c7re0","_id":"cjpaxs0n8000xs0vwmtijrz5d"},{"post_id":"cjpaxs0j90009s0vwg4rdbq2m","category_id":"cjpaxs0lk000ms0vwr7nybq8f","_id":"cjpaxs0ne000zs0vwxifuywnb"},{"post_id":"cjpaxs0ms000rs0vwcp17ll2y","category_id":"cjpaxs0jp000as0vw6h6g55ue","_id":"cjpaxs0nl0011s0vw8lqam0wt"},{"post_id":"cjpaxs0n6000ws0vwqvsg038r","category_id":"cjpaxs0kn000gs0vw3c1c7re0","_id":"cjpaxs0np0013s0vwlr5q8a8l"},{"post_id":"cjpaxs0mc000os0vwchju6a5e","category_id":"cjpaxs0n2000us0vwogunn2sl","_id":"cjpaxs0o40018s0vwexk3firn"},{"post_id":"cjpaxs0nb000ys0vwgak6btz9","category_id":"cjpaxs0kn000gs0vw3c1c7re0","_id":"cjpaxx68i0001ggvwm7uzvlcc"},{"post_id":"cjpaxs0le000ks0vwpud9zaxz","category_id":"cjpaywler00046gvwpqy3ko6r","_id":"cjpaywlf000066gvw6kei9772"},{"post_id":"cjpaxs0j00008s0vw5j0obxtu","category_id":"cjpaxs0kn000gs0vw3c1c7re0","_id":"cjpaz0jfl0000ygvwl7rfivhp"},{"post_id":"cjpaxs0gg0000s0vwhkok5jdm","category_id":"cjpaxs0kn000gs0vw3c1c7re0","_id":"cjpaz103s0001iwvwbi4r5a63"},{"post_id":"cjpaxs0kj000fs0vw9p3oz5sf","category_id":"cjpaxs0kn000gs0vw3c1c7re0","_id":"cjpaz103y0003iwvwz3yoys5b"}],"PostTag":[{"post_id":"cjpaxs0hu0002s0vw03tnnzqa","tag_id":"cjpaxs0js000bs0vwqv6nfyz3","_id":"cjpaxs0mn000qs0vwjheef8rl"},{"post_id":"cjpaxs0hu0002s0vw03tnnzqa","tag_id":"cjpaxs0kq000hs0vwfiifg6ah","_id":"cjpaxs0mz000ts0vwkf3jc4en"},{"post_id":"cjpaxs0ii0006s0vwibpejvrb","tag_id":"cjpaxs0lr000ns0vw517v8mkc","_id":"cjpaxs0nn0012s0vw30crp2es"},{"post_id":"cjpaxs0ii0006s0vwibpejvrb","tag_id":"cjpaxs0n4000vs0vw25pj8c6c","_id":"cjpaxs0nr0014s0vw682s2b2z"},{"post_id":"cjpaxs0j00008s0vw5j0obxtu","tag_id":"cjpaxs0nh0010s0vwvxp6vktq","_id":"cjpaxs0o00017s0vwi6lbjauk"},{"post_id":"cjpaxs0j90009s0vwg4rdbq2m","tag_id":"cjpaxs0nt0015s0vwxfp0qg7t","_id":"cjpaxs0o9001as0vw0e6qxy7b"},{"post_id":"cjpaxs0jz000ds0vwzvetr0qk","tag_id":"cjpaxs0o70019s0vw77xdi96b","_id":"cjpaxs0od001cs0vwskbzi996"},{"post_id":"cjpaxs0la000js0vwdvuuk6zm","tag_id":"cjpaxs0oh001ds0vwrlqahwxo","_id":"cjpaxs0os001gs0vwc523vddo"},{"post_id":"cjpaxs0mc000os0vwchju6a5e","tag_id":"cjpaxs0oq001fs0vwrly9e0rm","_id":"cjpaxs0ow001is0vwrvm483db"},{"post_id":"cjpaxs0ms000rs0vwcp17ll2y","tag_id":"cjpaxs0ou001hs0vwgj6ds9qn","_id":"cjpaxs0p3001ls0vwarqlhllo"},{"post_id":"cjpaxs0ms000rs0vwcp17ll2y","tag_id":"cjpaxs0oz001js0vwibf674sw","_id":"cjpaxs0p5001ms0vwx2k39h2j"},{"post_id":"cjpaxs0n6000ws0vwqvsg038r","tag_id":"cjpaxs0p1001ks0vw3erie0sx","_id":"cjpaxs0p9001os0vwt49e61s1"},{"post_id":"cjpaxs0nb000ys0vwgak6btz9","tag_id":"cjpaxs0p7001ns0vw4ravc3l8","_id":"cjpaxx68f0000ggvwmlq92yty"},{"post_id":"cjpaxs0gg0000s0vwhkok5jdm","tag_id":"cjpaxs0ia0005s0vw1oqr3orl","_id":"cjpaz103m0000iwvweu99xfvo"},{"post_id":"cjpaxs0kj000fs0vw9p3oz5sf","tag_id":"cjpaxs0ob001bs0vwk4ou6vwi","_id":"cjpaz103v0002iwvwba8lpxoa"}],"Tag":[{"name":"深度学习，论文笔记","_id":"cjpaxs0ia0005s0vw1oqr3orl"},{"name":"caffe","_id":"cjpaxs0js000bs0vwqv6nfyz3"},{"name":"调参","_id":"cjpaxs0kq000hs0vwfiifg6ah"},{"name":"Face","_id":"cjpaxs0lr000ns0vw517v8mkc"},{"name":"笔记","_id":"cjpaxs0n4000vs0vw25pj8c6c"},{"name":"笔记，人脸识别","_id":"cjpaxs0nh0010s0vwvxp6vktq"},{"name":"Hexo","_id":"cjpaxs0nt0015s0vwxfp0qg7t"},{"name":"Python","_id":"cjpaxs0o70019s0vw77xdi96b"},{"name":"深度学习，人脸识别","_id":"cjpaxs0ob001bs0vwk4ou6vwi"},{"name":"人脸识别","_id":"cjpaxs0oh001ds0vwrlqahwxo"},{"name":"工具","_id":"cjpaxs0oq001fs0vwrly9e0rm"},{"name":"深度学习","_id":"cjpaxs0ou001hs0vwgj6ds9qn"},{"name":"神经网络","_id":"cjpaxs0oz001js0vwibf674sw"},{"name":"杂","_id":"cjpaxs0p1001ks0vw3erie0sx"},{"name":"年龄估计","_id":"cjpaxs0p7001ns0vw4ravc3l8"}]}}