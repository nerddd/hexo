{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/images/images.jpg","path":"images/images.jpg","modified":1,"renderable":0},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":1,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/images/images.jpg","path":"images/images.jpg","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/next/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/next/source/images/favicon.ico","path":"images/favicon.ico","modified":1,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","path":"lib/algolia-instant-search/instantsearch.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","path":"lib/canvas-nest/canvas-nest.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","path":"lib/canvas-ribbon/canvas-ribbon.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/algolia-search.js","path":"js/src/algolia-search.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/exturl.js","path":"js/src/exturl.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/js.cookie.js","path":"js/src/js.cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/scroll-cookie.js","path":"js/src/scroll-cookie.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","path":"lib/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/bower.json","path":"lib/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","path":"lib/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","path":"lib/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","path":"lib/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","path":"lib/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","path":"lib/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","path":"lib/three/canvas_lines.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","path":"lib/three/canvas_sphere.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three-waves.min.js","path":"lib/three/three-waves.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/bower.json","path":"lib/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","path":"lib/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/LICENSE","path":"lib/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/README.md","path":"lib/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/bower.json","path":"lib/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/next/source/lib/jquery/index.js","path":"lib/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/next/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.js","path":"lib/Han/dist/han.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","path":"lib/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","path":"lib/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","path":"lib/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.min.css","path":"lib/Han/dist/han.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","path":"lib/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","path":"lib/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","path":"lib/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","path":"lib/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","path":"lib/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","path":"lib/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","path":"lib/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","path":"lib/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","path":"lib/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","path":"lib/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","path":"lib/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","path":"lib/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","path":"lib/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.js","path":"lib/Han/dist/han.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/han.css","path":"lib/Han/dist/han.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","path":"lib/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.js","path":"lib/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","path":"lib/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","path":"lib/Han/dist/font/han-space.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","path":"lib/Han/dist/font/han-space.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","path":"lib/Han/dist/font/han.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","path":"lib/Han/dist/font/han.woff","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","path":"lib/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","path":"lib/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","path":"lib/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","path":"lib/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","path":"lib/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"lib/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","path":"lib/algolia-instant-search/instantsearch.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/three/three.min.js","path":"lib/three/three.min.js","modified":1,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","path":"lib/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"source/_posts/DeepID3论文笔记.md","hash":"1ead35b32d061b7168077ec0d02a5ce86b7fc2f1","modified":1499149990490},{"_id":"source/_posts/Caffe调参.md","hash":"51466bdb2479d65998316af3099dad653a92482e","modified":1501120391297},{"_id":"source/_posts/人脸识别回顾.md","hash":"27e41c996b154dd0ffb58de482aa4c0e3adcf9f1","modified":1517821820415},{"_id":"source/_posts/Hexo相关.md","hash":"ac42712e0a0b74c402716ee3632385cb4e5b7868","modified":1498652890590},{"_id":"source/_posts/FaceNet论文笔记.md","hash":"177aa0c1cc0b5adea8dbb004161f027ff10d3465","modified":1498656216661},{"_id":"source/_posts/Python.md","hash":"837b8a369bc289f42332110a09f6047f15661eb7","modified":1497405084227},{"_id":"source/_posts/杂知识点.md","hash":"93efbfaa0c4b72e2f917f752fc2eebbe9cfc9bee","modified":1505100277234},{"_id":"source/_posts/行人检测.md","hash":"231e94aaa30e9abcbca6c46076e601a57291d88f","modified":1505994416052},{"_id":"source/about/index.md","hash":"b1c0ea16dab27b36c6961bc037d7cfeff409978e","modified":1496233446892},{"_id":"source/tags/index.md","hash":"27a029fbb025ae4aa8e5e3f65ab9a299f6f58c55","modified":1496233234309},{"_id":"source/_posts/Center-loss笔记.md","hash":"e63256d4e4d7ff9b4401044874e7777e6e8458cd","modified":1498825170937},{"_id":"source/_posts/文档归类.md","hash":"557bde040f9df5c6d11850175a6ca2861094566e","modified":1500277250206},{"_id":"themes/next/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1494683576000},{"_id":"themes/next/.gitattributes","hash":"44bd4729c74ccb88110804f41746fec07bf487d4","modified":1494683576000},{"_id":"themes/next/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1494683576000},{"_id":"themes/next/.bowerrc","hash":"3228a58ed0ece9f85e1e3136352094080b8dece1","modified":1494683576000},{"_id":"themes/next/.gitignore","hash":"32ea93f21d8693d5d8fa4eef1c51a21ad0670047","modified":1494683576000},{"_id":"themes/next/.javascript_ignore","hash":"f9ea3c5395f8feb225a24e2c32baa79afda30c16","modified":1494683576000},{"_id":"source/_posts/Triplet loss.md","hash":"d294f3a168a1d618e5844614d69d888cc29e4668","modified":1496408982119},{"_id":"themes/next/.travis.yml","hash":"c42d9608c8c7fe90de7b1581a8dc3886e90c179e","modified":1494683576000},{"_id":"themes/next/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1494683576000},{"_id":"themes/next/.stylintrc","hash":"b28e24704a5d8de08346c45286574c8e76cc109f","modified":1494683576000},{"_id":"source/_posts/待办及进度.md","hash":"cb10c680bf8692fff4dc7d076cd974bc26baa45c","modified":1517663932579},{"_id":"themes/next/LICENSE","hash":"f293bcfcdc06c0b77ba13570bb8af55eb5c059fd","modified":1494683576000},{"_id":"themes/next/_config.yml","hash":"74c0ba0f9ab6cdd6e15aeefc02fdbe5023fc3993","modified":1496390508132},{"_id":"source/images/images.jpg","hash":"a83aa0596ff8d7ab87cfc47d4a57dd01d6c4ccee","modified":1496232703693},{"_id":"themes/next/package.json","hash":"7e87b2621104b39a30488654c2a8a0c6a563574b","modified":1494683576000},{"_id":"themes/next/bower.json","hash":"be0a430362cb73a7e3cf9ecf51a67edf8214b637","modified":1494683576000},{"_id":"themes/next/README.en.md","hash":"4ece25ee5f64447cd522e54cb0fffd9a375f0bd4","modified":1494683576000},{"_id":"themes/next/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1494683576000},{"_id":"source/categories/index.md","hash":"4041694b9457789fceb11df6a2576543a2e18495","modified":1496233317600},{"_id":"themes/next/gulpfile.coffee","hash":"031bffc483e417b20e90eceb6cf358e7596d2e69","modified":1494683576000},{"_id":"source/_posts/pictures/QQ图片20170724143700.png","hash":"1d2b883010480b3cff8226fefffc4b3635d9bd01","modified":1500723203704},{"_id":"source/_posts/pictures/4]M8LF]VF14SLOBE)L2KZFE.png","hash":"501e9168bb7bd53b7e0583bbc563a0667bb8fa2c","modified":1500878036824},{"_id":"source/_posts/pictures/Q`4VLL2A~D`}00V$`([L4T6.png","hash":"9094446ffe38a5b71219615c605ae59323b7f5d0","modified":1500104208619},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"3b5eafd32abb718e56ccf8d1cee0607ad8ce611d","modified":1494683576000},{"_id":"themes/next/.github/ISSUE_TEMPLATE.md","hash":"fdd63b77472612337309eb93ec415a059b90756b","modified":1494683576000},{"_id":"themes/next/scripts/merge-configs.js","hash":"13c8b3a2d9fce06c2488820d9248d190c8100e0a","modified":1494683576000},{"_id":"themes/next/scripts/merge.js","hash":"9130dabe6a674c54b535f322b17d75fe6081472f","modified":1494683576000},{"_id":"source/_posts/pictures/唐小蔓.png","hash":"09b378d1237988b9575046e3b31446323aea5437","modified":1500723316448},{"_id":"themes/next/test/.jshintrc","hash":"19f93d13d1689fe033c82eb2d5f3ce30b6543cc0","modified":1494683576000},{"_id":"themes/next/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1494683576000},{"_id":"themes/next/layout/_layout.swig","hash":"9d1a23a6add6f3d0f88c2d17979956f14aaa37a4","modified":1494683576000},{"_id":"themes/next/layout/archive.swig","hash":"5de4dca06b05d99e4f6bad617a4b8f4f3592fb01","modified":1494683576000},{"_id":"themes/next/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1494683576000},{"_id":"themes/next/layout/category.swig","hash":"82e7bc278559b4335ad974659104eaaf04863032","modified":1494683576000},{"_id":"themes/next/layout/schedule.swig","hash":"f93c53f6fd5c712584f6efba6f770c30fa8a3e80","modified":1494683576000},{"_id":"themes/next/layout/index.swig","hash":"03e8a2cda03bad42ac0cb827025eb81f95d496a2","modified":1494683576000},{"_id":"themes/next/layout/post.swig","hash":"2d5f8d7f0a96b611e2d5a5e4d111fc17726a990f","modified":1494683576000},{"_id":"themes/next/layout/tag.swig","hash":"2e73ee478e981092ea9a5d10dd472a9461db395b","modified":1494683576000},{"_id":"themes/next/layout/page.swig","hash":"2c6a78999133b991d9221f484aee2eacae894251","modified":1494683576000},{"_id":"themes/next/languages/de.yml","hash":"306db8c865630f32c6b6260ade9d3209fbec8011","modified":1494683576000},{"_id":"themes/next/languages/id.yml","hash":"2835ea80dadf093fcf47edd957680973f1fb6b85","modified":1494683576000},{"_id":"themes/next/languages/en.yml","hash":"e7def07a709ef55684490b700a06998c67f35f39","modified":1494683576000},{"_id":"themes/next/languages/default.yml","hash":"4cc6aeb1ac09a58330e494c8771773758ab354af","modified":1494683576000},{"_id":"themes/next/languages/fr-FR.yml","hash":"24180322c83587a153cea110e74e96eacc3355ad","modified":1494683576000},{"_id":"themes/next/languages/ko.yml","hash":"be150543379150f78329815af427bf152c0e9431","modified":1494683576000},{"_id":"themes/next/languages/ru.yml","hash":"1549a7c2fe23caa7cbedcd0aa2b77c46e57caf27","modified":1494683576000},{"_id":"themes/next/languages/zh-Hans.yml","hash":"3c0c7dfd0256457ee24df9e9879226c58cb084b5","modified":1494683576000},{"_id":"themes/next/languages/zh-hk.yml","hash":"1c917997413bf566cb79e0975789f3c9c9128ccd","modified":1494683576000},{"_id":"themes/next/languages/zh-tw.yml","hash":"0b2c18aa76570364003c8d1cd429fa158ae89022","modified":1494683576000},{"_id":"source/_posts/pictures/facenet_triplet1.png","hash":"a298c92ba352c4ba11c840b55f72d6c11956d93d","modified":1498654743053},{"_id":"themes/next/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/languages/ja.yml","hash":"1c3a05ab80a6f8be63268b66da6f19da7aa2c638","modified":1494683576000},{"_id":"themes/next/languages/pt-BR.yml","hash":"958e49571818a34fdf4af3232a07a024050f8f4e","modified":1494683576000},{"_id":"themes/next/languages/pt.yml","hash":"36c8f60dacbe5d27d84d0e0d6974d7679f928da0","modified":1494683576000},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1494683576000},{"_id":"themes/next/scripts/tags/button.js","hash":"62e6dbeb53d07627a048132c79630b45d9a8f2cc","modified":1494683576000},{"_id":"themes/next/scripts/tags/exturl.js","hash":"8d7e60f60779bde050d20fd76f6fdc36fc85e06d","modified":1494683576000},{"_id":"themes/next/scripts/tags/full-image.js","hash":"8eeb3fb89540299bdbb799edfdfdac3743b50596","modified":1494683576000},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1494683576000},{"_id":"themes/next/scripts/tags/note.js","hash":"6752925eedbdb939d8ec4d11bdfb75199f18dd70","modified":1494683576000},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1494683576000},{"_id":"themes/next/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1494683576000},{"_id":"themes/next/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1494683576000},{"_id":"themes/next/source/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1494683576000},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1494683576000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1494683576000},{"_id":"themes/next/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1494683576000},{"_id":"themes/next/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1494683576000},{"_id":"themes/next/source/images/images.jpg","hash":"a83aa0596ff8d7ab87cfc47d4a57dd01d6c4ccee","modified":1496232703693},{"_id":"themes/next/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1494683576000},{"_id":"themes/next/layout/_custom/sidebar.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1494683576000},{"_id":"themes/next/layout/_custom/header.swig","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1494683576000},{"_id":"themes/next/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1494683576000},{"_id":"themes/next/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1494683576000},{"_id":"themes/next/source/images/favicon.ico","hash":"3354f46359b13ee21fede9a2f64a81875daae6a5","modified":1496235727301},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1494683576000},{"_id":"themes/next/layout/_partials/footer.swig","hash":"7172c6053118b7c291a56a7860128a652ae66b83","modified":1494683576000},{"_id":"themes/next/layout/_partials/comments.swig","hash":"1c7d3c975e499b9aa3119d6724b030b7b00fc87e","modified":1494683576000},{"_id":"themes/next/layout/_partials/head.swig","hash":"d4a05c51aac02f1f6248baccf2ddb8ee12b9122f","modified":1494683576000},{"_id":"themes/next/layout/_partials/header.swig","hash":"a1ffbb691dfad3eaf2832a11766e58a179003b8b","modified":1494683576000},{"_id":"themes/next/layout/_partials/page-header.swig","hash":"1efd925d34a5d4ba2dc0838d9c86ba911e705fc9","modified":1494683576000},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1494683576000},{"_id":"themes/next/layout/_partials/search.swig","hash":"9dbd378e94abfcb3f864a5b8dbbf18d212ca2ee0","modified":1494683576000},{"_id":"themes/next/layout/_macro/post-copyright.swig","hash":"b16fcbf0efd20c018d7545257a8533c497ea7647","modified":1494683576000},{"_id":"themes/next/layout/_macro/wechat-subscriber.swig","hash":"14e785adeb0e671ba0ff9a553e6f0d8def6c670c","modified":1494683576000},{"_id":"themes/next/layout/_macro/post.swig","hash":"c00261ee0dca8ef7d3f7753e8f8cd444f51118c4","modified":1494683576000},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"31322a7f57936cf2dc62e824af5490da5354cf02","modified":1494683576000},{"_id":"themes/next/layout/_third-party/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1494683576000},{"_id":"themes/next/layout/_macro/reward.swig","hash":"37e5b7c42ec17b9b6b786c5512bcc481a21c974e","modified":1494683576000},{"_id":"themes/next/layout/_third-party/exturl.swig","hash":"7c04a42319d728be356746363aff8ea247791d24","modified":1494683576000},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"911b99ba0445b2c07373128d87a4ef2eb7de341a","modified":1494683576000},{"_id":"themes/next/layout/_third-party/mathjax.swig","hash":"6d25596d6a7c57700d37b607f8d9a62d89708683","modified":1494683576000},{"_id":"themes/next/layout/_third-party/scroll-cookie.swig","hash":"1ddb2336a1a19b47af3017047012c01ec5f54529","modified":1494683576000},{"_id":"themes/next/layout/_third-party/schedule.swig","hash":"22369026c87fc23893c35a7f250b42f3bb1b60f1","modified":1494683576000},{"_id":"themes/next/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1494683576000},{"_id":"themes/next/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1494683576000},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"9baf90f7c40b3b10f288e9268c3191e895890cea","modified":1494683576000},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1494683576000},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1494683576000},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"06f432f328a5b8a9ef0dbd5301b002aba600b4ce","modified":1494683576000},{"_id":"themes/next/source/css/_variables/base.styl","hash":"d6a793bcada68d4b6c58392546bc48a482e4a7d3","modified":1494683576000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/Pisces.styl","hash":"715d5b40dc52f319fe4bff0325beb874774d9bd9","modified":1494683576000},{"_id":"themes/next/source/css/_mixins/base.styl","hash":"78a83c38f69a8747bb74e420e6c9eeef1ea76525","modified":1494683576000},{"_id":"themes/next/source/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1494683576000},{"_id":"themes/next/source/lib/canvas-ribbon/canvas-ribbon.js","hash":"3587602ad777b031628bb5944864d1a4fcfea4ac","modified":1494683576000},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1494683576000},{"_id":"themes/next/source/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1494683576000},{"_id":"themes/next/source/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1494683576000},{"_id":"themes/next/source/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1494683576000},{"_id":"themes/next/source/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1494683576000},{"_id":"themes/next/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1494683576000},{"_id":"themes/next/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1494683576000},{"_id":"themes/next/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1494683576000},{"_id":"themes/next/source/lib/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1494683576000},{"_id":"themes/next/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1494683576000},{"_id":"themes/next/source/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1494683576000},{"_id":"themes/next/source/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1494683576000},{"_id":"themes/next/source/js/src/utils.js","hash":"803f684fa7d0e729115a48851023a31f6fb6d0a7","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/.bower.json","hash":"a2aaaf12378db56bd10596ba3daae30950eac051","modified":1494683576000},{"_id":"themes/next/layout/_partials/head/custom-head.swig","hash":"9e1b9666efa77f4cf8d8261bcfa445a9ac608e53","modified":1494683576000},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"957701729b85fb0c5bfcf2fb99c19d54582f91ed","modified":1494683576000},{"_id":"themes/next/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1494683576000},{"_id":"themes/next/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1494683576000},{"_id":"themes/next/layout/_partials/head/external-fonts.swig","hash":"7ce76358411184482bb0934e70037949dd0da8ca","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/add-this.swig","hash":"23e23dc0f76ef3c631f24c65277adf7ea517b383","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1494683576000},{"_id":"themes/next/source/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/baidushare.swig","hash":"1f1107468aaf03f7d0dcd7eb2b653e2813a675b4","modified":1494683576000},{"_id":"themes/next/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1494683576000},{"_id":"themes/next/source/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1494683576000},{"_id":"themes/next/source/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1494683576000},{"_id":"themes/next/source/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1494683576000},{"_id":"themes/next/layout/_third-party/seo/baidu-push.swig","hash":"c057b17f79e8261680fbae8dc4e81317a127c799","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/gentie.swig","hash":"03592d1d731592103a41ebb87437fe4b0a4c78ca","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"0e3378f7c39b2b0f69638290873ede6b6b6825c0","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"c316758546dc9ba6c60cb4d852c17ca6bb6d6724","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/youyan.swig","hash":"af9dd8a4aed7d06cf47b363eebff48850888566c","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/duoshuo.swig","hash":"a356b2185d40914447fde817eb3d358ab6b3e4c3","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"1d0d01aaeb7bcde3671263d736718f8837c20182","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/hypercomments.swig","hash":"3e8dc5c6c912628a37e3b5f886bec7b2e5ed14ea","modified":1494683576000},{"_id":"themes/next/layout/_third-party/comments/index.swig","hash":"abb92620197a16ed2c0775edf18a0f044a82256e","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/index.swig","hash":"c747fb5c6b1f500e8f0c583e44195878b66e4e29","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"1f349aa30dd1f7022f7d07a1f085eea5ace3f26d","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/cnzz-analytics.swig","hash":"8160b27bee0aa372c7dc7c8476c05bae57f58d0f","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"deda6a814ed48debc694c4e0c466f06c127163d0","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/facebook-sdk.swig","hash":"394d008e5e94575280407ad8a1607a028026cbc3","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/application-insights.swig","hash":"60426bf73f8a89ba61fb1be2df3ad5398e32c4ef","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"5d9943d74cc2e0a91badcf4f755c6de77eab193a","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"3358d11b9a26185a2d36c96049e4340e701646e4","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/lean-analytics.swig","hash":"92dc60821307fc9769bea9b2d60adaeb798342af","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/tencent-mta.swig","hash":"0ddc94ed4ba0c19627765fdf1abc4d8efbe53d5a","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/busuanzi-counter.swig","hash":"18e7bef8923d83ea42df6c97405e515a876cede4","modified":1494683576000},{"_id":"themes/next/layout/_third-party/analytics/vkontakte-api.swig","hash":"c3971fd154d781088e1cc665035f8561a4098f4c","modified":1494683576000},{"_id":"themes/next/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1494683576000},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1494683576000},{"_id":"themes/next/source/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"59ad08bcc6fe9793594869ac2b4c525021453e78","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"ef089a407c90e58eca10c49bc47ec978f96e03ba","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"a6bb5256be6195e76addbda12f4ed7c662d65e7a","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expanded.styl","hash":"fda14bc35be2e1b332809b55b3d07155a833dbf4","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"9a5581a770af8964064fef7afd3e16963e45547f","modified":1494683576000},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"7804e31c44717c9a9ddf0f8482b9b9c1a0f74538","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/helpers.styl","hash":"9c25c75311e1bd4d68df031d3f2ae6d141a90766","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1494683576000},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"64f5d56c08d74a338813df1265580ca0cbf0190b","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_brand.styl","hash":"c4ed249798296f60bda02351fe6404fb3ef2126f","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5b93958239d3d2bf9aeaede44eced2434d784462","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"215de948be49bcf14f06d500cef9f7035e406a43","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"e3e23751d4ad24e8714b425d768cf68e37de7ded","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"69ecd6c97e7cdfd822ac8102b45ad0ede85050db","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"0efa036a15c18f5abb058b7c0fad1dd9ac5eed4c","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"82bbaa6322764779a1ac2e2c8390ce901c7972e2","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"a0e2030a606c934fb2c5c7373aaae04a1caac4c5","modified":1494683576000},{"_id":"themes/next/source/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1494683576000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1494683576000},{"_id":"themes/next/source/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/han.min.css","hash":"d9c0b3dc9158e717fde36f554709e6c3a22b5f85","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/algolia-search/dom.swig","hash":"ba698f49dd3a868c95b240d802f5b1b24ff287e4","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1494683576000},{"_id":"themes/next/source/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1494683576000},{"_id":"themes/next/layout/_third-party/search/algolia-search/assets.swig","hash":"28ff4ed6714c59124569ffcbd10f1173d53ca923","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/han.css","hash":"38e48f275ad00daa9dcdcb8d9b44e576acda4707","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1494683576000},{"_id":"themes/next/source/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/highlight/diff.styl","hash":"96f32ea6c3265a3889e6abe57587f6e2a2a40dfb","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/highlight/highlight.styl","hash":"740d37f428b8f4574a76fc95cc25e50e0565f45e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/highlight/theme.styl","hash":"b76387934fb6bb75212b23c1a194486892cc495e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/menu.styl","hash":"8a2421cb9005352905fae9d41a847ae56957247e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/archive.styl","hash":"7778920dd105fa4de3a7ab206eeba30b1a7bac45","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"2039590632bba3943c39319d80ef630af7928185","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-button.styl","hash":"beccb53dcd658136fb91a0c5678dea8f37d6e0b6","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"a82afbb72d83ee394aedc7b37ac0008a9823b4f4","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-meta.styl","hash":"c089419916988d0f51d89b225460fe11b631e0a3","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"dbc07ec641a537df5918b41ce40a6466712a44f6","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"88c7d75646b66b168213190ee4cd874609afd5e3","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-title.styl","hash":"963105a531403d7aad6d9e5e23e3bfabb8ec065a","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/exturl.styl","hash":"1b3cc9f4e5a7f6e05b4100e9990b37b20d4a2005","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"e792c8dc41561c96d128e9b421187f1c3dc978a0","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"8c0276883398651336853d5ec0e9da267a00dd86","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/full-image.styl","hash":"b8969e1654eec89a0fd10d88b337fee9cb03cd44","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/note.styl","hash":"74d0ba86f698165d13402670382a822c8736a556","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"f54367c0feda6986c030cc4d15a0ca6ceea14bcb","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/tags.styl","hash":"dd310c2d999185e881db007360176ee2f811df10","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"5f6ea57aabfa30a437059bf8352f1ad829dbd4ff","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"a2ec22ef4a6817bbb2abe8660fcd99fe4ca0cc5e","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/algolia-search.styl","hash":"fd42777b9125fd8969dc39d4f15473e2b91b4142","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar-author.styl","hash":"920343e41c124221a17f050bbb989494d44f7a24","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/site-state.styl","hash":"3623e7fa4324ec1307370f33d8f287a9e20a5578","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/han.styl","hash":"cce6772e2cdb4db85d35486ae4c6c59367fbdd40","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"bb3be8374c31c372ed0995bd8030d2b920d581de","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/localsearch.styl","hash":"d89c4b562b528e4746696b2ad8935764d133bdae","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/gentie.styl","hash":"586a3ec0f1015e7207cd6a2474362e068c341744","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Muse/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"a5d8617a24d7cb6c5ad91ea621183ca2c0917331","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1494683576000},{"_id":"themes/next/source/css/_common/components/third-party/busuanzi-counter.styl","hash":"d4e6d8d7b34dc69994593c208f875ae8f7e8a3ae","modified":1494683576000},{"_id":"themes/next/source/css/_schemes/Mist/sidebar/sidebar-blogroll.styl","hash":"817587e46df49e819858c8ecbafa08b53d5ff040","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1494683576000},{"_id":"themes/next/source/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1494683576000},{"_id":"themes/next/source/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1494683576000},{"_id":"themes/next/source/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1494683576000},{"_id":"themes/next/source/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1494683576000},{"_id":"themes/next/source/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1494683576000},{"_id":"public/search.xml","hash":"aaa8e0e9f6fb456f775d24e918caf2610c9c7904","modified":1517828864075},{"_id":"public/atom.xml","hash":"06cccf19e578006d1909fd08046693fc92c9f99c","modified":1517828864097},{"_id":"public/sitemap.xml","hash":"3efda178efc6c74d7b0277bfff2a0ad842b687b3","modified":1517828864098},{"_id":"public/about/index.html","hash":"8b81a081c9501d4233fcf3ca6ef38c4c8ac71606","modified":1517828864759},{"_id":"public/tags/index.html","hash":"f23ccd4035cd738e0f9d5b137faac75ae2554eaf","modified":1517828864763},{"_id":"public/categories/index.html","hash":"4f64a0b8da292e7f18eaad74c8f6c821e5e52f1c","modified":1517828864763},{"_id":"public/2017/07/17/文档归类/index.html","hash":"f5997c633ea180d6e5af62054d273f11b8e36411","modified":1517828864764},{"_id":"public/archives/2017/07/index.html","hash":"d5bd4f7df33c49121edd758ef8aa90f558fc81c3","modified":1517828864764},{"_id":"public/categories/论文笔记/index.html","hash":"34dcf5397ffcdc7b1b0c9f9a916076da9e7efe20","modified":1517828864764},{"_id":"public/categories/深度学习/index.html","hash":"4e605421a103e8a4a130b23cf227a5d35383dc9a","modified":1517828864764},{"_id":"public/categories/Hexo/index.html","hash":"e6609b8952f3e990524ed704cd61a1d0224967e4","modified":1517828864764},{"_id":"public/tags/Face/index.html","hash":"880504ac351622b9ea4b80d3de9a2de55cd6e31a","modified":1517828864764},{"_id":"public/tags/笔记/index.html","hash":"a7d5243b89c712a9a570b6d7c11b6a2f5a0b612e","modified":1517828864764},{"_id":"public/tags/caffe/index.html","hash":"337bc6910425bef0be753c4b0aa5c257f921d4f3","modified":1517828864764},{"_id":"public/tags/笔记，人脸识别/index.html","hash":"1cec7e70c5b3d305a5cf8b129cb369518fe5fb70","modified":1517828864764},{"_id":"public/tags/Python/index.html","hash":"aeae6bf04f1684376bc4cbfa0c86493a0738c420","modified":1517828864766},{"_id":"public/tags/深度学习/index.html","hash":"38a2ae910bf494766b6f0d7a9a456a8c5db19268","modified":1517828864767},{"_id":"public/tags/调参/index.html","hash":"64c89821550911697a17ca97c656ff9c875d0f11","modified":1517828864768},{"_id":"public/tags/神经网络/index.html","hash":"a4e08ce3334b33aa5e7eb5427af8dd39bfd90684","modified":1517828864768},{"_id":"public/tags/深度学习，论文笔记/index.html","hash":"c531bd0e21f5360e8de716d1fe016fb55411cb95","modified":1517828864768},{"_id":"public/tags/Hexo/index.html","hash":"c9171454b975ed7b9c51f3b695f6ec6c62eee0b9","modified":1517828864768},{"_id":"public/tags/深度学习，人脸识别/index.html","hash":"0c95938f1cc083daa61d4432afca4f1032785ab0","modified":1517828864768},{"_id":"public/categories/Face/index.html","hash":"2b173aef678b5be76f8652d51bb4c1962a981aeb","modified":1517828864768},{"_id":"public/2017/06/29/Center-loss笔记/index.html","hash":"93c579ec0422290f555890d63a0927a9d325d9d8","modified":1517828864768},{"_id":"public/2017/06/28/FaceNet论文笔记/index.html","hash":"c14b291bbe4c7d82caf4d3088b7a1884a515a05b","modified":1517828864768},{"_id":"public/2017/06/17/Caffe调参/index.html","hash":"9c004549cfc829976f3ffc9bfceb482a6c257725","modified":1517828864768},{"_id":"public/2017/06/02/Triplet loss/index.html","hash":"c90175e4089ef57ff90cbeb3fc6a3b32feecb5bb","modified":1517828864769},{"_id":"public/2017/06/09/Python/index.html","hash":"ca99c1e89e04b7af9b0f1f27783189e54b669506","modified":1517828864769},{"_id":"public/2017/06/02/Hexo相关/index.html","hash":"7a7959ad42a238c3e0902960a11da0260caca50e","modified":1517828864769},{"_id":"public/2017/06/02/待办及进度/index.html","hash":"f6782b4f4099c9ab9915bbb0272dc634e595d704","modified":1517828864769},{"_id":"public/archives/index.html","hash":"0e87b7737e7ce2a4db08e55305f53a1bc39b2783","modified":1517828864769},{"_id":"public/2017/06/01/杂知识点/index.html","hash":"f1af2d3eeb2ed79edb4588258c7b306cab2a6a5a","modified":1517828864769},{"_id":"public/archives/2017/index.html","hash":"ce739383517c7c1296551153a5bc16d6c7b3e595","modified":1517828864772},{"_id":"public/index.html","hash":"621f3b57ead41fd7d96178f1ad0d385878e75fb6","modified":1517828864772},{"_id":"public/archives/2017/06/index.html","hash":"61ee494863e2875253f3933681cad781edfda20e","modified":1517828864772},{"_id":"public/2017/07/04/DeepID3论文笔记/index.html","hash":"061178a447aad2d9fd2e858dba528034e3fa4689","modified":1517828864772},{"_id":"public/2017/09/21/行人检测/index.html","hash":"a96f7dc28831cd6176a3d850a3273b7d93d967b0","modified":1517828864808},{"_id":"public/archives/page/2/index.html","hash":"2c8aede019782fda7452c2a6d515b31e642dccbb","modified":1517828864808},{"_id":"public/archives/2017/page/2/index.html","hash":"b4a5c28331b28cec8d6edd12bd15b18ddea15303","modified":1517828864808},{"_id":"public/archives/2018/index.html","hash":"28a9c95df9f6d115b9a312917777651257c9e35d","modified":1517828864808},{"_id":"public/archives/2017/09/index.html","hash":"cc8650a6a64a2fb08540b89e67eeadd87db98c2e","modified":1517828864808},{"_id":"public/archives/2018/01/index.html","hash":"c185be1b936f976c30cf0c201de26bba6fb0f957","modified":1517828864809},{"_id":"public/tags/人脸识别/index.html","hash":"b05098a46dcb88ba0dd0817db0131fa6faf82d55","modified":1517828864809},{"_id":"public/2018/01/29/人脸识别回顾/index.html","hash":"f194f5a8e2301a4f9fc6e944aa5dd038c5769883","modified":1517828864809},{"_id":"public/page/2/index.html","hash":"88d505ccfe7bce44798905a1fb83a46bdddb9723","modified":1517828864809},{"_id":"public/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1517828864809},{"_id":"public/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1517828864809},{"_id":"public/images/avatar.gif","hash":"264082bb3a1af70d5499c7d22b0902cb454b6d12","modified":1517828864809},{"_id":"public/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1517828864809},{"_id":"public/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1517828864809},{"_id":"public/images/images.jpg","hash":"a83aa0596ff8d7ab87cfc47d4a57dd01d6c4ccee","modified":1517828864810},{"_id":"public/images/algolia_logo.svg","hash":"90035272fa31a3f65b3c0e2cb8a633876ef457dc","modified":1517828864810},{"_id":"public/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1517828864810},{"_id":"public/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1517828864810},{"_id":"public/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1517828864810},{"_id":"public/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1517828864810},{"_id":"public/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1517828864810},{"_id":"public/images/favicon.ico","hash":"3354f46359b13ee21fede9a2f64a81875daae6a5","modified":1517828864810},{"_id":"public/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1517828864810},{"_id":"public/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1517828864810},{"_id":"public/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1517828864810},{"_id":"public/lib/font-awesome/HELP-US-OUT.txt","hash":"4f7bf961f1bed448f6ba99aeb9219fabf930ba96","modified":1517828864811},{"_id":"public/lib/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1517828864811},{"_id":"public/lib/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1517828864811},{"_id":"public/lib/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1517828864811},{"_id":"public/lib/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1517828864811},{"_id":"public/lib/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1517828864811},{"_id":"public/lib/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1517828864811},{"_id":"public/lib/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1517828864811},{"_id":"public/lib/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1517828864811},{"_id":"public/lib/Han/dist/font/han-space.woff","hash":"7a635062b10bf5662ae1d218ba0980171005d060","modified":1517828864811},{"_id":"public/lib/Han/dist/font/han.otf","hash":"f1f6bb8f461f5672e000380195d3d2358a28494c","modified":1517828864811},{"_id":"public/lib/Han/dist/font/han-space.otf","hash":"07436f011b44051f61b8329c99de4bec64e86f4b","modified":1517828864812},{"_id":"public/lib/Han/dist/font/han.woff","hash":"f38ff9b2eecaa17b50b66aa2dae87e9e7436d195","modified":1517828864812},{"_id":"public/lib/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1517828864812},{"_id":"public/lib/algolia-instant-search/instantsearch.min.css","hash":"90ef19edc982645b118b095615838d9c5eaba0de","modified":1517828869976},{"_id":"public/lib/canvas-nest/canvas-nest.min.js","hash":"0387e75e23b1db108a755073fe52a0d03eb391a7","modified":1517828869977},{"_id":"public/js/src/algolia-search.js","hash":"b172f697ed339a24b1e80261075232978d164c35","modified":1517828869995},{"_id":"public/lib/canvas-ribbon/canvas-ribbon.js","hash":"3587602ad777b031628bb5944864d1a4fcfea4ac","modified":1517828869995},{"_id":"public/js/src/bootstrap.js","hash":"aab7be0a6e2724b3faa9338db93c19556c559625","modified":1517828869995},{"_id":"public/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1517828869996},{"_id":"public/js/src/exturl.js","hash":"e42e2aaab7bf4c19a0c8e779140e079c6aa5c0b1","modified":1517828869996},{"_id":"public/js/src/js.cookie.js","hash":"9b37973a90fd50e71ea91682265715e45ae82c75","modified":1517828869996},{"_id":"public/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1517828869996},{"_id":"public/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1517828869997},{"_id":"public/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1517828869997},{"_id":"public/js/src/utils.js","hash":"803f684fa7d0e729115a48851023a31f6fb6d0a7","modified":1517828869997},{"_id":"public/js/src/post-details.js","hash":"af7a417dd1cb02465a7b98211653e7c6192e6d55","modified":1517828869998},{"_id":"public/js/src/scroll-cookie.js","hash":"09dc828cbf5f31158ff6250d2bf7c3cde6365c67","modified":1517828869998},{"_id":"public/lib/jquery_lazyload/bower.json","hash":"ae3c3b61e6e7f9e1d7e3585ad854380ecc04cf53","modified":1517828869999},{"_id":"public/lib/font-awesome/bower.json","hash":"64394a2a9aa00f8e321d8daa5e51a420f0e96dad","modified":1517828869999},{"_id":"public/lib/jquery_lazyload/README.html","hash":"bde24335f6bc09d8801c0dcd7274f71b466552bd","modified":1517828869999},{"_id":"public/lib/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1517828869999},{"_id":"public/lib/jquery_lazyload/CONTRIBUTING.html","hash":"a6358170d346af13b1452ac157b60505bec7015c","modified":1517828870000},{"_id":"public/lib/velocity/bower.json","hash":"0ef14e7ccdfba5db6eb3f8fc6aa3b47282c36409","modified":1517828870000},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1517828870000},{"_id":"public/lib/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1517828870000},{"_id":"public/lib/fastclick/README.html","hash":"da3c74d484c73cc7df565e8abbfa4d6a5a18d4da","modified":1517828870001},{"_id":"public/lib/fastclick/bower.json","hash":"4dcecf83afddba148464d5339c93f6d0aa9f42e9","modified":1517828870001},{"_id":"public/lib/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1517828870001},{"_id":"public/lib/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1517828870002},{"_id":"public/js/src/schemes/pisces.js","hash":"79da92119bc246fe05d1626ac98426a83ec90a94","modified":1517828870002},{"_id":"public/lib/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1517828870003},{"_id":"public/lib/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1517828870003},{"_id":"public/css/main.css","hash":"6bb04e3649488005fce50736ac8618013c2fd861","modified":1517828870004},{"_id":"public/lib/three/canvas_lines.min.js","hash":"dce4a3b65f8bf958f973690caa7ec4952f353b0c","modified":1517828870004},{"_id":"public/lib/three/canvas_sphere.min.js","hash":"d8ea241a53c135a650f7335d2b6982b899fd58a9","modified":1517828870004},{"_id":"public/lib/three/three-waves.min.js","hash":"d968cba6b3a50b3626a02d67b544f349d83b147c","modified":1517828870004},{"_id":"public/lib/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1517828870005},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1517828870005},{"_id":"public/lib/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1517828870005},{"_id":"public/lib/Han/dist/han.min.css","hash":"d9c0b3dc9158e717fde36f554709e6c3a22b5f85","modified":1517828870005},{"_id":"public/lib/Han/dist/han.min.js","hash":"f559c68a25065a14f47da954a7617d87263e409d","modified":1517828870005},{"_id":"public/lib/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1517828870006},{"_id":"public/lib/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1517828870006},{"_id":"public/lib/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1517828870006},{"_id":"public/lib/Han/dist/han.css","hash":"38e48f275ad00daa9dcdcb8d9b44e576acda4707","modified":1517828870006},{"_id":"public/lib/Han/dist/han.js","hash":"e345397e0585c9fed1449e614ec13e0224acf2ab","modified":1517828870007},{"_id":"public/lib/font-awesome/css/font-awesome.min.css","hash":"512c7d79033e3028a9be61b540cf1a6870c896f8","modified":1517828870007},{"_id":"public/lib/velocity/velocity.js","hash":"9f08181baea0cc0e906703b7e5df9111b9ef3373","modified":1517828870007},{"_id":"public/lib/font-awesome/css/font-awesome.css","hash":"0140952c64e3f2b74ef64e050f2fe86eab6624c8","modified":1517828870008},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff2","hash":"d6f48cba7d076fb6f2fd6ba993a75b9dc1ecbf0c","modified":1517828870008},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.woff","hash":"28b782240b3e76db824e12c02754a9731a167527","modified":1517828870008},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1517828870093},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1517828870093},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1517828870093},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1517828870094},{"_id":"public/lib/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1517828870094},{"_id":"public/lib/algolia-instant-search/instantsearch.min.js","hash":"9ccc6f8144f54e86df9a3fd33a18368d81cf3a4f","modified":1517828870094},{"_id":"public/lib/three/three.min.js","hash":"73f4cdc17e51a72b9bf5b9291f65386d615c483b","modified":1517828870094},{"_id":"public/lib/font-awesome/fonts/FontAwesome.otf","hash":"048707bc52ac4b6563aaa383bfe8660a0ddc908c","modified":1517828870095},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.ttf","hash":"13b1eab65a983c7a73bc7997c479d66943f7c6cb","modified":1517828870095},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.eot","hash":"d980c2ce873dc43af460d4d572d441304499f400","modified":1517828870096},{"_id":"public/lib/font-awesome/fonts/fontawesome-webfont.svg","hash":"98a8aa5cf7d62c2eff5f07ede8d844b874ef06ed","modified":1517828870424}],"Category":[{"name":"论文笔记","_id":"cjda440ma00022sc8350gp7qr"},{"name":"深度学习","_id":"cjda440na00052sc8q9alqrfr"},{"name":"Hexo","_id":"cjda440sw000n2sc8bvjmsgja"},{"name":"Face","_id":"cjda440td000s2sc83di72d8a"}],"Data":[],"Page":[{"title":"about","date":"2017-05-31T12:22:54.000Z","_content":"\n本来无一物\n\nFrom Calyp\n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2017-05-31 20:22:54\n---\n\n本来无一物\n\nFrom Calyp\n","updated":"2017-05-31T12:24:06.892Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjda440s2000k2sc8u31o87m2","content":"<p>本来无一物</p>\n<p>From Calyp</p>\n","site":{"data":{}},"excerpt":"","more":"<p>本来无一物</p>\n<p>From Calyp</p>\n"},{"title":"tags","date":"2017-05-31T12:19:32.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2017-05-31 20:19:32\ntype: \"tags\"\n---\n","updated":"2017-05-31T12:20:34.309Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cjda440sn000m2sc8u0as35kz","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2017-05-31T12:21:40.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2017-05-31 20:21:40\ntype: \"categories\"\n---\n","updated":"2017-05-31T12:21:57.600Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cjda440xg001f2sc8x2tnvrqc","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"DeepID3论文笔记","date":"2017-07-04T02:34:31.000Z","description":null,"mathjax":null,"_content":"\n# 摘要\n\n深层的网络由于良好的学习能力，近年来在目标识别领域取得巨大成功，基于此，提出两个非常深的网络架构，DeepID3，这两个架构分别基于VGG net和GoogLeNet通过改进使之用于人脸识别。Joint face identification-verification supervisory signals are added to both intermediate and\nfinal feature extraction layers during training.        \n\n\n\n# 介绍\n\n\n\n","source":"_posts/DeepID3论文笔记.md","raw":"---\ntitle: DeepID3论文笔记\ndate: 2017-07-04 10:34:31\ncategories: 论文笔记\ntags: [Face,笔记]\ndescription:\nmathjax:\n---\n\n# 摘要\n\n深层的网络由于良好的学习能力，近年来在目标识别领域取得巨大成功，基于此，提出两个非常深的网络架构，DeepID3，这两个架构分别基于VGG net和GoogLeNet通过改进使之用于人脸识别。Joint face identification-verification supervisory signals are added to both intermediate and\nfinal feature extraction layers during training.        \n\n\n\n# 介绍\n\n\n\n","slug":"DeepID3论文笔记","published":1,"updated":"2017-07-04T06:33:10.490Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440la00002sc8gjwoy16z","content":"<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>深层的网络由于良好的学习能力，近年来在目标识别领域取得巨大成功，基于此，提出两个非常深的网络架构，DeepID3，这两个架构分别基于VGG net和GoogLeNet通过改进使之用于人脸识别。Joint face identification-verification supervisory signals are added to both intermediate and<br>final feature extraction layers during training.        </p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1>","site":{"data":{}},"excerpt":"","more":"<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>深层的网络由于良好的学习能力，近年来在目标识别领域取得巨大成功，基于此，提出两个非常深的网络架构，DeepID3，这两个架构分别基于VGG net和GoogLeNet通过改进使之用于人脸识别。Joint face identification-verification supervisory signals are added to both intermediate and<br>final feature extraction layers during training.        </p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1>"},{"title":"Caffe调参","date":"2017-06-17T01:26:12.000Z","description":null,"mathjax":null,"_content":"\n# loss为nan\n\n**梯度爆炸**\n\n**原因**：梯度变得非常大，使得学习过程难以继续\n\n**现象：**观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。\n\n**措施**： \n\n1. 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 \n2. 设置clip gradient，用于限制过大的diff\n\n## \n\n**不当的损失函数**\n\n**原因**：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。\n\n**现象**：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。\n\n**措施**：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。\n\n示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。\n\n## \n\n**不当的输入**\n\n**原因**：输入中就含有NaN。\n\n**现象**：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。\n\n**措施**：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。\n\n**案例**：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。\n\n**池化层中步长比核的尺寸大**\n\n如下例所示，当池化层中stride > kernel的时候会在y中产生NaN\n\n```\n    layer {\n      name: \"faulty_pooling\"\n      type: \"Pooling\"\n      bottom: \"x\"\n      top: \"y\"\n      pooling_param {\n      pool: AVE\n      stride: 5\n      kernel: 3\n      }\n    }\n```\n\n**致谢**\n\n*http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training*\n\n\n\n# Accuracy一直为0\n\n考虑标签是否从0开始递增\n\n","source":"_posts/Caffe调参.md","raw":"---\ntitle: Caffe调参\ndate: 2017-06-17 09:26:12\ncategories: 深度学习\ntags: [caffe,调参]\ndescription:\nmathjax:\n---\n\n# loss为nan\n\n**梯度爆炸**\n\n**原因**：梯度变得非常大，使得学习过程难以继续\n\n**现象：**观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。\n\n**措施**： \n\n1. 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 \n2. 设置clip gradient，用于限制过大的diff\n\n## \n\n**不当的损失函数**\n\n**原因**：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。\n\n**现象**：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。\n\n**措施**：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。\n\n示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。\n\n## \n\n**不当的输入**\n\n**原因**：输入中就含有NaN。\n\n**现象**：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。\n\n**措施**：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。\n\n**案例**：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。\n\n**池化层中步长比核的尺寸大**\n\n如下例所示，当池化层中stride > kernel的时候会在y中产生NaN\n\n```\n    layer {\n      name: \"faulty_pooling\"\n      type: \"Pooling\"\n      bottom: \"x\"\n      top: \"y\"\n      pooling_param {\n      pool: AVE\n      stride: 5\n      kernel: 3\n      }\n    }\n```\n\n**致谢**\n\n*http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training*\n\n\n\n# Accuracy一直为0\n\n考虑标签是否从0开始递增\n\n","slug":"Caffe调参","published":1,"updated":"2017-07-27T01:53:11.297Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440m000012sc8gjqgkrtl","content":"<h1 id=\"loss为nan\"><a href=\"#loss为nan\" class=\"headerlink\" title=\"loss为nan\"></a>loss为nan</h1><p><strong>梯度爆炸</strong></p>\n<p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续</p>\n<p><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。</p>\n<p><strong>措施</strong>： </p>\n<ol>\n<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>\n<li>设置clip gradient，用于限制过大的diff</li>\n</ol>\n<p>## </p>\n<p><strong>不当的损失函数</strong></p>\n<p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p>\n<p><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>\n<p><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。</p>\n<p>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>\n<p>## </p>\n<p><strong>不当的输入</strong></p>\n<p><strong>原因</strong>：输入中就含有NaN。</p>\n<p><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>\n<p><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。</p>\n<p><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>\n<p><strong>池化层中步长比核的尺寸大</strong></p>\n<p>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">  name: &quot;faulty_pooling&quot;</div><div class=\"line\">  type: &quot;Pooling&quot;</div><div class=\"line\">  bottom: &quot;x&quot;</div><div class=\"line\">  top: &quot;y&quot;</div><div class=\"line\">  pooling_param &#123;</div><div class=\"line\">  pool: AVE</div><div class=\"line\">  stride: 5</div><div class=\"line\">  kernel: 3</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p><strong>致谢</strong></p>\n<p><em><a href=\"http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training</a></em></p>\n<h1 id=\"Accuracy一直为0\"><a href=\"#Accuracy一直为0\" class=\"headerlink\" title=\"Accuracy一直为0\"></a>Accuracy一直为0</h1><p>考虑标签是否从0开始递增</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"loss为nan\"><a href=\"#loss为nan\" class=\"headerlink\" title=\"loss为nan\"></a>loss为nan</h1><p><strong>梯度爆炸</strong></p>\n<p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续</p>\n<p><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。</p>\n<p><strong>措施</strong>： </p>\n<ol>\n<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>\n<li>设置clip gradient，用于限制过大的diff</li>\n</ol>\n<p>## </p>\n<p><strong>不当的损失函数</strong></p>\n<p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p>\n<p><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>\n<p><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。</p>\n<p>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>\n<p>## </p>\n<p><strong>不当的输入</strong></p>\n<p><strong>原因</strong>：输入中就含有NaN。</p>\n<p><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>\n<p><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。</p>\n<p><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>\n<p><strong>池化层中步长比核的尺寸大</strong></p>\n<p>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">  name: &quot;faulty_pooling&quot;</div><div class=\"line\">  type: &quot;Pooling&quot;</div><div class=\"line\">  bottom: &quot;x&quot;</div><div class=\"line\">  top: &quot;y&quot;</div><div class=\"line\">  pooling_param &#123;</div><div class=\"line\">  pool: AVE</div><div class=\"line\">  stride: 5</div><div class=\"line\">  kernel: 3</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p><strong>致谢</strong></p>\n<p><em><a href=\"http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training\" target=\"_blank\" rel=\"external\">http://stackoverflow.com/questions/33962226/common-causes-of-NaNs-during-training</a></em></p>\n<h1 id=\"Accuracy一直为0\"><a href=\"#Accuracy一直为0\" class=\"headerlink\" title=\"Accuracy一直为0\"></a>Accuracy一直为0</h1><p>考虑标签是否从0开始递增</p>\n"},{"title":"人脸识别回顾","date":"2018-01-29T01:47:23.000Z","description":null,"mathjax":null,"_content":"\n# 数据准备\n\n**数据清洗**：去除错误标签，图片质量不好的样本，如果有benchmark，还需要去除和benchmark重复的样本。\n\n**构造验证集**:  对于给定的数据集，可能没有划分train set和validation set ，需要手动从给定的训练集中按照一定的比例分离出验证集（比如9:1）\n\n**数据均衡**：数据集可能存在类别不均衡的问题，可以通过重新组合不均衡数据, 使之均衡，方式一: 复制或者合成（比如jitter操作）少数部分的样本, 使之和多数部分差不多数量； 方式二: 砍掉一些多数部分, 使两者数量差不多\n\n**数据扩充**：对于一些样本数据比较少的数据集，为了更好的训练网络，有时候需要人为增加一些训练样本，比如随机的剪裁、缩放和旋转等。\n\n**预处理**：常见的就是减均值、除方差。\n\n\n\n# 训练模型\n\n####1. 模型选择：\n\n根据具体任务和数据集规模选择合适的网络结构，对于分类任务来说，如果数据集的规模不大，则网络的层数不应太深，结构也不应太复杂。\n\n####2. 激励函数的选择\n\n+ **sigmoid函数**：取值范围为(0,1)，可以将一个实数映射到(0,1)的区间，可以用来做二分类，在特征相差比较复杂或是相差不是特别大时效果比较好，缺点是：激活函数计算量大，反向传播求梯度时，求导涉及除法，反向传播时，很容易出现梯度消失的情况\n\n+ **Tanh函数**：取值范围为[-1,1]，在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果，与sigmoid的区别是,tanh是0均值的，因此实际应用中，tanh会比sigmoid更好。\n\n+ **ReLU函数**：使用ReLU得到的SGD的收敛速度会比sigmoid/tanh快很多，缺点是训练的时候很”脆弱”，很容易就”die”了例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。\n\n  选择的时候，就是根据各个函数的优缺点来配置，例如：\n\n  如果使用 ReLU，要小心设置 learning rate，注意不要让网络出现很多 “dead” 神经元，如果不好解决，可以试试 Leaky ReLU、PReLU 或者 Maxout.\n\n  **详细细节请参看**：[常用激活函数比较](https://www.jianshu.com/p/22d9720dbf1a)\n\n  ​\n\n####3. 卷积tricks\n\n图片输入是2的幂次方，例如32、64、96、224等。\n\n卷积核大小是3*3或者5*5。\n\n输入图片上下左右需要用0补充，即padding，且假如卷积核大小是5，那么padding就是2（图片左右上下都补充2）；卷积核大小是3，padding大小就是1。\n\n####4. pooling层tricks\n\npoolin层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。max pooling比avg pooling效果会好一些。avg-global pooling进入全卷积时代。\n\n#### 5. Loss函数的选择\n\n+ **softmax loss**\n\n+ contrastive loss\n\n+ triplet loss\n\n+ **center loss**\n\n  **triplet loss比softmax的优势**\n\n  - 在于softmax不直接，（三元组直接优化距离），因而性能也不好。\n  - softmax产生的特征表示向量都很大，一般超过1000维。\n  - 利用triplet loss的metric learning目的在于减小类内的L2距离，同时增大类间的距离\n\n  **center loss VS triplet loss**\n\n  - triplet loss:dramatic data expansion\n  - center loss:more directly and efficiently\n\n\n\n\n\n\n\n# 神经网络的设计模式\n\n1. **Architectural Structure follows the Application**（架构遵循应用）\n\n   应该根据自己的应用场景选择合适的网络架构。\n\n2. **Proliferate Paths**（路径扩增）\n\n   每年ImageNet Challenge的赢家都比上一年的冠军使用更加深层的网络。从AlexNet 到Inception到Resnets，Smith和他的团队也观察到“网络的路径数量成倍增长”的趋势，ResNet可以是不同长度的网络的指数集合。\n\n3. **Strive for Simplicity**（简洁原则）\n\n   更大的并不一定是更好的。在名为“Bigger is not necessarily better”的论文中，Springenberg 等人演示了如何用更少的单元实现最先进的结果。\n\n4. **Increase Symmetry** （增加对称性）\n\n   无论是在建筑上，还是在生物上，对称性被认为是质量和工艺的标志。Smith 将 FractalNet 的优雅归功于网络的对称性。\n\n5. **Pyramid Shape** （金字塔型）\n\n   在表征能力和减少冗余或者无用信息之间权衡。CNNs通常会降低激活函数的采样，并会增加从输入层到最终层之间的连接通道。\n\n6. **Over-train** （过训练）\n\n   另一个权衡是训练准确率和泛化能力。用正则化的方法类似 drop-out 或 drop-path进行提升泛化能力，这是神经网络的重要优势。用比实际用例更难的问题训练你的网络，以提高泛化性能。\n\n7. **Cover the Problem Space** （覆盖问题空间）\n\n   为了扩大你的训练数据和提升泛化能力，要使用噪声和人工增加训练集的大小，例如随机旋转、裁剪和一些图像操作。\n\n8. **Incremental Feature Construction** （递增的功能结构）\n\n   当架构变得成功时，它们会简化每一层的“工作”。在非常深的神经网络中，每个 层只会递增地修改输入。在ResNets中，每一层的输出可能类似于输入。所以，在实践中，请在ResNet中使用短的跳过长度。\n\n9. **Normalize Layer Inputs** （规范化层输入）\n\n   标准化是可以使计算层的工作变得更加容易的一条捷径，并且在实际中可以提升训练的准确性。批量标准化的发明者认为标准化发挥作用的原因在于处理内部的协变量，但Smith 认为，“标准化把所有层的输入样本放在了一个平等的基础上（类似于单位转换），这允许反向传播可以更有效地训练”。\n\n10. **Input Transition** （输入转换）\n\n  研究表明，在Wide ResNets中,性能随着通道数量的增加而提升，但是要权衡训练成本与准确性。AlexNet，VGG，Inception和ResNets都是在第一层中进行输入变换，以保证多种方式检查输入数据。\n\n11. **Available Resources Guide Layer Widths** （可用资源决定层宽度）\n\n    可供选择的输出数量并不明显，相应的是，这取决于您的硬件功能和所需的准确性。\n\n12. **Summation Joining** （求和连接）\n\n    Summation是一种流行的合并分支的方式。在 ResNets 中，使用求和作为连接机制可以让每一个分支都能计算残差和整体近似。如果输入跳跃连接始终存在，那么Summation会让每一层学到正确地东西（例如：输入的差别）。在任何分支都可以被丢弃的网络（例如 FractalNet）中，你应该使用这种方式保持输出的平滑。\n\n13. **Down-sampling Transition** （下采样变换）\n\n    在汇聚的时候，利用级联连接来增加输出的数量。当使用大于1的步幅时，这会同时处理加入并增加通道的数量。\n\n14. **Maxout for Competition ** （用于竞争的Maxout）\n\n    Maxout 用在只需要选择一个激活函数的局部竞争网络中。用的方法包含所有的激活函数，不同之处在于 maxout 只选择一个“胜出者”。Maxout 的一个明显的用例是每个分支具有不同大小的内核，而 Maxout 可以尺度不变。\n\n\n\n\n\n\n\n# Caffe调参\n\n###1. loss为nan(83.3365)  [标签错误、学习率太大]\n\n**梯度爆炸**\n\n**原因**：梯度变得非常大，使得学习过程难以继续\n\n**现象：**观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。\n\n**措施**： \n\n1. 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 \n2. 设置clip gradient，用于限制过大的diff\n\n\n\n**不当的损失函数**\n\n**原因**：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。\n\n**现象**：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。\n\n**措施**：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。\n\n示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。\n\n\n\n**不当的输入**\n\n**原因**：输入中就含有NaN。\n\n**现象**：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。\n\n**措施**：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。\n\n**案例**：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。\n\n**池化层中步长比核的尺寸大**\n\n如下例所示，当池化层中stride > kernel的时候会在y中产生NaN\n\n```\n    layer {\n      name: \"faulty_pooling\"\n      type: \"Pooling\"\n      bottom: \"x\"\n      top: \"y\"\n      pooling_param {\n      pool: AVE\n      stride: 5\n      kernel: 3\n      }\n    }\n```\n\n###2. 避免overfitting\n\n- simpler model structure\n\n- regularization\n\n- data augmentation\n\n- dropall(dropout+drop-path)\n\n- Bootstrap/Bagging\n\n- ensemble\n\n- early stopping\n\n- utilize invariance\n\n- Bayesian\n\n  ​\n\n\n\n# 人脸识别流程\n\n**一般流程**:数据准备->人脸检测->人脸对齐->生成数据文件->训练模型->调参->model\n\n**tricks**:\n\n+ 在数据准备阶段，可以采用jitter等操作对样本数量较少的种类进行扩充，弥补类别不均衡造成的影响\n+ 在生成模型阶段，可以采用ensemble的方式对多个模型进行融合，比如mirror。\n\n\n\n**参考及致谢**\n\n[Common causes of nans during training](https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training)\n\n[常用激活函数比较](https://www.jianshu.com/p/22d9720dbf1a)\n\n[深度学习之五常见tricks](https://chenrudan.github.io/blog/2015/08/04/dl5tricks.html)\n\n《Deep convolutional neural network design patterns 》\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/人脸识别回顾.md","raw":"---\ntitle: 人脸识别回顾\ndate: 2018-01-29 09:47:23\ncategories: 深度学习\ntags: [人脸识别]\ndescription:\nmathjax:\n---\n\n# 数据准备\n\n**数据清洗**：去除错误标签，图片质量不好的样本，如果有benchmark，还需要去除和benchmark重复的样本。\n\n**构造验证集**:  对于给定的数据集，可能没有划分train set和validation set ，需要手动从给定的训练集中按照一定的比例分离出验证集（比如9:1）\n\n**数据均衡**：数据集可能存在类别不均衡的问题，可以通过重新组合不均衡数据, 使之均衡，方式一: 复制或者合成（比如jitter操作）少数部分的样本, 使之和多数部分差不多数量； 方式二: 砍掉一些多数部分, 使两者数量差不多\n\n**数据扩充**：对于一些样本数据比较少的数据集，为了更好的训练网络，有时候需要人为增加一些训练样本，比如随机的剪裁、缩放和旋转等。\n\n**预处理**：常见的就是减均值、除方差。\n\n\n\n# 训练模型\n\n####1. 模型选择：\n\n根据具体任务和数据集规模选择合适的网络结构，对于分类任务来说，如果数据集的规模不大，则网络的层数不应太深，结构也不应太复杂。\n\n####2. 激励函数的选择\n\n+ **sigmoid函数**：取值范围为(0,1)，可以将一个实数映射到(0,1)的区间，可以用来做二分类，在特征相差比较复杂或是相差不是特别大时效果比较好，缺点是：激活函数计算量大，反向传播求梯度时，求导涉及除法，反向传播时，很容易出现梯度消失的情况\n\n+ **Tanh函数**：取值范围为[-1,1]，在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果，与sigmoid的区别是,tanh是0均值的，因此实际应用中，tanh会比sigmoid更好。\n\n+ **ReLU函数**：使用ReLU得到的SGD的收敛速度会比sigmoid/tanh快很多，缺点是训练的时候很”脆弱”，很容易就”die”了例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。\n\n  选择的时候，就是根据各个函数的优缺点来配置，例如：\n\n  如果使用 ReLU，要小心设置 learning rate，注意不要让网络出现很多 “dead” 神经元，如果不好解决，可以试试 Leaky ReLU、PReLU 或者 Maxout.\n\n  **详细细节请参看**：[常用激活函数比较](https://www.jianshu.com/p/22d9720dbf1a)\n\n  ​\n\n####3. 卷积tricks\n\n图片输入是2的幂次方，例如32、64、96、224等。\n\n卷积核大小是3*3或者5*5。\n\n输入图片上下左右需要用0补充，即padding，且假如卷积核大小是5，那么padding就是2（图片左右上下都补充2）；卷积核大小是3，padding大小就是1。\n\n####4. pooling层tricks\n\npoolin层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。max pooling比avg pooling效果会好一些。avg-global pooling进入全卷积时代。\n\n#### 5. Loss函数的选择\n\n+ **softmax loss**\n\n+ contrastive loss\n\n+ triplet loss\n\n+ **center loss**\n\n  **triplet loss比softmax的优势**\n\n  - 在于softmax不直接，（三元组直接优化距离），因而性能也不好。\n  - softmax产生的特征表示向量都很大，一般超过1000维。\n  - 利用triplet loss的metric learning目的在于减小类内的L2距离，同时增大类间的距离\n\n  **center loss VS triplet loss**\n\n  - triplet loss:dramatic data expansion\n  - center loss:more directly and efficiently\n\n\n\n\n\n\n\n# 神经网络的设计模式\n\n1. **Architectural Structure follows the Application**（架构遵循应用）\n\n   应该根据自己的应用场景选择合适的网络架构。\n\n2. **Proliferate Paths**（路径扩增）\n\n   每年ImageNet Challenge的赢家都比上一年的冠军使用更加深层的网络。从AlexNet 到Inception到Resnets，Smith和他的团队也观察到“网络的路径数量成倍增长”的趋势，ResNet可以是不同长度的网络的指数集合。\n\n3. **Strive for Simplicity**（简洁原则）\n\n   更大的并不一定是更好的。在名为“Bigger is not necessarily better”的论文中，Springenberg 等人演示了如何用更少的单元实现最先进的结果。\n\n4. **Increase Symmetry** （增加对称性）\n\n   无论是在建筑上，还是在生物上，对称性被认为是质量和工艺的标志。Smith 将 FractalNet 的优雅归功于网络的对称性。\n\n5. **Pyramid Shape** （金字塔型）\n\n   在表征能力和减少冗余或者无用信息之间权衡。CNNs通常会降低激活函数的采样，并会增加从输入层到最终层之间的连接通道。\n\n6. **Over-train** （过训练）\n\n   另一个权衡是训练准确率和泛化能力。用正则化的方法类似 drop-out 或 drop-path进行提升泛化能力，这是神经网络的重要优势。用比实际用例更难的问题训练你的网络，以提高泛化性能。\n\n7. **Cover the Problem Space** （覆盖问题空间）\n\n   为了扩大你的训练数据和提升泛化能力，要使用噪声和人工增加训练集的大小，例如随机旋转、裁剪和一些图像操作。\n\n8. **Incremental Feature Construction** （递增的功能结构）\n\n   当架构变得成功时，它们会简化每一层的“工作”。在非常深的神经网络中，每个 层只会递增地修改输入。在ResNets中，每一层的输出可能类似于输入。所以，在实践中，请在ResNet中使用短的跳过长度。\n\n9. **Normalize Layer Inputs** （规范化层输入）\n\n   标准化是可以使计算层的工作变得更加容易的一条捷径，并且在实际中可以提升训练的准确性。批量标准化的发明者认为标准化发挥作用的原因在于处理内部的协变量，但Smith 认为，“标准化把所有层的输入样本放在了一个平等的基础上（类似于单位转换），这允许反向传播可以更有效地训练”。\n\n10. **Input Transition** （输入转换）\n\n  研究表明，在Wide ResNets中,性能随着通道数量的增加而提升，但是要权衡训练成本与准确性。AlexNet，VGG，Inception和ResNets都是在第一层中进行输入变换，以保证多种方式检查输入数据。\n\n11. **Available Resources Guide Layer Widths** （可用资源决定层宽度）\n\n    可供选择的输出数量并不明显，相应的是，这取决于您的硬件功能和所需的准确性。\n\n12. **Summation Joining** （求和连接）\n\n    Summation是一种流行的合并分支的方式。在 ResNets 中，使用求和作为连接机制可以让每一个分支都能计算残差和整体近似。如果输入跳跃连接始终存在，那么Summation会让每一层学到正确地东西（例如：输入的差别）。在任何分支都可以被丢弃的网络（例如 FractalNet）中，你应该使用这种方式保持输出的平滑。\n\n13. **Down-sampling Transition** （下采样变换）\n\n    在汇聚的时候，利用级联连接来增加输出的数量。当使用大于1的步幅时，这会同时处理加入并增加通道的数量。\n\n14. **Maxout for Competition ** （用于竞争的Maxout）\n\n    Maxout 用在只需要选择一个激活函数的局部竞争网络中。用的方法包含所有的激活函数，不同之处在于 maxout 只选择一个“胜出者”。Maxout 的一个明显的用例是每个分支具有不同大小的内核，而 Maxout 可以尺度不变。\n\n\n\n\n\n\n\n# Caffe调参\n\n###1. loss为nan(83.3365)  [标签错误、学习率太大]\n\n**梯度爆炸**\n\n**原因**：梯度变得非常大，使得学习过程难以继续\n\n**现象：**观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。\n\n**措施**： \n\n1. 减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 \n2. 设置clip gradient，用于限制过大的diff\n\n\n\n**不当的损失函数**\n\n**原因**：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。\n\n**现象**：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。\n\n**措施**：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。\n\n示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。\n\n\n\n**不当的输入**\n\n**原因**：输入中就含有NaN。\n\n**现象**：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。\n\n**措施**：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。\n\n**案例**：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。\n\n**池化层中步长比核的尺寸大**\n\n如下例所示，当池化层中stride > kernel的时候会在y中产生NaN\n\n```\n    layer {\n      name: \"faulty_pooling\"\n      type: \"Pooling\"\n      bottom: \"x\"\n      top: \"y\"\n      pooling_param {\n      pool: AVE\n      stride: 5\n      kernel: 3\n      }\n    }\n```\n\n###2. 避免overfitting\n\n- simpler model structure\n\n- regularization\n\n- data augmentation\n\n- dropall(dropout+drop-path)\n\n- Bootstrap/Bagging\n\n- ensemble\n\n- early stopping\n\n- utilize invariance\n\n- Bayesian\n\n  ​\n\n\n\n# 人脸识别流程\n\n**一般流程**:数据准备->人脸检测->人脸对齐->生成数据文件->训练模型->调参->model\n\n**tricks**:\n\n+ 在数据准备阶段，可以采用jitter等操作对样本数量较少的种类进行扩充，弥补类别不均衡造成的影响\n+ 在生成模型阶段，可以采用ensemble的方式对多个模型进行融合，比如mirror。\n\n\n\n**参考及致谢**\n\n[Common causes of nans during training](https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training)\n\n[常用激活函数比较](https://www.jianshu.com/p/22d9720dbf1a)\n\n[深度学习之五常见tricks](https://chenrudan.github.io/blog/2015/08/04/dl5tricks.html)\n\n《Deep convolutional neural network design patterns 》\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"人脸识别回顾","published":1,"updated":"2018-02-05T09:10:20.415Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440n100042sc8swzpqe1d","content":"<h1 id=\"数据准备\"><a href=\"#数据准备\" class=\"headerlink\" title=\"数据准备\"></a>数据准备</h1><p><strong>数据清洗</strong>：去除错误标签，图片质量不好的样本，如果有benchmark，还需要去除和benchmark重复的样本。</p>\n<p><strong>构造验证集</strong>:  对于给定的数据集，可能没有划分train set和validation set ，需要手动从给定的训练集中按照一定的比例分离出验证集（比如9:1）</p>\n<p><strong>数据均衡</strong>：数据集可能存在类别不均衡的问题，可以通过重新组合不均衡数据, 使之均衡，方式一: 复制或者合成（比如jitter操作）少数部分的样本, 使之和多数部分差不多数量； 方式二: 砍掉一些多数部分, 使两者数量差不多</p>\n<p><strong>数据扩充</strong>：对于一些样本数据比较少的数据集，为了更好的训练网络，有时候需要人为增加一些训练样本，比如随机的剪裁、缩放和旋转等。</p>\n<p><strong>预处理</strong>：常见的就是减均值、除方差。</p>\n<h1 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h1><p>####1. 模型选择：</p>\n<p>根据具体任务和数据集规模选择合适的网络结构，对于分类任务来说，如果数据集的规模不大，则网络的层数不应太深，结构也不应太复杂。</p>\n<p>####2. 激励函数的选择</p>\n<ul>\n<li><p><strong>sigmoid函数</strong>：取值范围为(0,1)，可以将一个实数映射到(0,1)的区间，可以用来做二分类，在特征相差比较复杂或是相差不是特别大时效果比较好，缺点是：激活函数计算量大，反向传播求梯度时，求导涉及除法，反向传播时，很容易出现梯度消失的情况</p>\n</li>\n<li><p><strong>Tanh函数</strong>：取值范围为[-1,1]，在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果，与sigmoid的区别是,tanh是0均值的，因此实际应用中，tanh会比sigmoid更好。</p>\n</li>\n<li><p><strong>ReLU函数</strong>：使用ReLU得到的SGD的收敛速度会比sigmoid/tanh快很多，缺点是训练的时候很”脆弱”，很容易就”die”了例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。</p>\n<p>选择的时候，就是根据各个函数的优缺点来配置，例如：</p>\n<p>如果使用 ReLU，要小心设置 learning rate，注意不要让网络出现很多 “dead” 神经元，如果不好解决，可以试试 Leaky ReLU、PReLU 或者 Maxout.</p>\n<p><strong>详细细节请参看</strong>：<a href=\"https://www.jianshu.com/p/22d9720dbf1a\" target=\"_blank\" rel=\"external\">常用激活函数比较</a></p>\n<p>​</p>\n</li>\n</ul>\n<p>####3. 卷积tricks</p>\n<p>图片输入是2的幂次方，例如32、64、96、224等。</p>\n<p>卷积核大小是3<em>3或者5</em>5。</p>\n<p>输入图片上下左右需要用0补充，即padding，且假如卷积核大小是5，那么padding就是2（图片左右上下都补充2）；卷积核大小是3，padding大小就是1。</p>\n<p>####4. pooling层tricks</p>\n<p>poolin层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。max pooling比avg pooling效果会好一些。avg-global pooling进入全卷积时代。</p>\n<h4 id=\"5-Loss函数的选择\"><a href=\"#5-Loss函数的选择\" class=\"headerlink\" title=\"5. Loss函数的选择\"></a>5. Loss函数的选择</h4><ul>\n<li><p><strong>softmax loss</strong></p>\n</li>\n<li><p>contrastive loss</p>\n</li>\n<li><p>triplet loss</p>\n</li>\n<li><p><strong>center loss</strong></p>\n<p><strong>triplet loss比softmax的优势</strong></p>\n<ul>\n<li>在于softmax不直接，（三元组直接优化距离），因而性能也不好。</li>\n<li>softmax产生的特征表示向量都很大，一般超过1000维。</li>\n<li>利用triplet loss的metric learning目的在于减小类内的L2距离，同时增大类间的距离</li>\n</ul>\n<p><strong>center loss VS triplet loss</strong></p>\n<ul>\n<li>triplet loss:dramatic data expansion</li>\n<li>center loss:more directly and efficiently</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"神经网络的设计模式\"><a href=\"#神经网络的设计模式\" class=\"headerlink\" title=\"神经网络的设计模式\"></a>神经网络的设计模式</h1><ol>\n<li><p><strong>Architectural Structure follows the Application</strong>（架构遵循应用）</p>\n<p>应该根据自己的应用场景选择合适的网络架构。</p>\n</li>\n<li><p><strong>Proliferate Paths</strong>（路径扩增）</p>\n<p>每年ImageNet Challenge的赢家都比上一年的冠军使用更加深层的网络。从AlexNet 到Inception到Resnets，Smith和他的团队也观察到“网络的路径数量成倍增长”的趋势，ResNet可以是不同长度的网络的指数集合。</p>\n</li>\n<li><p><strong>Strive for Simplicity</strong>（简洁原则）</p>\n<p>更大的并不一定是更好的。在名为“Bigger is not necessarily better”的论文中，Springenberg 等人演示了如何用更少的单元实现最先进的结果。</p>\n</li>\n<li><p><strong>Increase Symmetry</strong> （增加对称性）</p>\n<p>无论是在建筑上，还是在生物上，对称性被认为是质量和工艺的标志。Smith 将 FractalNet 的优雅归功于网络的对称性。</p>\n</li>\n<li><p><strong>Pyramid Shape</strong> （金字塔型）</p>\n<p>在表征能力和减少冗余或者无用信息之间权衡。CNNs通常会降低激活函数的采样，并会增加从输入层到最终层之间的连接通道。</p>\n</li>\n<li><p><strong>Over-train</strong> （过训练）</p>\n<p>另一个权衡是训练准确率和泛化能力。用正则化的方法类似 drop-out 或 drop-path进行提升泛化能力，这是神经网络的重要优势。用比实际用例更难的问题训练你的网络，以提高泛化性能。</p>\n</li>\n<li><p><strong>Cover the Problem Space</strong> （覆盖问题空间）</p>\n<p>为了扩大你的训练数据和提升泛化能力，要使用噪声和人工增加训练集的大小，例如随机旋转、裁剪和一些图像操作。</p>\n</li>\n<li><p><strong>Incremental Feature Construction</strong> （递增的功能结构）</p>\n<p>当架构变得成功时，它们会简化每一层的“工作”。在非常深的神经网络中，每个 层只会递增地修改输入。在ResNets中，每一层的输出可能类似于输入。所以，在实践中，请在ResNet中使用短的跳过长度。</p>\n</li>\n<li><p><strong>Normalize Layer Inputs</strong> （规范化层输入）</p>\n<p>标准化是可以使计算层的工作变得更加容易的一条捷径，并且在实际中可以提升训练的准确性。批量标准化的发明者认为标准化发挥作用的原因在于处理内部的协变量，但Smith 认为，“标准化把所有层的输入样本放在了一个平等的基础上（类似于单位转换），这允许反向传播可以更有效地训练”。</p>\n</li>\n<li><p><strong>Input Transition</strong> （输入转换）</p>\n<p>研究表明，在Wide ResNets中,性能随着通道数量的增加而提升，但是要权衡训练成本与准确性。AlexNet，VGG，Inception和ResNets都是在第一层中进行输入变换，以保证多种方式检查输入数据。</p>\n</li>\n<li><p><strong>Available Resources Guide Layer Widths</strong> （可用资源决定层宽度）</p>\n<p>可供选择的输出数量并不明显，相应的是，这取决于您的硬件功能和所需的准确性。</p>\n</li>\n<li><p><strong>Summation Joining</strong> （求和连接）</p>\n<p>Summation是一种流行的合并分支的方式。在 ResNets 中，使用求和作为连接机制可以让每一个分支都能计算残差和整体近似。如果输入跳跃连接始终存在，那么Summation会让每一层学到正确地东西（例如：输入的差别）。在任何分支都可以被丢弃的网络（例如 FractalNet）中，你应该使用这种方式保持输出的平滑。</p>\n</li>\n<li><p><strong>Down-sampling Transition</strong> （下采样变换）</p>\n<p>在汇聚的时候，利用级联连接来增加输出的数量。当使用大于1的步幅时，这会同时处理加入并增加通道的数量。</p>\n</li>\n<li><p><strong>Maxout for Competition </strong> （用于竞争的Maxout）</p>\n<p>Maxout 用在只需要选择一个激活函数的局部竞争网络中。用的方法包含所有的激活函数，不同之处在于 maxout 只选择一个“胜出者”。Maxout 的一个明显的用例是每个分支具有不同大小的内核，而 Maxout 可以尺度不变。</p>\n</li>\n</ol>\n<h1 id=\"Caffe调参\"><a href=\"#Caffe调参\" class=\"headerlink\" title=\"Caffe调参\"></a>Caffe调参</h1><p>###1. loss为nan(83.3365)  [标签错误、学习率太大]</p>\n<p><strong>梯度爆炸</strong></p>\n<p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续</p>\n<p><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。</p>\n<p><strong>措施</strong>： </p>\n<ol>\n<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>\n<li>设置clip gradient，用于限制过大的diff</li>\n</ol>\n<p><strong>不当的损失函数</strong></p>\n<p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p>\n<p><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>\n<p><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。</p>\n<p>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>\n<p><strong>不当的输入</strong></p>\n<p><strong>原因</strong>：输入中就含有NaN。</p>\n<p><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>\n<p><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。</p>\n<p><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>\n<p><strong>池化层中步长比核的尺寸大</strong></p>\n<p>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">  name: &quot;faulty_pooling&quot;</div><div class=\"line\">  type: &quot;Pooling&quot;</div><div class=\"line\">  bottom: &quot;x&quot;</div><div class=\"line\">  top: &quot;y&quot;</div><div class=\"line\">  pooling_param &#123;</div><div class=\"line\">  pool: AVE</div><div class=\"line\">  stride: 5</div><div class=\"line\">  kernel: 3</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>###2. 避免overfitting</p>\n<ul>\n<li><p>simpler model structure</p>\n</li>\n<li><p>regularization</p>\n</li>\n<li><p>data augmentation</p>\n</li>\n<li><p>dropall(dropout+drop-path)</p>\n</li>\n<li><p>Bootstrap/Bagging</p>\n</li>\n<li><p>ensemble</p>\n</li>\n<li><p>early stopping</p>\n</li>\n<li><p>utilize invariance</p>\n</li>\n<li><p>Bayesian</p>\n<p>​</p>\n</li>\n</ul>\n<h1 id=\"人脸识别流程\"><a href=\"#人脸识别流程\" class=\"headerlink\" title=\"人脸识别流程\"></a>人脸识别流程</h1><p><strong>一般流程</strong>:数据准备-&gt;人脸检测-&gt;人脸对齐-&gt;生成数据文件-&gt;训练模型-&gt;调参-&gt;model</p>\n<p><strong>tricks</strong>:</p>\n<ul>\n<li>在数据准备阶段，可以采用jitter等操作对样本数量较少的种类进行扩充，弥补类别不均衡造成的影响</li>\n<li>在生成模型阶段，可以采用ensemble的方式对多个模型进行融合，比如mirror。</li>\n</ul>\n<p><strong>参考及致谢</strong></p>\n<p><a href=\"https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training\" target=\"_blank\" rel=\"external\">Common causes of nans during training</a></p>\n<p><a href=\"https://www.jianshu.com/p/22d9720dbf1a\" target=\"_blank\" rel=\"external\">常用激活函数比较</a></p>\n<p><a href=\"https://chenrudan.github.io/blog/2015/08/04/dl5tricks.html\" target=\"_blank\" rel=\"external\">深度学习之五常见tricks</a></p>\n<p>《Deep convolutional neural network design patterns 》</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"数据准备\"><a href=\"#数据准备\" class=\"headerlink\" title=\"数据准备\"></a>数据准备</h1><p><strong>数据清洗</strong>：去除错误标签，图片质量不好的样本，如果有benchmark，还需要去除和benchmark重复的样本。</p>\n<p><strong>构造验证集</strong>:  对于给定的数据集，可能没有划分train set和validation set ，需要手动从给定的训练集中按照一定的比例分离出验证集（比如9:1）</p>\n<p><strong>数据均衡</strong>：数据集可能存在类别不均衡的问题，可以通过重新组合不均衡数据, 使之均衡，方式一: 复制或者合成（比如jitter操作）少数部分的样本, 使之和多数部分差不多数量； 方式二: 砍掉一些多数部分, 使两者数量差不多</p>\n<p><strong>数据扩充</strong>：对于一些样本数据比较少的数据集，为了更好的训练网络，有时候需要人为增加一些训练样本，比如随机的剪裁、缩放和旋转等。</p>\n<p><strong>预处理</strong>：常见的就是减均值、除方差。</p>\n<h1 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h1><p>####1. 模型选择：</p>\n<p>根据具体任务和数据集规模选择合适的网络结构，对于分类任务来说，如果数据集的规模不大，则网络的层数不应太深，结构也不应太复杂。</p>\n<p>####2. 激励函数的选择</p>\n<ul>\n<li><p><strong>sigmoid函数</strong>：取值范围为(0,1)，可以将一个实数映射到(0,1)的区间，可以用来做二分类，在特征相差比较复杂或是相差不是特别大时效果比较好，缺点是：激活函数计算量大，反向传播求梯度时，求导涉及除法，反向传播时，很容易出现梯度消失的情况</p>\n</li>\n<li><p><strong>Tanh函数</strong>：取值范围为[-1,1]，在特征相差明显时的效果会很好，在循环过程中会不断扩大特征效果，与sigmoid的区别是,tanh是0均值的，因此实际应用中，tanh会比sigmoid更好。</p>\n</li>\n<li><p><strong>ReLU函数</strong>：使用ReLU得到的SGD的收敛速度会比sigmoid/tanh快很多，缺点是训练的时候很”脆弱”，很容易就”die”了例如，一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0.如果 learning rate 很大，那么很有可能网络中的 40% 的神经元都”dead”了。</p>\n<p>选择的时候，就是根据各个函数的优缺点来配置，例如：</p>\n<p>如果使用 ReLU，要小心设置 learning rate，注意不要让网络出现很多 “dead” 神经元，如果不好解决，可以试试 Leaky ReLU、PReLU 或者 Maxout.</p>\n<p><strong>详细细节请参看</strong>：<a href=\"https://www.jianshu.com/p/22d9720dbf1a\" target=\"_blank\" rel=\"external\">常用激活函数比较</a></p>\n<p>​</p>\n</li>\n</ul>\n<p>####3. 卷积tricks</p>\n<p>图片输入是2的幂次方，例如32、64、96、224等。</p>\n<p>卷积核大小是3<em>3或者5</em>5。</p>\n<p>输入图片上下左右需要用0补充，即padding，且假如卷积核大小是5，那么padding就是2（图片左右上下都补充2）；卷积核大小是3，padding大小就是1。</p>\n<p>####4. pooling层tricks</p>\n<p>poolin层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。max pooling比avg pooling效果会好一些。avg-global pooling进入全卷积时代。</p>\n<h4 id=\"5-Loss函数的选择\"><a href=\"#5-Loss函数的选择\" class=\"headerlink\" title=\"5. Loss函数的选择\"></a>5. Loss函数的选择</h4><ul>\n<li><p><strong>softmax loss</strong></p>\n</li>\n<li><p>contrastive loss</p>\n</li>\n<li><p>triplet loss</p>\n</li>\n<li><p><strong>center loss</strong></p>\n<p><strong>triplet loss比softmax的优势</strong></p>\n<ul>\n<li>在于softmax不直接，（三元组直接优化距离），因而性能也不好。</li>\n<li>softmax产生的特征表示向量都很大，一般超过1000维。</li>\n<li>利用triplet loss的metric learning目的在于减小类内的L2距离，同时增大类间的距离</li>\n</ul>\n<p><strong>center loss VS triplet loss</strong></p>\n<ul>\n<li>triplet loss:dramatic data expansion</li>\n<li>center loss:more directly and efficiently</li>\n</ul>\n</li>\n</ul>\n<h1 id=\"神经网络的设计模式\"><a href=\"#神经网络的设计模式\" class=\"headerlink\" title=\"神经网络的设计模式\"></a>神经网络的设计模式</h1><ol>\n<li><p><strong>Architectural Structure follows the Application</strong>（架构遵循应用）</p>\n<p>应该根据自己的应用场景选择合适的网络架构。</p>\n</li>\n<li><p><strong>Proliferate Paths</strong>（路径扩增）</p>\n<p>每年ImageNet Challenge的赢家都比上一年的冠军使用更加深层的网络。从AlexNet 到Inception到Resnets，Smith和他的团队也观察到“网络的路径数量成倍增长”的趋势，ResNet可以是不同长度的网络的指数集合。</p>\n</li>\n<li><p><strong>Strive for Simplicity</strong>（简洁原则）</p>\n<p>更大的并不一定是更好的。在名为“Bigger is not necessarily better”的论文中，Springenberg 等人演示了如何用更少的单元实现最先进的结果。</p>\n</li>\n<li><p><strong>Increase Symmetry</strong> （增加对称性）</p>\n<p>无论是在建筑上，还是在生物上，对称性被认为是质量和工艺的标志。Smith 将 FractalNet 的优雅归功于网络的对称性。</p>\n</li>\n<li><p><strong>Pyramid Shape</strong> （金字塔型）</p>\n<p>在表征能力和减少冗余或者无用信息之间权衡。CNNs通常会降低激活函数的采样，并会增加从输入层到最终层之间的连接通道。</p>\n</li>\n<li><p><strong>Over-train</strong> （过训练）</p>\n<p>另一个权衡是训练准确率和泛化能力。用正则化的方法类似 drop-out 或 drop-path进行提升泛化能力，这是神经网络的重要优势。用比实际用例更难的问题训练你的网络，以提高泛化性能。</p>\n</li>\n<li><p><strong>Cover the Problem Space</strong> （覆盖问题空间）</p>\n<p>为了扩大你的训练数据和提升泛化能力，要使用噪声和人工增加训练集的大小，例如随机旋转、裁剪和一些图像操作。</p>\n</li>\n<li><p><strong>Incremental Feature Construction</strong> （递增的功能结构）</p>\n<p>当架构变得成功时，它们会简化每一层的“工作”。在非常深的神经网络中，每个 层只会递增地修改输入。在ResNets中，每一层的输出可能类似于输入。所以，在实践中，请在ResNet中使用短的跳过长度。</p>\n</li>\n<li><p><strong>Normalize Layer Inputs</strong> （规范化层输入）</p>\n<p>标准化是可以使计算层的工作变得更加容易的一条捷径，并且在实际中可以提升训练的准确性。批量标准化的发明者认为标准化发挥作用的原因在于处理内部的协变量，但Smith 认为，“标准化把所有层的输入样本放在了一个平等的基础上（类似于单位转换），这允许反向传播可以更有效地训练”。</p>\n</li>\n<li><p><strong>Input Transition</strong> （输入转换）</p>\n<p>研究表明，在Wide ResNets中,性能随着通道数量的增加而提升，但是要权衡训练成本与准确性。AlexNet，VGG，Inception和ResNets都是在第一层中进行输入变换，以保证多种方式检查输入数据。</p>\n</li>\n<li><p><strong>Available Resources Guide Layer Widths</strong> （可用资源决定层宽度）</p>\n<p>可供选择的输出数量并不明显，相应的是，这取决于您的硬件功能和所需的准确性。</p>\n</li>\n<li><p><strong>Summation Joining</strong> （求和连接）</p>\n<p>Summation是一种流行的合并分支的方式。在 ResNets 中，使用求和作为连接机制可以让每一个分支都能计算残差和整体近似。如果输入跳跃连接始终存在，那么Summation会让每一层学到正确地东西（例如：输入的差别）。在任何分支都可以被丢弃的网络（例如 FractalNet）中，你应该使用这种方式保持输出的平滑。</p>\n</li>\n<li><p><strong>Down-sampling Transition</strong> （下采样变换）</p>\n<p>在汇聚的时候，利用级联连接来增加输出的数量。当使用大于1的步幅时，这会同时处理加入并增加通道的数量。</p>\n</li>\n<li><p><strong>Maxout for Competition </strong> （用于竞争的Maxout）</p>\n<p>Maxout 用在只需要选择一个激活函数的局部竞争网络中。用的方法包含所有的激活函数，不同之处在于 maxout 只选择一个“胜出者”。Maxout 的一个明显的用例是每个分支具有不同大小的内核，而 Maxout 可以尺度不变。</p>\n</li>\n</ol>\n<h1 id=\"Caffe调参\"><a href=\"#Caffe调参\" class=\"headerlink\" title=\"Caffe调参\"></a>Caffe调参</h1><p>###1. loss为nan(83.3365)  [标签错误、学习率太大]</p>\n<p><strong>梯度爆炸</strong></p>\n<p><strong>原因</strong>：梯度变得非常大，使得学习过程难以继续</p>\n<p><strong>现象：</strong>观察log，注意每一轮迭代后的loss。loss随着每轮迭代越来越大，最终超过了浮点型表示的范围，就变成了NaN。</p>\n<p><strong>措施</strong>： </p>\n<ol>\n<li>减小solver.prototxt中的base_lr，至少减小一个数量级。如果有多个loss layer，需要找出哪个损失层导致了梯度爆炸，并在train_val.prototxt中减小该层的loss_weight，而非是减小通用的base_lr。 </li>\n<li>设置clip gradient，用于限制过大的diff</li>\n</ol>\n<p><strong>不当的损失函数</strong></p>\n<p><strong>原因</strong>：有时候损失层中loss的计算可能导致NaN的出现。比如，给InfogainLoss层（信息熵损失）输入没有归一化的值，使用带有bug的自定义损失层等等。</p>\n<p><strong>现象</strong>：观测训练产生的log时一开始并不能看到异常，loss也在逐步的降低，但突然之间NaN就出现了。</p>\n<p><strong>措施</strong>：看看你是否能重现这个错误，在loss layer中加入一些输出以进行调试。</p>\n<p>示例：有一次我使用的loss归一化了batch中label错误的次数。如果某个label从未在batch中出现过，loss就会变成NaN。在这种情况下，可以用足够大的batch来尽量避免这个错误。</p>\n<p><strong>不当的输入</strong></p>\n<p><strong>原因</strong>：输入中就含有NaN。</p>\n<p><strong>现象</strong>：每当学习的过程中碰到这个错误的输入，就会变成NaN。观察log的时候也许不能察觉任何异常，loss逐步的降低，但突然间就变成NaN了。</p>\n<p><strong>措施</strong>：重整你的数据集，确保训练集和验证集里面没有损坏的图片。调试中你可以使用一个简单的网络来读取输入层，有一个缺省的loss，并过一遍所有输入，如果其中有错误的输入，这个缺省的层也会产生NaN。</p>\n<p><strong>案例</strong>：有一次公司需要训练一个模型，把标注好的图片放在了七牛上，拉下来的时候发生了dns劫持，有一张图片被换成了淘宝的购物二维码，且这个二维码格式与原图的格式不符合，因此成为了一张“损坏”图片。每次训练遇到这个图片的时候就会产生NaN。良好的习惯是，你有一个检测性的网络，每次训练目标网络之前把所有的样本在这个检测性的网络里面过一遍，去掉非法值。</p>\n<p><strong>池化层中步长比核的尺寸大</strong></p>\n<p>如下例所示，当池化层中stride &gt; kernel的时候会在y中产生NaN</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">layer &#123;</div><div class=\"line\">  name: &quot;faulty_pooling&quot;</div><div class=\"line\">  type: &quot;Pooling&quot;</div><div class=\"line\">  bottom: &quot;x&quot;</div><div class=\"line\">  top: &quot;y&quot;</div><div class=\"line\">  pooling_param &#123;</div><div class=\"line\">  pool: AVE</div><div class=\"line\">  stride: 5</div><div class=\"line\">  kernel: 3</div><div class=\"line\">  &#125;</div><div class=\"line\">&#125;</div></pre></td></tr></table></figure>\n<p>###2. 避免overfitting</p>\n<ul>\n<li><p>simpler model structure</p>\n</li>\n<li><p>regularization</p>\n</li>\n<li><p>data augmentation</p>\n</li>\n<li><p>dropall(dropout+drop-path)</p>\n</li>\n<li><p>Bootstrap/Bagging</p>\n</li>\n<li><p>ensemble</p>\n</li>\n<li><p>early stopping</p>\n</li>\n<li><p>utilize invariance</p>\n</li>\n<li><p>Bayesian</p>\n<p>​</p>\n</li>\n</ul>\n<h1 id=\"人脸识别流程\"><a href=\"#人脸识别流程\" class=\"headerlink\" title=\"人脸识别流程\"></a>人脸识别流程</h1><p><strong>一般流程</strong>:数据准备-&gt;人脸检测-&gt;人脸对齐-&gt;生成数据文件-&gt;训练模型-&gt;调参-&gt;model</p>\n<p><strong>tricks</strong>:</p>\n<ul>\n<li>在数据准备阶段，可以采用jitter等操作对样本数量较少的种类进行扩充，弥补类别不均衡造成的影响</li>\n<li>在生成模型阶段，可以采用ensemble的方式对多个模型进行融合，比如mirror。</li>\n</ul>\n<p><strong>参考及致谢</strong></p>\n<p><a href=\"https://stackoverflow.com/questions/33962226/common-causes-of-nans-during-training\" target=\"_blank\" rel=\"external\">Common causes of nans during training</a></p>\n<p><a href=\"https://www.jianshu.com/p/22d9720dbf1a\" target=\"_blank\" rel=\"external\">常用激活函数比较</a></p>\n<p><a href=\"https://chenrudan.github.io/blog/2015/08/04/dl5tricks.html\" target=\"_blank\" rel=\"external\">深度学习之五常见tricks</a></p>\n<p>《Deep convolutional neural network design patterns 》</p>\n"},{"title":"Hexo相关","mathjax":true,"date":"2017-06-02T08:05:45.000Z","description":null,"_content":"\n# 问题\n\n## Hexo无法正常显示公式\n\n善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：\n\n```\n# MathJax Support\nmathjax:\n  enable: true\n  per_page: true\n```\n\n另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：\n\n```\n---\ntitle: index.html\ndate: 2016-12-28 21:01:30\ntags:\nmathjax: true\n--\n```\n\n题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：\n\n```\ntitle: {{ title }}\ndate: {{ date }}\ncategories: \ntags:\ndescription: \nmathjax: \n```\n\n\n\n# 优化\n\n[Hexo的版本控制与持续集成](https://formulahendry.github.io/2016/12/04/hexo-ci/)\n\n","source":"_posts/Hexo相关.md","raw":"---\ntitle: Hexo相关\nmathjax: true\ndate: 2017-06-02 16:05:45\ncategories: Hexo\ntags: [Hexo]\ndescription:\n---\n\n# 问题\n\n## Hexo无法正常显示公式\n\n善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：\n\n```\n# MathJax Support\nmathjax:\n  enable: true\n  per_page: true\n```\n\n另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：\n\n```\n---\ntitle: index.html\ndate: 2016-12-28 21:01:30\ntags:\nmathjax: true\n--\n```\n\n题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：\n\n```\ntitle: {{ title }}\ndate: {{ date }}\ncategories: \ntags:\ndescription: \nmathjax: \n```\n\n\n\n# 优化\n\n[Hexo的版本控制与持续集成](https://formulahendry.github.io/2016/12/04/hexo-ci/)\n\n","slug":"Hexo相关","published":1,"updated":"2017-06-28T12:28:10.590Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440rx000j2sc8a4u0ndk1","content":"<h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><h2 id=\"Hexo无法正常显示公式\"><a href=\"#Hexo无法正常显示公式\" class=\"headerlink\" title=\"Hexo无法正常显示公式\"></a>Hexo无法正常显示公式</h2><p>善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"># MathJax Support</div><div class=\"line\">mathjax:</div><div class=\"line\">  enable: true</div><div class=\"line\">  per_page: true</div></pre></td></tr></table></figure>\n<p>另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">---</div><div class=\"line\">title: index.html</div><div class=\"line\">date: 2016-12-28 21:01:30</div><div class=\"line\">tags:</div><div class=\"line\">mathjax: true</div><div class=\"line\">--</div></pre></td></tr></table></figure>\n<p>题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">title: &#123;&#123; title &#125;&#125;</div><div class=\"line\">date: &#123;&#123; date &#125;&#125;</div><div class=\"line\">categories: </div><div class=\"line\">tags:</div><div class=\"line\">description: </div><div class=\"line\">mathjax:</div></pre></td></tr></table></figure>\n<h1 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h1><p><a href=\"https://formulahendry.github.io/2016/12/04/hexo-ci/\" target=\"_blank\" rel=\"external\">Hexo的版本控制与持续集成</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h1><h2 id=\"Hexo无法正常显示公式\"><a href=\"#Hexo无法正常显示公式\" class=\"headerlink\" title=\"Hexo无法正常显示公式\"></a>Hexo无法正常显示公式</h2><p>善用主题(theme)，以我使用的next主题为例，打开/themes/next/_config.yml文件，更改mathjax开关为：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\"># MathJax Support</div><div class=\"line\">mathjax:</div><div class=\"line\">  enable: true</div><div class=\"line\">  per_page: true</div></pre></td></tr></table></figure>\n<p>另外，还要在文章(.md文件)头设置开关，只用在有用公式显示的页面才加载Mathjax渲染，不影响其他的页面渲染速度，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">---</div><div class=\"line\">title: index.html</div><div class=\"line\">date: 2016-12-28 21:01:30</div><div class=\"line\">tags:</div><div class=\"line\">mathjax: true</div><div class=\"line\">--</div></pre></td></tr></table></figure>\n<p>题外话，可以在/scaffolds/post.md文件中添加mathjax一行，这样每次layout如果是由默认的post 生成新的文章的开头都会有mathjax，可以自己选择true或是false(注意mathjax冒号后面不要掉了空格)，如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">title: &#123;&#123; title &#125;&#125;</div><div class=\"line\">date: &#123;&#123; date &#125;&#125;</div><div class=\"line\">categories: </div><div class=\"line\">tags:</div><div class=\"line\">description: </div><div class=\"line\">mathjax:</div></pre></td></tr></table></figure>\n<h1 id=\"优化\"><a href=\"#优化\" class=\"headerlink\" title=\"优化\"></a>优化</h1><p><a href=\"https://formulahendry.github.io/2016/12/04/hexo-ci/\" target=\"_blank\" rel=\"external\">Hexo的版本控制与持续集成</a></p>\n"},{"title":"FaceNet论文笔记","date":"2017-06-28T02:11:42.000Z","description":null,"mathjax":null,"_content":"\n*原文链接*:[FaceNet:A unified embedding for face recognition and clustering](https://arxiv.org/abs/1503.03832)\n\n## 简介\n\nFaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。\n\n## 前言\n\nFaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。\n\n当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。选取的triplets包含两个匹配脸部缩略图（为紧密裁剪的脸部区域，不用使用2d、3d对齐以及放大转换等预处理）和一个非匹配的脸部缩略图，loss函数目标是通过距离边界区分正负类。\n\n本文中，探索了两类深度卷积神经网络，第一类为Zeiler&Fergus研究中使用的神经网络，包含多个交错的卷积层、非线性激励函数，局部相应归一化和最大池化层。我们额外的添加了一些1x1xd的卷积层。第二种结构是基于Inception model，这种网络利用了一些不同的卷积层和池化层并行和级联响应。我们发现这些模型可以减小20倍以上的参数数量，并且可能会减少FLOPS数量。\n\ntriplet loss的启发是传统loss函数趋向于将有一类特征的人脸图像映射到同一个空间，而triplet loss尝试将一个个体的人脸图像和其他人脸图像分开。\n\n![facenet_triplet1](.\\pictures\\facenet_triplet1.png)\n\n\n\n\n\n## 总结\n\n- 三元组的目标函数并不是这篇论文首创，我在之前的一些Hash索引的论文中也见过相似的应用。可见，并不是所有的学习特征的模型都必须用softmax。用其他的效果也会好。\n- 三元组比softmax的优势在于\n  - softmax不直接，（三元组直接优化距离），因而性能也不好。\n  - softmax产生的特征表示向量都很大，一般超过1000维。\n- FaceNet并没有像DeepFace和DeepID那样需要对齐。\n- FaceNet得到最终表示后不用像DeepID那样需要再训练模型进行分类，直接计算距离就好了，简单而有效。\n- 论文并未探讨二元对的有效性，直接使用的三元对。\n\n\n\n## 参考文献\n\n[谷歌人脸识别系统FaceNet解析](https://zhuanlan.zhihu.com/p/24837264)\n\n[FaceNet--Google的人脸识别](http://blog.csdn.net/stdcoutzyx/article/details/46687471)\n\n","source":"_posts/FaceNet论文笔记.md","raw":"---\ntitle: FaceNet论文笔记\ndate: 2017-06-28 10:11:42\ncategories: Face\ntags: [笔记，人脸识别]\ndescription:\nmathjax:\n---\n\n*原文链接*:[FaceNet:A unified embedding for face recognition and clustering](https://arxiv.org/abs/1503.03832)\n\n## 简介\n\nFaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。\n\n## 前言\n\nFaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。\n\n当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。选取的triplets包含两个匹配脸部缩略图（为紧密裁剪的脸部区域，不用使用2d、3d对齐以及放大转换等预处理）和一个非匹配的脸部缩略图，loss函数目标是通过距离边界区分正负类。\n\n本文中，探索了两类深度卷积神经网络，第一类为Zeiler&Fergus研究中使用的神经网络，包含多个交错的卷积层、非线性激励函数，局部相应归一化和最大池化层。我们额外的添加了一些1x1xd的卷积层。第二种结构是基于Inception model，这种网络利用了一些不同的卷积层和池化层并行和级联响应。我们发现这些模型可以减小20倍以上的参数数量，并且可能会减少FLOPS数量。\n\ntriplet loss的启发是传统loss函数趋向于将有一类特征的人脸图像映射到同一个空间，而triplet loss尝试将一个个体的人脸图像和其他人脸图像分开。\n\n![facenet_triplet1](.\\pictures\\facenet_triplet1.png)\n\n\n\n\n\n## 总结\n\n- 三元组的目标函数并不是这篇论文首创，我在之前的一些Hash索引的论文中也见过相似的应用。可见，并不是所有的学习特征的模型都必须用softmax。用其他的效果也会好。\n- 三元组比softmax的优势在于\n  - softmax不直接，（三元组直接优化距离），因而性能也不好。\n  - softmax产生的特征表示向量都很大，一般超过1000维。\n- FaceNet并没有像DeepFace和DeepID那样需要对齐。\n- FaceNet得到最终表示后不用像DeepID那样需要再训练模型进行分类，直接计算距离就好了，简单而有效。\n- 论文并未探讨二元对的有效性，直接使用的三元对。\n\n\n\n## 参考文献\n\n[谷歌人脸识别系统FaceNet解析](https://zhuanlan.zhihu.com/p/24837264)\n\n[FaceNet--Google的人脸识别](http://blog.csdn.net/stdcoutzyx/article/details/46687471)\n\n","slug":"FaceNet论文笔记","published":1,"updated":"2017-06-28T13:23:36.661Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440sd000l2sc88g9deobd","content":"<p><em>原文链接</em>:<a href=\"https://arxiv.org/abs/1503.03832\" target=\"_blank\" rel=\"external\">FaceNet:A unified embedding for face recognition and clustering</a></p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>FaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。</p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>FaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。</p>\n<p>当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。选取的triplets包含两个匹配脸部缩略图（为紧密裁剪的脸部区域，不用使用2d、3d对齐以及放大转换等预处理）和一个非匹配的脸部缩略图，loss函数目标是通过距离边界区分正负类。</p>\n<p>本文中，探索了两类深度卷积神经网络，第一类为Zeiler&amp;Fergus研究中使用的神经网络，包含多个交错的卷积层、非线性激励函数，局部相应归一化和最大池化层。我们额外的添加了一些1x1xd的卷积层。第二种结构是基于Inception model，这种网络利用了一些不同的卷积层和池化层并行和级联响应。我们发现这些模型可以减小20倍以上的参数数量，并且可能会减少FLOPS数量。</p>\n<p>triplet loss的启发是传统loss函数趋向于将有一类特征的人脸图像映射到同一个空间，而triplet loss尝试将一个个体的人脸图像和其他人脸图像分开。</p>\n<p><img src=\".\\pictures\\facenet_triplet1.png\" alt=\"facenet_triplet1\"></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>三元组的目标函数并不是这篇论文首创，我在之前的一些Hash索引的论文中也见过相似的应用。可见，并不是所有的学习特征的模型都必须用softmax。用其他的效果也会好。</li>\n<li>三元组比softmax的优势在于<ul>\n<li>softmax不直接，（三元组直接优化距离），因而性能也不好。</li>\n<li>softmax产生的特征表示向量都很大，一般超过1000维。</li>\n</ul>\n</li>\n<li>FaceNet并没有像DeepFace和DeepID那样需要对齐。</li>\n<li>FaceNet得到最终表示后不用像DeepID那样需要再训练模型进行分类，直接计算距离就好了，简单而有效。</li>\n<li>论文并未探讨二元对的有效性，直接使用的三元对。</li>\n</ul>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><p><a href=\"https://zhuanlan.zhihu.com/p/24837264\" target=\"_blank\" rel=\"external\">谷歌人脸识别系统FaceNet解析</a></p>\n<p><a href=\"http://blog.csdn.net/stdcoutzyx/article/details/46687471\" target=\"_blank\" rel=\"external\">FaceNet–Google的人脸识别</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p><em>原文链接</em>:<a href=\"https://arxiv.org/abs/1503.03832\" target=\"_blank\" rel=\"external\">FaceNet:A unified embedding for face recognition and clustering</a></p>\n<h2 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h2><p>FaceNet，可以直接将人脸图像映射到欧几里得空间，空间距离的长度代表了人脸图像的相似性，只要该映射空间生成，人脸识别、验证和聚类等任务就可以轻松完成。FaceNet在LFW数据集上的准确率为99.63%，在YouTube Faces 数据集上准确率为95.12%。</p>\n<h2 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h2><p>FaceNet采用的是通过卷积神经网络学习将图像映射到欧几里得空间，空间距离之间和图片相似度相关：同一个人的不同图像的空间距离很小，不同人的图像的空间距离较大，只要该映射确定下来，相关的人脸识别任务就变得简单。</p>\n<p>当前存在的基于深度神经网络的人脸识别模型使用了分类层：中间层为人脸图像的向量映射，然后以分类层作为输出层，这类方法的缺点就是不直接和效率不高。与当前方法不同，FaceNet直接使用基于triplets的LMNN（最大边界近邻分类）的loss函数训练神经网络，网络直接输出为128维度的向量空间。选取的triplets包含两个匹配脸部缩略图（为紧密裁剪的脸部区域，不用使用2d、3d对齐以及放大转换等预处理）和一个非匹配的脸部缩略图，loss函数目标是通过距离边界区分正负类。</p>\n<p>本文中，探索了两类深度卷积神经网络，第一类为Zeiler&amp;Fergus研究中使用的神经网络，包含多个交错的卷积层、非线性激励函数，局部相应归一化和最大池化层。我们额外的添加了一些1x1xd的卷积层。第二种结构是基于Inception model，这种网络利用了一些不同的卷积层和池化层并行和级联响应。我们发现这些模型可以减小20倍以上的参数数量，并且可能会减少FLOPS数量。</p>\n<p>triplet loss的启发是传统loss函数趋向于将有一类特征的人脸图像映射到同一个空间，而triplet loss尝试将一个个体的人脸图像和其他人脸图像分开。</p>\n<p><img src=\".\\pictures\\facenet_triplet1.png\" alt=\"facenet_triplet1\"></p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><ul>\n<li>三元组的目标函数并不是这篇论文首创，我在之前的一些Hash索引的论文中也见过相似的应用。可见，并不是所有的学习特征的模型都必须用softmax。用其他的效果也会好。</li>\n<li>三元组比softmax的优势在于<ul>\n<li>softmax不直接，（三元组直接优化距离），因而性能也不好。</li>\n<li>softmax产生的特征表示向量都很大，一般超过1000维。</li>\n</ul>\n</li>\n<li>FaceNet并没有像DeepFace和DeepID那样需要对齐。</li>\n<li>FaceNet得到最终表示后不用像DeepID那样需要再训练模型进行分类，直接计算距离就好了，简单而有效。</li>\n<li>论文并未探讨二元对的有效性，直接使用的三元对。</li>\n</ul>\n<h2 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h2><p><a href=\"https://zhuanlan.zhihu.com/p/24837264\" target=\"_blank\" rel=\"external\">谷歌人脸识别系统FaceNet解析</a></p>\n<p><a href=\"http://blog.csdn.net/stdcoutzyx/article/details/46687471\" target=\"_blank\" rel=\"external\">FaceNet–Google的人脸识别</a></p>\n"},{"title":"Python","date":"2017-06-09T12:52:56.000Z","description":"Python","mathjax":false,"_content":"\n# 图像操作\n\n## 关于PIL image和skimage的图像处理\n\n### 对skimage图像\n\n镜像处理：\n\n```\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\nimg=io.imread(\"test.jpg\")\n#img=transform.rotate(img,180)\nimg=cv2.flip(img,1)\nplt.figure('skimage')\nplt.imshow(img)\nplt.show()\nprint img.shape\nprint(img.dtype)\n```\n\n# 文本处理\n\n## 提取两个文件中相同的部分\n\na.txt内容：\n\n```\naaa 0\nbbb 0\nccc 0\n```\n\nb.txt内容：\n\n```\naaa 0\nbbb 1\nccc 0\n```\n\n### 提取相同的部分\n\n写入到c.txt\n\n```\nfa=open('a.txt','r')\na=fa.readlines()\nfa.close()\nfb=open('b.txt','r')\nb=fb.readlines()\nfb.close()\nc= [i for i in a if i in b]\nfc=open('c.txt','w')\nfc.writelines(c)\nfc.close()\nprint 'Done'\n```\n\n最后c.txt内容\n\n```\naaa 0\nccc 0\n```\n\n\n\n# 读取文件并绘制图片\n\n## 散点图\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    X = []\n    y = []\n    for line in inFile:\n        trainingSet = line.split()  \n        X.append(trainingSet[0])\n        y.append(trainingSet[1])\n    return (X, y)\n\n\ndef plotData(X, y):\n    length = len(y)\n    plt.figure(1)\n    #plt.plot(X, y, 'rx')\n    plt.scatter(X,y,c='r',marker='.')\n    plt.xlabel('eye_width')\n    plt.ylabel('eye_height')\n    #plt.show()\n    plt.savefig('dis.png')\n\nif __name__ == '__main__':\n    (X, y) = loadData('dis.txt')\n\n    plotData(X, y)\n```\n\n## 折线图\n\n```\nimport matplotlib.pyplot as plt \n\ny1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] \nx1=range(0,200,10) \n\nnum = 0\nfor i in range(20):\n    num+=y1[i]\nprint num\nplt.plot(x1,y1,label='Frist line',linewidth=1,color='r',marker='o', \nmarkerfacecolor='blue',markersize=6) \nplt.xlabel('eye_width distribute') \nplt.ylabel('Num') \nplt.title('Eye\\nCheck it out') \nplt.legend()\nplt.savefig('figure.png') \nplt.show() \n```\n\n\n\n# 统计文件中的数据分布\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    x_lines=inFile.readlines()#x_lines为str的list\n    x_distribute=[0]*20 #对列表元素进行重复复制\n    for x_line in x_lines:\n        x_point=x_line.split()[0]\n        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型\n        x_distribute[i]+=1\n    print x_distribute\n\nif __name__ == '__main__':\n    loadData('dis.txt')\n```\n\n# windows下安装OpenCV for Python\n\n1. Download Python, Numpy, OpenCV from their official sites.\n\n2. Extract OpenCV (will be extracted to a folder opencv)\n\n3. Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd\n\n4. Paste it in C:\\Python27\\Lib\\site-packages\n\n5. Open Python IDLE or terminal, and type\n\n   ```\n   >>> import cv2\n   ```\n\n# 读取文件，进行批量创建目录\n\n存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：\n\n```\nxxx/0.jpg\nyyy/0.jpg\nzzz/0.jpg\n```\n\n创建xxx目录，yyy目录，zzz目录\n\n```\nimport sys\nimport os\nimport numpy as np\nimport skimage\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef read_file(path):\n    with open(path) as f:\n        return list(f)\n\n\ndef make_dir(image_path):   \n    image_lines = read_file(image_path)\n    if not image_lines:\n        print 'empty file'\n        return\n    i = 0\n    for image_line in image_lines:\n        image_line = image_line.strip('\\n')\n        subdir_name = image_line.split('/')[0]\n        print subdir_name\n        isExists=os.path.exists(subdir_name)\n        if not isExists:\n            os.mkdir(subdir_name)\n            print subdir_name+\"created successfully!\"\n       \n        i = i+1\n        sys.stdout.write('\\riter %d\\n' %(i))\n        sys.stdout.flush()\n\n    print 'Done'\n\nif __name__=='__main__': \n    image_path='./image.txt'\n    make_dir(image_path)\n\n```\n\n","source":"_posts/Python.md","raw":"---\ntitle: Python\ndate: 2017-06-09 20:52:56\ncategories:\ntags: Python\ndescription: Python\nmathjax: False\n---\n\n# 图像操作\n\n## 关于PIL image和skimage的图像处理\n\n### 对skimage图像\n\n镜像处理：\n\n```\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\nimg=io.imread(\"test.jpg\")\n#img=transform.rotate(img,180)\nimg=cv2.flip(img,1)\nplt.figure('skimage')\nplt.imshow(img)\nplt.show()\nprint img.shape\nprint(img.dtype)\n```\n\n# 文本处理\n\n## 提取两个文件中相同的部分\n\na.txt内容：\n\n```\naaa 0\nbbb 0\nccc 0\n```\n\nb.txt内容：\n\n```\naaa 0\nbbb 1\nccc 0\n```\n\n### 提取相同的部分\n\n写入到c.txt\n\n```\nfa=open('a.txt','r')\na=fa.readlines()\nfa.close()\nfb=open('b.txt','r')\nb=fb.readlines()\nfb.close()\nc= [i for i in a if i in b]\nfc=open('c.txt','w')\nfc.writelines(c)\nfc.close()\nprint 'Done'\n```\n\n最后c.txt内容\n\n```\naaa 0\nccc 0\n```\n\n\n\n# 读取文件并绘制图片\n\n## 散点图\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    X = []\n    y = []\n    for line in inFile:\n        trainingSet = line.split()  \n        X.append(trainingSet[0])\n        y.append(trainingSet[1])\n    return (X, y)\n\n\ndef plotData(X, y):\n    length = len(y)\n    plt.figure(1)\n    #plt.plot(X, y, 'rx')\n    plt.scatter(X,y,c='r',marker='.')\n    plt.xlabel('eye_width')\n    plt.ylabel('eye_height')\n    #plt.show()\n    plt.savefig('dis.png')\n\nif __name__ == '__main__':\n    (X, y) = loadData('dis.txt')\n\n    plotData(X, y)\n```\n\n## 折线图\n\n```\nimport matplotlib.pyplot as plt \n\ny1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] \nx1=range(0,200,10) \n\nnum = 0\nfor i in range(20):\n    num+=y1[i]\nprint num\nplt.plot(x1,y1,label='Frist line',linewidth=1,color='r',marker='o', \nmarkerfacecolor='blue',markersize=6) \nplt.xlabel('eye_width distribute') \nplt.ylabel('Num') \nplt.title('Eye\\nCheck it out') \nplt.legend()\nplt.savefig('figure.png') \nplt.show() \n```\n\n\n\n# 统计文件中的数据分布\n\n```\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef loadData(fileName):\n    inFile=open(fileName,'r')\n    x_lines=inFile.readlines()#x_lines为str的list\n    x_distribute=[0]*20 #对列表元素进行重复复制\n    for x_line in x_lines:\n        x_point=x_line.split()[0]\n        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型\n        x_distribute[i]+=1\n    print x_distribute\n\nif __name__ == '__main__':\n    loadData('dis.txt')\n```\n\n# windows下安装OpenCV for Python\n\n1. Download Python, Numpy, OpenCV from their official sites.\n\n2. Extract OpenCV (will be extracted to a folder opencv)\n\n3. Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd\n\n4. Paste it in C:\\Python27\\Lib\\site-packages\n\n5. Open Python IDLE or terminal, and type\n\n   ```\n   >>> import cv2\n   ```\n\n# 读取文件，进行批量创建目录\n\n存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：\n\n```\nxxx/0.jpg\nyyy/0.jpg\nzzz/0.jpg\n```\n\n创建xxx目录，yyy目录，zzz目录\n\n```\nimport sys\nimport os\nimport numpy as np\nimport skimage\nfrom skimage import io,transform\nimport matplotlib.pyplot as plt\nimport cv2\n\ndef read_file(path):\n    with open(path) as f:\n        return list(f)\n\n\ndef make_dir(image_path):   \n    image_lines = read_file(image_path)\n    if not image_lines:\n        print 'empty file'\n        return\n    i = 0\n    for image_line in image_lines:\n        image_line = image_line.strip('\\n')\n        subdir_name = image_line.split('/')[0]\n        print subdir_name\n        isExists=os.path.exists(subdir_name)\n        if not isExists:\n            os.mkdir(subdir_name)\n            print subdir_name+\"created successfully!\"\n       \n        i = i+1\n        sys.stdout.write('\\riter %d\\n' %(i))\n        sys.stdout.flush()\n\n    print 'Done'\n\nif __name__=='__main__': \n    image_path='./image.txt'\n    make_dir(image_path)\n\n```\n\n","slug":"Python","published":1,"updated":"2017-06-14T01:51:24.227Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440sy000p2sc89qdz2ih2","content":"<h1 id=\"图像操作\"><a href=\"#图像操作\" class=\"headerlink\" title=\"图像操作\"></a>图像操作</h1><h2 id=\"关于PIL-image和skimage的图像处理\"><a href=\"#关于PIL-image和skimage的图像处理\" class=\"headerlink\" title=\"关于PIL image和skimage的图像处理\"></a>关于PIL image和skimage的图像处理</h2><h3 id=\"对skimage图像\"><a href=\"#对skimage图像\" class=\"headerlink\" title=\"对skimage图像\"></a>对skimage图像</h3><p>镜像处理：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">img=io.imread(&quot;test.jpg&quot;)</div><div class=\"line\">#img=transform.rotate(img,180)</div><div class=\"line\">img=cv2.flip(img,1)</div><div class=\"line\">plt.figure(&apos;skimage&apos;)</div><div class=\"line\">plt.imshow(img)</div><div class=\"line\">plt.show()</div><div class=\"line\">print img.shape</div><div class=\"line\">print(img.dtype)</div></pre></td></tr></table></figure>\n<h1 id=\"文本处理\"><a href=\"#文本处理\" class=\"headerlink\" title=\"文本处理\"></a>文本处理</h1><h2 id=\"提取两个文件中相同的部分\"><a href=\"#提取两个文件中相同的部分\" class=\"headerlink\" title=\"提取两个文件中相同的部分\"></a>提取两个文件中相同的部分</h2><p>a.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<p>b.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 1</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h3 id=\"提取相同的部分\"><a href=\"#提取相同的部分\" class=\"headerlink\" title=\"提取相同的部分\"></a>提取相同的部分</h3><p>写入到c.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">fa=open(&apos;a.txt&apos;,&apos;r&apos;)</div><div class=\"line\">a=fa.readlines()</div><div class=\"line\">fa.close()</div><div class=\"line\">fb=open(&apos;b.txt&apos;,&apos;r&apos;)</div><div class=\"line\">b=fb.readlines()</div><div class=\"line\">fb.close()</div><div class=\"line\">c= [i for i in a if i in b]</div><div class=\"line\">fc=open(&apos;c.txt&apos;,&apos;w&apos;)</div><div class=\"line\">fc.writelines(c)</div><div class=\"line\">fc.close()</div><div class=\"line\">print &apos;Done&apos;</div></pre></td></tr></table></figure>\n<p>最后c.txt内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h1 id=\"读取文件并绘制图片\"><a href=\"#读取文件并绘制图片\" class=\"headerlink\" title=\"读取文件并绘制图片\"></a>读取文件并绘制图片</h1><h2 id=\"散点图\"><a href=\"#散点图\" class=\"headerlink\" title=\"散点图\"></a>散点图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    X = []</div><div class=\"line\">    y = []</div><div class=\"line\">    for line in inFile:</div><div class=\"line\">        trainingSet = line.split()  </div><div class=\"line\">        X.append(trainingSet[0])</div><div class=\"line\">        y.append(trainingSet[1])</div><div class=\"line\">    return (X, y)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def plotData(X, y):</div><div class=\"line\">    length = len(y)</div><div class=\"line\">    plt.figure(1)</div><div class=\"line\">    #plt.plot(X, y, &apos;rx&apos;)</div><div class=\"line\">    plt.scatter(X,y,c=&apos;r&apos;,marker=&apos;.&apos;)</div><div class=\"line\">    plt.xlabel(&apos;eye_width&apos;)</div><div class=\"line\">    plt.ylabel(&apos;eye_height&apos;)</div><div class=\"line\">    #plt.show()</div><div class=\"line\">    plt.savefig(&apos;dis.png&apos;)</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    (X, y) = loadData(&apos;dis.txt&apos;)</div><div class=\"line\"></div><div class=\"line\">    plotData(X, y)</div></pre></td></tr></table></figure>\n<h2 id=\"折线图\"><a href=\"#折线图\" class=\"headerlink\" title=\"折线图\"></a>折线图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib.pyplot as plt </div><div class=\"line\"></div><div class=\"line\">y1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] </div><div class=\"line\">x1=range(0,200,10) </div><div class=\"line\"></div><div class=\"line\">num = 0</div><div class=\"line\">for i in range(20):</div><div class=\"line\">    num+=y1[i]</div><div class=\"line\">print num</div><div class=\"line\">plt.plot(x1,y1,label=&apos;Frist line&apos;,linewidth=1,color=&apos;r&apos;,marker=&apos;o&apos;, </div><div class=\"line\">markerfacecolor=&apos;blue&apos;,markersize=6) </div><div class=\"line\">plt.xlabel(&apos;eye_width distribute&apos;) </div><div class=\"line\">plt.ylabel(&apos;Num&apos;) </div><div class=\"line\">plt.title(&apos;Eye\\nCheck it out&apos;) </div><div class=\"line\">plt.legend()</div><div class=\"line\">plt.savefig(&apos;figure.png&apos;) </div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure>\n<h1 id=\"统计文件中的数据分布\"><a href=\"#统计文件中的数据分布\" class=\"headerlink\" title=\"统计文件中的数据分布\"></a>统计文件中的数据分布</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import numpy as np</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    x_lines=inFile.readlines()#x_lines为str的list</div><div class=\"line\">    x_distribute=[0]*20 #对列表元素进行重复复制</div><div class=\"line\">    for x_line in x_lines:</div><div class=\"line\">        x_point=x_line.split()[0]</div><div class=\"line\">        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型</div><div class=\"line\">        x_distribute[i]+=1</div><div class=\"line\">    print x_distribute</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    loadData(&apos;dis.txt&apos;)</div></pre></td></tr></table></figure>\n<h1 id=\"windows下安装OpenCV-for-Python\"><a href=\"#windows下安装OpenCV-for-Python\" class=\"headerlink\" title=\"windows下安装OpenCV for Python\"></a>windows下安装OpenCV for Python</h1><ol>\n<li><p>Download Python, Numpy, OpenCV from their official sites.</p>\n</li>\n<li><p>Extract OpenCV (will be extracted to a folder opencv)</p>\n</li>\n<li><p>Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd</p>\n</li>\n<li><p>Paste it in C:\\Python27\\Lib\\site-packages</p>\n</li>\n<li><p>Open Python IDLE or terminal, and type</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&gt;&gt;&gt; import cv2</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"读取文件，进行批量创建目录\"><a href=\"#读取文件，进行批量创建目录\" class=\"headerlink\" title=\"读取文件，进行批量创建目录\"></a>读取文件，进行批量创建目录</h1><p>存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">xxx/0.jpg</div><div class=\"line\">yyy/0.jpg</div><div class=\"line\">zzz/0.jpg</div></pre></td></tr></table></figure>\n<p>创建xxx目录，yyy目录，zzz目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">import sys</div><div class=\"line\">import os</div><div class=\"line\">import numpy as np</div><div class=\"line\">import skimage</div><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">def read_file(path):</div><div class=\"line\">    with open(path) as f:</div><div class=\"line\">        return list(f)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def make_dir(image_path):   </div><div class=\"line\">    image_lines = read_file(image_path)</div><div class=\"line\">    if not image_lines:</div><div class=\"line\">        print &apos;empty file&apos;</div><div class=\"line\">        return</div><div class=\"line\">    i = 0</div><div class=\"line\">    for image_line in image_lines:</div><div class=\"line\">        image_line = image_line.strip(&apos;\\n&apos;)</div><div class=\"line\">        subdir_name = image_line.split(&apos;/&apos;)[0]</div><div class=\"line\">        print subdir_name</div><div class=\"line\">        isExists=os.path.exists(subdir_name)</div><div class=\"line\">        if not isExists:</div><div class=\"line\">            os.mkdir(subdir_name)</div><div class=\"line\">            print subdir_name+&quot;created successfully!&quot;</div><div class=\"line\">       </div><div class=\"line\">        i = i+1</div><div class=\"line\">        sys.stdout.write(&apos;\\riter %d\\n&apos; %(i))</div><div class=\"line\">        sys.stdout.flush()</div><div class=\"line\"></div><div class=\"line\">    print &apos;Done&apos;</div><div class=\"line\"></div><div class=\"line\">if __name__==&apos;__main__&apos;: </div><div class=\"line\">    image_path=&apos;./image.txt&apos;</div><div class=\"line\">    make_dir(image_path)</div></pre></td></tr></table></figure>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"图像操作\"><a href=\"#图像操作\" class=\"headerlink\" title=\"图像操作\"></a>图像操作</h1><h2 id=\"关于PIL-image和skimage的图像处理\"><a href=\"#关于PIL-image和skimage的图像处理\" class=\"headerlink\" title=\"关于PIL image和skimage的图像处理\"></a>关于PIL image和skimage的图像处理</h2><h3 id=\"对skimage图像\"><a href=\"#对skimage图像\" class=\"headerlink\" title=\"对skimage图像\"></a>对skimage图像</h3><p>镜像处理：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">img=io.imread(&quot;test.jpg&quot;)</div><div class=\"line\">#img=transform.rotate(img,180)</div><div class=\"line\">img=cv2.flip(img,1)</div><div class=\"line\">plt.figure(&apos;skimage&apos;)</div><div class=\"line\">plt.imshow(img)</div><div class=\"line\">plt.show()</div><div class=\"line\">print img.shape</div><div class=\"line\">print(img.dtype)</div></pre></td></tr></table></figure>\n<h1 id=\"文本处理\"><a href=\"#文本处理\" class=\"headerlink\" title=\"文本处理\"></a>文本处理</h1><h2 id=\"提取两个文件中相同的部分\"><a href=\"#提取两个文件中相同的部分\" class=\"headerlink\" title=\"提取两个文件中相同的部分\"></a>提取两个文件中相同的部分</h2><p>a.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<p>b.txt内容：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">bbb 1</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h3 id=\"提取相同的部分\"><a href=\"#提取相同的部分\" class=\"headerlink\" title=\"提取相同的部分\"></a>提取相同的部分</h3><p>写入到c.txt</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div></pre></td><td class=\"code\"><pre><div class=\"line\">fa=open(&apos;a.txt&apos;,&apos;r&apos;)</div><div class=\"line\">a=fa.readlines()</div><div class=\"line\">fa.close()</div><div class=\"line\">fb=open(&apos;b.txt&apos;,&apos;r&apos;)</div><div class=\"line\">b=fb.readlines()</div><div class=\"line\">fb.close()</div><div class=\"line\">c= [i for i in a if i in b]</div><div class=\"line\">fc=open(&apos;c.txt&apos;,&apos;w&apos;)</div><div class=\"line\">fc.writelines(c)</div><div class=\"line\">fc.close()</div><div class=\"line\">print &apos;Done&apos;</div></pre></td></tr></table></figure>\n<p>最后c.txt内容</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">aaa 0</div><div class=\"line\">ccc 0</div></pre></td></tr></table></figure>\n<h1 id=\"读取文件并绘制图片\"><a href=\"#读取文件并绘制图片\" class=\"headerlink\" title=\"读取文件并绘制图片\"></a>读取文件并绘制图片</h1><h2 id=\"散点图\"><a href=\"#散点图\" class=\"headerlink\" title=\"散点图\"></a>散点图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    X = []</div><div class=\"line\">    y = []</div><div class=\"line\">    for line in inFile:</div><div class=\"line\">        trainingSet = line.split()  </div><div class=\"line\">        X.append(trainingSet[0])</div><div class=\"line\">        y.append(trainingSet[1])</div><div class=\"line\">    return (X, y)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def plotData(X, y):</div><div class=\"line\">    length = len(y)</div><div class=\"line\">    plt.figure(1)</div><div class=\"line\">    #plt.plot(X, y, &apos;rx&apos;)</div><div class=\"line\">    plt.scatter(X,y,c=&apos;r&apos;,marker=&apos;.&apos;)</div><div class=\"line\">    plt.xlabel(&apos;eye_width&apos;)</div><div class=\"line\">    plt.ylabel(&apos;eye_height&apos;)</div><div class=\"line\">    #plt.show()</div><div class=\"line\">    plt.savefig(&apos;dis.png&apos;)</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    (X, y) = loadData(&apos;dis.txt&apos;)</div><div class=\"line\"></div><div class=\"line\">    plotData(X, y)</div></pre></td></tr></table></figure>\n<h2 id=\"折线图\"><a href=\"#折线图\" class=\"headerlink\" title=\"折线图\"></a>折线图</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib.pyplot as plt </div><div class=\"line\"></div><div class=\"line\">y1=[14,3329,213675,451416,491919,728911,1379232,1287442,309026,85674,29481,9051,2894,932,279,86,14,6,0,0] </div><div class=\"line\">x1=range(0,200,10) </div><div class=\"line\"></div><div class=\"line\">num = 0</div><div class=\"line\">for i in range(20):</div><div class=\"line\">    num+=y1[i]</div><div class=\"line\">print num</div><div class=\"line\">plt.plot(x1,y1,label=&apos;Frist line&apos;,linewidth=1,color=&apos;r&apos;,marker=&apos;o&apos;, </div><div class=\"line\">markerfacecolor=&apos;blue&apos;,markersize=6) </div><div class=\"line\">plt.xlabel(&apos;eye_width distribute&apos;) </div><div class=\"line\">plt.ylabel(&apos;Num&apos;) </div><div class=\"line\">plt.title(&apos;Eye\\nCheck it out&apos;) </div><div class=\"line\">plt.legend()</div><div class=\"line\">plt.savefig(&apos;figure.png&apos;) </div><div class=\"line\">plt.show()</div></pre></td></tr></table></figure>\n<h1 id=\"统计文件中的数据分布\"><a href=\"#统计文件中的数据分布\" class=\"headerlink\" title=\"统计文件中的数据分布\"></a>统计文件中的数据分布</h1><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div></pre></td><td class=\"code\"><pre><div class=\"line\">import matplotlib</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import numpy as np</div><div class=\"line\"></div><div class=\"line\">def loadData(fileName):</div><div class=\"line\">    inFile=open(fileName,&apos;r&apos;)</div><div class=\"line\">    x_lines=inFile.readlines()#x_lines为str的list</div><div class=\"line\">    x_distribute=[0]*20 #对列表元素进行重复复制</div><div class=\"line\">    for x_line in x_lines:</div><div class=\"line\">        x_point=x_line.split()[0]</div><div class=\"line\">        i=np.int(np.float32(x_point)/10)  #注意str要先转化为np.float才能转化为int型</div><div class=\"line\">        x_distribute[i]+=1</div><div class=\"line\">    print x_distribute</div><div class=\"line\"></div><div class=\"line\">if __name__ == &apos;__main__&apos;:</div><div class=\"line\">    loadData(&apos;dis.txt&apos;)</div></pre></td></tr></table></figure>\n<h1 id=\"windows下安装OpenCV-for-Python\"><a href=\"#windows下安装OpenCV-for-Python\" class=\"headerlink\" title=\"windows下安装OpenCV for Python\"></a>windows下安装OpenCV for Python</h1><ol>\n<li><p>Download Python, Numpy, OpenCV from their official sites.</p>\n</li>\n<li><p>Extract OpenCV (will be extracted to a folder opencv)</p>\n</li>\n<li><p>Copy ..\\opencv\\build\\python\\x86\\2.7\\cv2.pyd</p>\n</li>\n<li><p>Paste it in C:\\Python27\\Lib\\site-packages</p>\n</li>\n<li><p>Open Python IDLE or terminal, and type</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">&gt;&gt;&gt; import cv2</div></pre></td></tr></table></figure>\n</li>\n</ol>\n<h1 id=\"读取文件，进行批量创建目录\"><a href=\"#读取文件，进行批量创建目录\" class=\"headerlink\" title=\"读取文件，进行批量创建目录\"></a>读取文件，进行批量创建目录</h1><p>存在一个image.txt里面每一行都是目录/文件名，要提取目录名，并由此创建新目录，内容如下：</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div></pre></td><td class=\"code\"><pre><div class=\"line\">xxx/0.jpg</div><div class=\"line\">yyy/0.jpg</div><div class=\"line\">zzz/0.jpg</div></pre></td></tr></table></figure>\n<p>创建xxx目录，yyy目录，zzz目录</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div><div class=\"line\">28</div><div class=\"line\">29</div><div class=\"line\">30</div><div class=\"line\">31</div><div class=\"line\">32</div><div class=\"line\">33</div><div class=\"line\">34</div><div class=\"line\">35</div><div class=\"line\">36</div><div class=\"line\">37</div></pre></td><td class=\"code\"><pre><div class=\"line\">import sys</div><div class=\"line\">import os</div><div class=\"line\">import numpy as np</div><div class=\"line\">import skimage</div><div class=\"line\">from skimage import io,transform</div><div class=\"line\">import matplotlib.pyplot as plt</div><div class=\"line\">import cv2</div><div class=\"line\"></div><div class=\"line\">def read_file(path):</div><div class=\"line\">    with open(path) as f:</div><div class=\"line\">        return list(f)</div><div class=\"line\"></div><div class=\"line\"></div><div class=\"line\">def make_dir(image_path):   </div><div class=\"line\">    image_lines = read_file(image_path)</div><div class=\"line\">    if not image_lines:</div><div class=\"line\">        print &apos;empty file&apos;</div><div class=\"line\">        return</div><div class=\"line\">    i = 0</div><div class=\"line\">    for image_line in image_lines:</div><div class=\"line\">        image_line = image_line.strip(&apos;\\n&apos;)</div><div class=\"line\">        subdir_name = image_line.split(&apos;/&apos;)[0]</div><div class=\"line\">        print subdir_name</div><div class=\"line\">        isExists=os.path.exists(subdir_name)</div><div class=\"line\">        if not isExists:</div><div class=\"line\">            os.mkdir(subdir_name)</div><div class=\"line\">            print subdir_name+&quot;created successfully!&quot;</div><div class=\"line\">       </div><div class=\"line\">        i = i+1</div><div class=\"line\">        sys.stdout.write(&apos;\\riter %d\\n&apos; %(i))</div><div class=\"line\">        sys.stdout.flush()</div><div class=\"line\"></div><div class=\"line\">    print &apos;Done&apos;</div><div class=\"line\"></div><div class=\"line\">if __name__==&apos;__main__&apos;: </div><div class=\"line\">    image_path=&apos;./image.txt&apos;</div><div class=\"line\">    make_dir(image_path)</div></pre></td></tr></table></figure>\n"},{"title":"杂知识点","date":"2017-06-01T01:42:05.000Z","mathjax":true,"_content":"\n# 分类与回归\n\n**本节部分转载于穆文发表于知乎的[分类与回归区别是什么](https://www.zhihu.com/question/21329754/answer/151216012)下面的回答，获得原作者授权**\n\n*分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。*\n\n1. Logistic Regression&Linear Regression:\n\n   + Linear Regression:输出一个标量**wx+b**，这个值是连续值，用以回归问题\n   + Logistic Regression:将上面的**wx+b**通过**sigmoid**函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题\n   + 对于N分类问题，可以先计算N组w值不同的**wx+b** ，然后归一化，比如**softmax**函数变成N个类上的概率，用以多分类\n\n2. SVR &SVM\n\n   + SVR:输出**wx+b**，即某个样本点到分类面的距离，是连续值，属于回归问题\n   + SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类\n\n3. Naive Bayes用来分类和回归\n\n4. 前馈神经网络（CNN系列）用于分类和回归\n\n   + 回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出看做向量v，现全部连接到一个神经元上，这个神经元的输出**wv+b**，是一个连续值，处理回归问题，和Linear Regression的思想一样\n   + 分类：将m个神经元最后连接到N个神经元，有N组不同的**wv+b**，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题\n\n5. 循环神经网络（RNN系列）用于分类和回归\n\n   + 回归和分类与CNN类似，输出层的值**y=wx+b**，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列\n\n6. Logistic回归&SVM\n\n    两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。[^1]\n\n    线性模型的表达式为\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\n$$\n\n​\t将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。\n\n​\t用n表示Feature数量,m表示训练集个数。下面分情况讨论[^2]：\n\n- n很大，m很小\n  n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择`线性kernel的SVM`，也可以选择`Logistic回归`。\n- n很小，m中等 \n  n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择`非线性kernel的SVM`，比如`高斯核kernel的SVM`。\n- n很小，m很大\n  n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，`带核函数的SVM`计算也非常慢。所以此时应该选`线性kernel的SVM`，也可以选择`Logistic回归`。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。\n\n\n\n\n\n\n\n# 一些概念\n\n## 迁移学习[^3]：\n\n有监督预训练(*supervised pre-training*)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。\n\n+ **NMS(非极大值抑制)：**\n\n在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列[算法](http://lib.csdn.net/base/datastructure)中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。\n\n定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。\n(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;\n(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。\n(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。\n就这样一直重复，找到所有被保留下来的矩形框。\n\n+ **IoU(交并比)：**\n\n物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们[算法](http://lib.csdn.net/base/datastructure)不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式\n$$\nIoU=\\frac{A\\cap B}{A\\cup B}\n$$\n\n## 准确率&精确率&召回率:\n\n​\t_准确率是正确预测的样本占总的预测样本比例_\n​\t*精确率是预测为正的样本中有多少是真的正类*\n​\t*召回率是样本中有多少正例被正确的预测*\n​\t_F值=准确率\\*召回率\\*2/(准确率+召回率)，是准确率和召回率的调和平均值_\n\n​*TP*：正类被预测为正类\n​*FN*：正类被预测为负类\n​*FP*：负类被预测为正类\n​*TN*：负类被预测为负类\n\n$$\n准确率=\\frac{TP+TN}{TP+TF+FN+FP}\n$$\n\n$$\n精确率=\\frac{TP}{TP+FP}\n$$\n\n$$\n召回率=\\frac{TP}{TP+FN}\n$$\n\n## 卷积计算后的图片尺寸：\n\n$$\noutputsize=\\frac{imagesize+2*padding-kernelsize}{stride}+1\n$$\n\n# RankBoost:\n\n​\tRankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类\t器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1>r2>r3>r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题\n\n\n\n# 真阳率，假阳率，AUC，ROC\n\n![pic1](http://img.blog.csdn.net/20150919111349931)\n\n$真阳率=\\frac{a}{a+c}$:含义是检测出来的真阳性样本数除以所有真实阳性样本数。\n\n$假阳率=\\frac{b}{b+d}$:含义是检测出来的假阳性样本数除以所有真实阴性样本数\n\nROC曲线就是把假阳率当x轴，真阳率当y轴画一个二维平面直角坐标系。然后不断调整检测方法（或机器学习中的分类器）的阈值，即最终得分高于某个值就是阳性，反之就是阴性，得到不同的真阳率和假阳率数值，然后描点。就可以得到一条ROC曲线。 \n需要注意的是，ROC曲线必定起于（0，0），止于（1，1）。因为，当全都判断为阴性(-)时，就是（0，0）；全部判断为阳性(+)时就是（1，1）。这两点间斜率为1的线段表示随机分类器（对真实的正负样本没有区分能力）。所以一般分类器需要在这条线上方\n\n![pic2](http://img.blog.csdn.net/20150919114145488)\n\nAUC就是ROC曲线下方的面积，越接近1表示分类器越好。\n\n# 参考文献\n\n[^1]: [SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n[^2]: [SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n[^3]: [[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n\n[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n\n[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n[机器学习算法常用指标总结](http://www.cnblogs.com/maybe2030/p/5375175.html)\n\n\n\n","source":"_posts/杂知识点.md","raw":"---\ntitle: 杂知识点\ndate: 2017-06-01 09:42:05\ncategories: 深度学习\ntags: [深度学习,神经网络]\nmathjax: true\n---\n\n# 分类与回归\n\n**本节部分转载于穆文发表于知乎的[分类与回归区别是什么](https://www.zhihu.com/question/21329754/answer/151216012)下面的回答，获得原作者授权**\n\n*分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。*\n\n1. Logistic Regression&Linear Regression:\n\n   + Linear Regression:输出一个标量**wx+b**，这个值是连续值，用以回归问题\n   + Logistic Regression:将上面的**wx+b**通过**sigmoid**函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题\n   + 对于N分类问题，可以先计算N组w值不同的**wx+b** ，然后归一化，比如**softmax**函数变成N个类上的概率，用以多分类\n\n2. SVR &SVM\n\n   + SVR:输出**wx+b**，即某个样本点到分类面的距离，是连续值，属于回归问题\n   + SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类\n\n3. Naive Bayes用来分类和回归\n\n4. 前馈神经网络（CNN系列）用于分类和回归\n\n   + 回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出看做向量v，现全部连接到一个神经元上，这个神经元的输出**wv+b**，是一个连续值，处理回归问题，和Linear Regression的思想一样\n   + 分类：将m个神经元最后连接到N个神经元，有N组不同的**wv+b**，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题\n\n5. 循环神经网络（RNN系列）用于分类和回归\n\n   + 回归和分类与CNN类似，输出层的值**y=wx+b**，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列\n\n6. Logistic回归&SVM\n\n    两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。[^1]\n\n    线性模型的表达式为\n\n$$\nh_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n\n$$\n\n​\t将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。\n\n​\t用n表示Feature数量,m表示训练集个数。下面分情况讨论[^2]：\n\n- n很大，m很小\n  n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择`线性kernel的SVM`，也可以选择`Logistic回归`。\n- n很小，m中等 \n  n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择`非线性kernel的SVM`，比如`高斯核kernel的SVM`。\n- n很小，m很大\n  n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，`带核函数的SVM`计算也非常慢。所以此时应该选`线性kernel的SVM`，也可以选择`Logistic回归`。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。\n\n\n\n\n\n\n\n# 一些概念\n\n## 迁移学习[^3]：\n\n有监督预训练(*supervised pre-training*)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。\n\n+ **NMS(非极大值抑制)：**\n\n在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列[算法](http://lib.csdn.net/base/datastructure)中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。\n\n定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。\n(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;\n(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。\n(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。\n就这样一直重复，找到所有被保留下来的矩形框。\n\n+ **IoU(交并比)：**\n\n物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们[算法](http://lib.csdn.net/base/datastructure)不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式\n$$\nIoU=\\frac{A\\cap B}{A\\cup B}\n$$\n\n## 准确率&精确率&召回率:\n\n​\t_准确率是正确预测的样本占总的预测样本比例_\n​\t*精确率是预测为正的样本中有多少是真的正类*\n​\t*召回率是样本中有多少正例被正确的预测*\n​\t_F值=准确率\\*召回率\\*2/(准确率+召回率)，是准确率和召回率的调和平均值_\n\n​*TP*：正类被预测为正类\n​*FN*：正类被预测为负类\n​*FP*：负类被预测为正类\n​*TN*：负类被预测为负类\n\n$$\n准确率=\\frac{TP+TN}{TP+TF+FN+FP}\n$$\n\n$$\n精确率=\\frac{TP}{TP+FP}\n$$\n\n$$\n召回率=\\frac{TP}{TP+FN}\n$$\n\n## 卷积计算后的图片尺寸：\n\n$$\noutputsize=\\frac{imagesize+2*padding-kernelsize}{stride}+1\n$$\n\n# RankBoost:\n\n​\tRankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类\t器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1>r2>r3>r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题\n\n\n\n# 真阳率，假阳率，AUC，ROC\n\n![pic1](http://img.blog.csdn.net/20150919111349931)\n\n$真阳率=\\frac{a}{a+c}$:含义是检测出来的真阳性样本数除以所有真实阳性样本数。\n\n$假阳率=\\frac{b}{b+d}$:含义是检测出来的假阳性样本数除以所有真实阴性样本数\n\nROC曲线就是把假阳率当x轴，真阳率当y轴画一个二维平面直角坐标系。然后不断调整检测方法（或机器学习中的分类器）的阈值，即最终得分高于某个值就是阳性，反之就是阴性，得到不同的真阳率和假阳率数值，然后描点。就可以得到一条ROC曲线。 \n需要注意的是，ROC曲线必定起于（0，0），止于（1，1）。因为，当全都判断为阴性(-)时，就是（0，0）；全部判断为阳性(+)时就是（1，1）。这两点间斜率为1的线段表示随机分类器（对真实的正负样本没有区分能力）。所以一般分类器需要在这条线上方\n\n![pic2](http://img.blog.csdn.net/20150919114145488)\n\nAUC就是ROC曲线下方的面积，越接近1表示分类器越好。\n\n# 参考文献\n\n[^1]: [SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n[^2]: [SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n[^3]: [[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\n\n[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\n\n[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\n\n[机器学习算法常用指标总结](http://www.cnblogs.com/maybe2030/p/5375175.html)\n\n\n\n","slug":"杂知识点","published":1,"updated":"2017-09-11T03:24:37.234Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440t4000q2sc80h4mg7jy","content":"<h1 id=\"分类与回归\"><a href=\"#分类与回归\" class=\"headerlink\" title=\"分类与回归\"></a>分类与回归</h1><p><strong>本节部分转载于穆文发表于知乎的<a href=\"https://www.zhihu.com/question/21329754/answer/151216012\" target=\"_blank\" rel=\"external\">分类与回归区别是什么</a>下面的回答，获得原作者授权</strong></p>\n<p><em>分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。</em></p>\n<ol>\n<li><p>Logistic Regression&amp;Linear Regression:</p>\n<ul>\n<li>Linear Regression:输出一个标量<strong>wx+b</strong>，这个值是连续值，用以回归问题</li>\n<li>Logistic Regression:将上面的<strong>wx+b</strong>通过<strong>sigmoid</strong>函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题</li>\n<li>对于N分类问题，可以先计算N组w值不同的<strong>wx+b</strong> ，然后归一化，比如<strong>softmax</strong>函数变成N个类上的概率，用以多分类</li>\n</ul>\n</li>\n<li><p>SVR &amp;SVM</p>\n<ul>\n<li>SVR:输出<strong>wx+b</strong>，即某个样本点到分类面的距离，是连续值，属于回归问题</li>\n<li>SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类</li>\n</ul>\n</li>\n<li><p>Naive Bayes用来分类和回归</p>\n</li>\n<li><p>前馈神经网络（CNN系列）用于分类和回归</p>\n<ul>\n<li>回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出看做向量v，现全部连接到一个神经元上，这个神经元的输出<strong>wv+b</strong>，是一个连续值，处理回归问题，和Linear Regression的思想一样</li>\n<li>分类：将m个神经元最后连接到N个神经元，有N组不同的<strong>wv+b</strong>，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题</li>\n</ul>\n</li>\n<li><p>循环神经网络（RNN系列）用于分类和回归</p>\n<ul>\n<li>回归和分类与CNN类似，输出层的值<strong>y=wx+b</strong>，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列</li>\n</ul>\n</li>\n<li><p>Logistic回归&amp;SVM</p>\n<p> 两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。<a href=\"[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\">^1</a></p>\n<p> 线性模型的表达式为</p>\n</li>\n</ol>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n<br>$$</p>\n<p>​    将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。</p>\n<p>​    用n表示Feature数量,m表示训练集个数。下面分情况讨论<a href=\"[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\">^2</a>：</p>\n<ul>\n<li>n很大，m很小<br>n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。</li>\n<li>n很小，m中等<br>n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择<code>非线性kernel的SVM</code>，比如<code>高斯核kernel的SVM</code>。</li>\n<li>n很小，m很大<br>n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，<code>带核函数的SVM</code>计算也非常慢。所以此时应该选<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。</li>\n</ul>\n<h1 id=\"一些概念\"><a href=\"#一些概念\" class=\"headerlink\" title=\"一些概念\"></a>一些概念</h1><h2 id=\"迁移学习-3：\"><a href=\"#迁移学习-3：\" class=\"headerlink\" title=\"迁移学习^3：\"></a>迁移学习<a href=\"[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\">^3</a>：</h2><p>有监督预训练(<em>supervised pre-training</em>)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。</p>\n<ul>\n<li><strong>NMS(非极大值抑制)：</strong></li>\n</ul>\n<p>在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。</p>\n<p>定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。<br>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;<br>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。<br>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。<br>就这样一直重复，找到所有被保留下来的矩形框。</p>\n<ul>\n<li><strong>IoU(交并比)：</strong></li>\n</ul>\n<p>物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式<br>$$<br>IoU=\\frac{A\\cap B}{A\\cup B}<br>$$</p>\n<h2 id=\"准确率-amp-精确率-amp-召回率\"><a href=\"#准确率-amp-精确率-amp-召回率\" class=\"headerlink\" title=\"准确率&amp;精确率&amp;召回率:\"></a>准确率&amp;精确率&amp;召回率:</h2><p>​    <em>准确率是正确预测的样本占总的预测样本比例</em><br>​    <em>精确率是预测为正的样本中有多少是真的正类</em><br>​    <em>召回率是样本中有多少正例被正确的预测</em><br>​    <em>F值=准确率*召回率*2/(准确率+召回率)，是准确率和召回率的调和平均值</em></p>\n<p>​<em>TP</em>：正类被预测为正类<br>​<em>FN</em>：正类被预测为负类<br>​<em>FP</em>：负类被预测为正类<br>​<em>TN</em>：负类被预测为负类</p>\n<p>$$<br>准确率=\\frac{TP+TN}{TP+TF+FN+FP}<br>$$</p>\n<p>$$<br>精确率=\\frac{TP}{TP+FP}<br>$$</p>\n<p>$$<br>召回率=\\frac{TP}{TP+FN}<br>$$</p>\n<h2 id=\"卷积计算后的图片尺寸：\"><a href=\"#卷积计算后的图片尺寸：\" class=\"headerlink\" title=\"卷积计算后的图片尺寸：\"></a>卷积计算后的图片尺寸：</h2><p>$$<br>outputsize=\\frac{imagesize+2*padding-kernelsize}{stride}+1<br>$$</p>\n<h1 id=\"RankBoost\"><a href=\"#RankBoost\" class=\"headerlink\" title=\"RankBoost:\"></a>RankBoost:</h1><p>​    RankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类    器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1&gt;r2&gt;r3&gt;r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题</p>\n<h1 id=\"真阳率，假阳率，AUC，ROC\"><a href=\"#真阳率，假阳率，AUC，ROC\" class=\"headerlink\" title=\"真阳率，假阳率，AUC，ROC\"></a>真阳率，假阳率，AUC，ROC</h1><p><img src=\"http://img.blog.csdn.net/20150919111349931\" alt=\"pic1\"></p>\n<p>$真阳率=\\frac{a}{a+c}$:含义是检测出来的真阳性样本数除以所有真实阳性样本数。</p>\n<p>$假阳率=\\frac{b}{b+d}$:含义是检测出来的假阳性样本数除以所有真实阴性样本数</p>\n<p>ROC曲线就是把假阳率当x轴，真阳率当y轴画一个二维平面直角坐标系。然后不断调整检测方法（或机器学习中的分类器）的阈值，即最终得分高于某个值就是阳性，反之就是阴性，得到不同的真阳率和假阳率数值，然后描点。就可以得到一条ROC曲线。<br>需要注意的是，ROC曲线必定起于（0，0），止于（1，1）。因为，当全都判断为阴性(-)时，就是（0，0）；全部判断为阳性(+)时就是（1，1）。这两点间斜率为1的线段表示随机分类器（对真实的正负样本没有区分能力）。所以一般分类器需要在这条线上方</p>\n<p><img src=\"http://img.blog.csdn.net/20150919114145488\" alt=\"pic2\"></p>\n<p>AUC就是ROC曲线下方的面积，越接近1表示分类器越好。</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p><a href=\"https://www.zhihu.com/question/21704547/answer/20293255\" target=\"_blank\" rel=\"external\">SVM和logistic回归分别在什么情况下使用</a></p>\n<p><a href=\"http://blog.csdn.net/ybdesire/article/details/54143481\" target=\"_blank\" rel=\"external\">SVM和Logistic的区别</a></p>\n<p>[<a href=\"http://blog.csdn.net/zhang_shuai12/article/details/52716952\" target=\"_blank\" rel=\"external\">物体检测中常用的几个概念迁移学习、IOU、NMS理解</a>]</p>\n<p><a href=\"http://www.cnblogs.com/maybe2030/p/5375175.html\" target=\"_blank\" rel=\"external\">机器学习算法常用指标总结</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"分类与回归\"><a href=\"#分类与回归\" class=\"headerlink\" title=\"分类与回归\"></a>分类与回归</h1><p><strong>本节部分转载于穆文发表于知乎的<a href=\"https://www.zhihu.com/question/21329754/answer/151216012\" target=\"_blank\" rel=\"external\">分类与回归区别是什么</a>下面的回答，获得原作者授权</strong></p>\n<p><em>分类与回归的模型本质一样，分类模型可将回归模型的输出离散化，回归模型也可将分类模型的输出连续化。</em></p>\n<ol>\n<li><p>Logistic Regression&amp;Linear Regression:</p>\n<ul>\n<li>Linear Regression:输出一个标量<strong>wx+b</strong>，这个值是连续值，用以回归问题</li>\n<li>Logistic Regression:将上面的<strong>wx+b</strong>通过<strong>sigmoid</strong>函数映射到(0,1)上，划分阈值，大于阈值的分为一类，小于的分为另一类，用以处理二分类的问题</li>\n<li>对于N分类问题，可以先计算N组w值不同的<strong>wx+b</strong> ，然后归一化，比如<strong>softmax</strong>函数变成N个类上的概率，用以多分类</li>\n</ul>\n</li>\n<li><p>SVR &amp;SVM</p>\n<ul>\n<li>SVR:输出<strong>wx+b</strong>，即某个样本点到分类面的距离，是连续值，属于回归问题</li>\n<li>SVM：将SVR的距离用sign(.)函数作用，距离为正的样本点属于一类，为负的属于另一类</li>\n</ul>\n</li>\n<li><p>Naive Bayes用来分类和回归</p>\n</li>\n<li><p>前馈神经网络（CNN系列）用于分类和回归</p>\n<ul>\n<li>回归：最后一层有m个神经元，每个神经元输出一个标量，m个神经元的输出看做向量v，现全部连接到一个神经元上，这个神经元的输出<strong>wv+b</strong>，是一个连续值，处理回归问题，和Linear Regression的思想一样</li>\n<li>分类：将m个神经元最后连接到N个神经元，有N组不同的<strong>wv+b</strong>，进行归一化（比如softmax)，就变成N个类上的概率，如果不用softmax，而是每个wx+b用一个sigmoid，就变成多标签问题</li>\n</ul>\n</li>\n<li><p>循环神经网络（RNN系列）用于分类和回归</p>\n<ul>\n<li>回归和分类与CNN类似，输出层的值<strong>y=wx+b</strong>，可做分类和回归，区别在于，RNN的输出和时间有关，即输出的是{y(t),y(t+1),..}序列</li>\n</ul>\n</li>\n<li><p>Logistic回归&amp;SVM</p>\n<p> 两种方法都是常见的分类算法，从目标函数来看，区别在于逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。此外，根据需要，两个方法都可以增加不同的正则化项，如l1,l2等等。所以在很多实验中，两种算法的结果是很接近的。<a href=\"[SVM和logistic回归分别在什么情况下使用](https://www.zhihu.com/question/21704547/answer/20293255)\">^1</a></p>\n<p> 线性模型的表达式为</p>\n</li>\n</ol>\n<p>$$<br>h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+…+\\theta_nx_n<br>$$</p>\n<p>​    将线性模型的输出送给sigmoid函数，就得到logistic回归模型；将线性模型表达式中的xi换为fi，就得到SVM模型的表达式。其中fi是xi的核函数，也就是xi的非线性多项式，例如f1=x1*x2，所以线性kernel的SVM(fi=xi)，在选择和使用上跟logistic回归没有区别。</p>\n<p>​    用n表示Feature数量,m表示训练集个数。下面分情况讨论<a href=\"[SVM和Logistic的区别](http://blog.csdn.net/ybdesire/article/details/54143481)\">^2</a>：</p>\n<ul>\n<li>n很大，m很小<br>n很大，一般指n=10000；m很小，一般m=10-1000。m很小，说明没有足够的训练集来拟合非常复杂的非线性模型，所以这种情况既可以选择<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。</li>\n<li>n很小，m中等<br>n很小，一般指n=1-1000；m很小，一般m=1000-10000。m中等，说明有足够的训练集来拟合非常复杂的非线性模型，此时适合选择<code>非线性kernel的SVM</code>，比如<code>高斯核kernel的SVM</code>。</li>\n<li>n很小，m很大<br>n很小，一般指n=1-1000；m很大，一般m=50000-1000000。m很大，说明有足够的训练集来拟合非常复杂的非线性模型，但m很大的情况下，<code>带核函数的SVM</code>计算也非常慢。所以此时应该选<code>线性kernel的SVM</code>，也可以选择<code>Logistic回归</code>。n很小，说明Feature可能不足以表达模型，所以要添加更多Feature。</li>\n</ul>\n<h1 id=\"一些概念\"><a href=\"#一些概念\" class=\"headerlink\" title=\"一些概念\"></a>一些概念</h1><h2 id=\"迁移学习-3：\"><a href=\"#迁移学习-3：\" class=\"headerlink\" title=\"迁移学习^3：\"></a>迁移学习<a href=\"[[物体检测中常用的几个概念迁移学习、IOU、NMS理解](http://blog.csdn.net/zhang_shuai12/article/details/52716952)]\">^3</a>：</h2><p>有监督预训练(<em>supervised pre-training</em>)，把一个任务训练好的参数拿到另一个任务作为神经网络的初始参数值。</p>\n<ul>\n<li><strong>NMS(非极大值抑制)：</strong></li>\n</ul>\n<p>在物体检测中NMS（Non-maximum suppression）非极大抑制应用十分广泛，其目的是为了消除多余的框，找到最佳的物体检测的位置。在RCNN系列<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>中，会从一张图片中找出很多个候选框（可能包含物体的矩形边框），然后为每个矩形框为做类别分类概率。非极大值抑制（NMS）非极大值抑制顾名思义就是抑制不是极大值的元素，搜索局部的极大值。例如在对象检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分类及分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用到NMS来选取那些邻域里分数最高（是某类对象的概率最大），并且抑制那些分数低的窗口。</p>\n<p>定位一个车辆，最后算法就找出了一堆的方框，我们需要判别哪些矩形框是没用的。 所谓非极大值抑制：先假设有6个矩形框，根据分类器类别分类概率做排序，从小到大分别属于车辆的概率分别为A、B、C、D、E、F。<br>(1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;<br>(2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。<br>(3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。<br>就这样一直重复，找到所有被保留下来的矩形框。</p>\n<ul>\n<li><strong>IoU(交并比)：</strong></li>\n</ul>\n<p>物体检测需要定位出物体的bounding box，比如车辆检查中，我们不仅要定位出车辆的bounding box ，还要识别出bounding box 里面的物体就是车辆。对于bounding box的定位精度，有一个很重要的概念，因为我们<a href=\"http://lib.csdn.net/base/datastructure\" target=\"_blank\" rel=\"external\">算法</a>不可能百分百跟人工标注的数据完全匹配，因此就存在一个定位精度评价公式：IOU。IOU表示了bounding box 与 ground truth 的重叠度。即IoU是定位精度的评价公式<br>$$<br>IoU=\\frac{A\\cap B}{A\\cup B}<br>$$</p>\n<h2 id=\"准确率-amp-精确率-amp-召回率\"><a href=\"#准确率-amp-精确率-amp-召回率\" class=\"headerlink\" title=\"准确率&amp;精确率&amp;召回率:\"></a>准确率&amp;精确率&amp;召回率:</h2><p>​    <em>准确率是正确预测的样本占总的预测样本比例</em><br>​    <em>精确率是预测为正的样本中有多少是真的正类</em><br>​    <em>召回率是样本中有多少正例被正确的预测</em><br>​    <em>F值=准确率*召回率*2/(准确率+召回率)，是准确率和召回率的调和平均值</em></p>\n<p>​<em>TP</em>：正类被预测为正类<br>​<em>FN</em>：正类被预测为负类<br>​<em>FP</em>：负类被预测为正类<br>​<em>TN</em>：负类被预测为负类</p>\n<p>$$<br>准确率=\\frac{TP+TN}{TP+TF+FN+FP}<br>$$</p>\n<p>$$<br>精确率=\\frac{TP}{TP+FP}<br>$$</p>\n<p>$$<br>召回率=\\frac{TP}{TP+FN}<br>$$</p>\n<h2 id=\"卷积计算后的图片尺寸：\"><a href=\"#卷积计算后的图片尺寸：\" class=\"headerlink\" title=\"卷积计算后的图片尺寸：\"></a>卷积计算后的图片尺寸：</h2><p>$$<br>outputsize=\\frac{imagesize+2*padding-kernelsize}{stride}+1<br>$$</p>\n<h1 id=\"RankBoost\"><a href=\"#RankBoost\" class=\"headerlink\" title=\"RankBoost:\"></a>RankBoost:</h1><p>​    RankBoost的思想比较简单，是二元Learning to rank的常规思路：通过构造目标分类    器，使得pair之间的对象存在相对大小关系。通俗点说，把对象组成一对对的pair，比如一组排序r1&gt;r2&gt;r3&gt;r4，那可以构成pair：(r1,r2)(r1,r3),(r1,r4),(r2,r3)(r3,r4),这样的pair是正值，也就是label是1；而余下的pair如(r2,r1)的值应该是-1或0。这样一个排序问题就被巧妙的转换为了分类问题。近来CV界很多又用这种learning to rank的思想做识别问题，先把识别转换为排序问题再转换为分类问题</p>\n<h1 id=\"真阳率，假阳率，AUC，ROC\"><a href=\"#真阳率，假阳率，AUC，ROC\" class=\"headerlink\" title=\"真阳率，假阳率，AUC，ROC\"></a>真阳率，假阳率，AUC，ROC</h1><p><img src=\"http://img.blog.csdn.net/20150919111349931\" alt=\"pic1\"></p>\n<p>$真阳率=\\frac{a}{a+c}$:含义是检测出来的真阳性样本数除以所有真实阳性样本数。</p>\n<p>$假阳率=\\frac{b}{b+d}$:含义是检测出来的假阳性样本数除以所有真实阴性样本数</p>\n<p>ROC曲线就是把假阳率当x轴，真阳率当y轴画一个二维平面直角坐标系。然后不断调整检测方法（或机器学习中的分类器）的阈值，即最终得分高于某个值就是阳性，反之就是阴性，得到不同的真阳率和假阳率数值，然后描点。就可以得到一条ROC曲线。<br>需要注意的是，ROC曲线必定起于（0，0），止于（1，1）。因为，当全都判断为阴性(-)时，就是（0，0）；全部判断为阳性(+)时就是（1，1）。这两点间斜率为1的线段表示随机分类器（对真实的正负样本没有区分能力）。所以一般分类器需要在这条线上方</p>\n<p><img src=\"http://img.blog.csdn.net/20150919114145488\" alt=\"pic2\"></p>\n<p>AUC就是ROC曲线下方的面积，越接近1表示分类器越好。</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><p><a href=\"https://www.zhihu.com/question/21704547/answer/20293255\" target=\"_blank\" rel=\"external\">SVM和logistic回归分别在什么情况下使用</a></p>\n<p><a href=\"http://blog.csdn.net/ybdesire/article/details/54143481\" target=\"_blank\" rel=\"external\">SVM和Logistic的区别</a></p>\n<p>[<a href=\"http://blog.csdn.net/zhang_shuai12/article/details/52716952\" target=\"_blank\" rel=\"external\">物体检测中常用的几个概念迁移学习、IOU、NMS理解</a>]</p>\n<p><a href=\"http://www.cnblogs.com/maybe2030/p/5375175.html\" target=\"_blank\" rel=\"external\">机器学习算法常用指标总结</a></p>\n"},{"title":"行人检测","date":"2017-09-21T11:46:55.000Z","description":null,"mathjax":null,"_content":"","source":"_posts/行人检测.md","raw":"---\ntitle: 行人检测\ndate: 2017-09-21 19:46:55\ncategories:\ntags:\ndescription:\nmathjax:\n---\n","slug":"行人检测","published":1,"updated":"2017-09-21T11:46:56.052Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440t9000r2sc86ltvk6iy","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Center loss笔记","date":"2017-06-29T09:05:51.000Z","description":null,"mathjax":null,"_content":"\n论文：[A Discriminative Feature Learning Approach for Deep Face Recognition](https://link.springer.com/chapter/10.1007%2F978-3-319-46478-7_31)\n\n# 摘要\n\n对于一般的CNN网络，softmax通常作为监督信号来训练深层网络，为了增强提取的特征的可辨别性（discriminative），提出center loss，应对人脸识别任务。center loss同时学习每个类别的深层特征中心和惩罚深层特征和它们对应的类中心的距离（the center loss simultaneously learns a center for deep\nfeatures of each class and penalizes the distances between the deep features and their corresponding class centers）。将softmax和center loss联合起来，可以训练一个健壮的CNN来获取深层特征的两个关键目标，类内紧凑和类间分散。\n\n# 介绍\n\n预先收集所有可能的测试身份用于训练是不现实的，所以CNN的标签预测并不总是适用的。经过深层网络获取得到的特征不仅需要可分开性(separable)，更需要识别性(discriminative)和广义性，足够用来识别新的没有遇见过的类。可识别性的特征可以利用最邻近(NN)或k-NN算法很好的分类，就没有必要依赖标签预测了。但是softmax只能产生可分开(separable)特征，结果特征就不足以用以人脸识别。\n\n![Separabale Feature Vs Discriminative Feature](https://static.leiphone.com/uploads/new/article/740_740/201612/585bb8742235c.png?imageMogr2/format/jpg/quality/90)\n\n\n\n因为随机梯度下降(SGD)优化CNN是基于mini-batch，不能很好的反映深层特征的全局分布，由于训练集的庞大，在每次迭代中输入所有的训练样本是不现实的。constractive loss和triplet loss分别作为图像对和三元组的loss函数。然而，与图像样本相比，训练图像对或三元组的数量显著增长，导致收敛缓慢和不稳定性。仔细选择图像对或者三元组，问题可能会部分缓解，但是它增加了计算复杂度，训练过程变得不方便。为了解决这个问题，提出center loss，用于有效的增强特征的可识别性，我们将会得到每个类的深层特征的中心。在训练阶段，我们同时更新中心和最小化特征和它们相应的类中心的距离。CNN同时在softmax loss和center loss的监督下进行训练，通过一个超参来平衡这两个监督信号。直觉上，softmax loss将不同类别的特征分开，center loss有效的将同一类别的特征拉向类的中心，使得类内特征分布变得紧凑。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/Center-loss笔记.md","raw":"---\ntitle: Center loss笔记\ndate: 2017-06-29 17:05:51\ncategories: Face\ntags: [深度学习，论文笔记]\ndescription:\nmathjax:\n---\n\n论文：[A Discriminative Feature Learning Approach for Deep Face Recognition](https://link.springer.com/chapter/10.1007%2F978-3-319-46478-7_31)\n\n# 摘要\n\n对于一般的CNN网络，softmax通常作为监督信号来训练深层网络，为了增强提取的特征的可辨别性（discriminative），提出center loss，应对人脸识别任务。center loss同时学习每个类别的深层特征中心和惩罚深层特征和它们对应的类中心的距离（the center loss simultaneously learns a center for deep\nfeatures of each class and penalizes the distances between the deep features and their corresponding class centers）。将softmax和center loss联合起来，可以训练一个健壮的CNN来获取深层特征的两个关键目标，类内紧凑和类间分散。\n\n# 介绍\n\n预先收集所有可能的测试身份用于训练是不现实的，所以CNN的标签预测并不总是适用的。经过深层网络获取得到的特征不仅需要可分开性(separable)，更需要识别性(discriminative)和广义性，足够用来识别新的没有遇见过的类。可识别性的特征可以利用最邻近(NN)或k-NN算法很好的分类，就没有必要依赖标签预测了。但是softmax只能产生可分开(separable)特征，结果特征就不足以用以人脸识别。\n\n![Separabale Feature Vs Discriminative Feature](https://static.leiphone.com/uploads/new/article/740_740/201612/585bb8742235c.png?imageMogr2/format/jpg/quality/90)\n\n\n\n因为随机梯度下降(SGD)优化CNN是基于mini-batch，不能很好的反映深层特征的全局分布，由于训练集的庞大，在每次迭代中输入所有的训练样本是不现实的。constractive loss和triplet loss分别作为图像对和三元组的loss函数。然而，与图像样本相比，训练图像对或三元组的数量显著增长，导致收敛缓慢和不稳定性。仔细选择图像对或者三元组，问题可能会部分缓解，但是它增加了计算复杂度，训练过程变得不方便。为了解决这个问题，提出center loss，用于有效的增强特征的可识别性，我们将会得到每个类的深层特征的中心。在训练阶段，我们同时更新中心和最小化特征和它们相应的类中心的距离。CNN同时在softmax loss和center loss的监督下进行训练，通过一个超参来平衡这两个监督信号。直觉上，softmax loss将不同类别的特征分开，center loss有效的将同一类别的特征拉向类的中心，使得类内特征分布变得紧凑。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"Center-loss笔记","published":1,"updated":"2017-06-30T12:19:30.937Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440th000v2sc88qj4rpcx","content":"<p>论文：<a href=\"https://link.springer.com/chapter/10.1007%2F978-3-319-46478-7_31\" target=\"_blank\" rel=\"external\">A Discriminative Feature Learning Approach for Deep Face Recognition</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>对于一般的CNN网络，softmax通常作为监督信号来训练深层网络，为了增强提取的特征的可辨别性（discriminative），提出center loss，应对人脸识别任务。center loss同时学习每个类别的深层特征中心和惩罚深层特征和它们对应的类中心的距离（the center loss simultaneously learns a center for deep<br>features of each class and penalizes the distances between the deep features and their corresponding class centers）。将softmax和center loss联合起来，可以训练一个健壮的CNN来获取深层特征的两个关键目标，类内紧凑和类间分散。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>预先收集所有可能的测试身份用于训练是不现实的，所以CNN的标签预测并不总是适用的。经过深层网络获取得到的特征不仅需要可分开性(separable)，更需要识别性(discriminative)和广义性，足够用来识别新的没有遇见过的类。可识别性的特征可以利用最邻近(NN)或k-NN算法很好的分类，就没有必要依赖标签预测了。但是softmax只能产生可分开(separable)特征，结果特征就不足以用以人脸识别。</p>\n<p><img src=\"https://static.leiphone.com/uploads/new/article/740_740/201612/585bb8742235c.png?imageMogr2/format/jpg/quality/90\" alt=\"Separabale Feature Vs Discriminative Feature\"></p>\n<p>因为随机梯度下降(SGD)优化CNN是基于mini-batch，不能很好的反映深层特征的全局分布，由于训练集的庞大，在每次迭代中输入所有的训练样本是不现实的。constractive loss和triplet loss分别作为图像对和三元组的loss函数。然而，与图像样本相比，训练图像对或三元组的数量显著增长，导致收敛缓慢和不稳定性。仔细选择图像对或者三元组，问题可能会部分缓解，但是它增加了计算复杂度，训练过程变得不方便。为了解决这个问题，提出center loss，用于有效的增强特征的可识别性，我们将会得到每个类的深层特征的中心。在训练阶段，我们同时更新中心和最小化特征和它们相应的类中心的距离。CNN同时在softmax loss和center loss的监督下进行训练，通过一个超参来平衡这两个监督信号。直觉上，softmax loss将不同类别的特征分开，center loss有效的将同一类别的特征拉向类的中心，使得类内特征分布变得紧凑。</p>\n","site":{"data":{}},"excerpt":"","more":"<p>论文：<a href=\"https://link.springer.com/chapter/10.1007%2F978-3-319-46478-7_31\" target=\"_blank\" rel=\"external\">A Discriminative Feature Learning Approach for Deep Face Recognition</a></p>\n<h1 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h1><p>对于一般的CNN网络，softmax通常作为监督信号来训练深层网络，为了增强提取的特征的可辨别性（discriminative），提出center loss，应对人脸识别任务。center loss同时学习每个类别的深层特征中心和惩罚深层特征和它们对应的类中心的距离（the center loss simultaneously learns a center for deep<br>features of each class and penalizes the distances between the deep features and their corresponding class centers）。将softmax和center loss联合起来，可以训练一个健壮的CNN来获取深层特征的两个关键目标，类内紧凑和类间分散。</p>\n<h1 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h1><p>预先收集所有可能的测试身份用于训练是不现实的，所以CNN的标签预测并不总是适用的。经过深层网络获取得到的特征不仅需要可分开性(separable)，更需要识别性(discriminative)和广义性，足够用来识别新的没有遇见过的类。可识别性的特征可以利用最邻近(NN)或k-NN算法很好的分类，就没有必要依赖标签预测了。但是softmax只能产生可分开(separable)特征，结果特征就不足以用以人脸识别。</p>\n<p><img src=\"https://static.leiphone.com/uploads/new/article/740_740/201612/585bb8742235c.png?imageMogr2/format/jpg/quality/90\" alt=\"Separabale Feature Vs Discriminative Feature\"></p>\n<p>因为随机梯度下降(SGD)优化CNN是基于mini-batch，不能很好的反映深层特征的全局分布，由于训练集的庞大，在每次迭代中输入所有的训练样本是不现实的。constractive loss和triplet loss分别作为图像对和三元组的loss函数。然而，与图像样本相比，训练图像对或三元组的数量显著增长，导致收敛缓慢和不稳定性。仔细选择图像对或者三元组，问题可能会部分缓解，但是它增加了计算复杂度，训练过程变得不方便。为了解决这个问题，提出center loss，用于有效的增强特征的可识别性，我们将会得到每个类的深层特征的中心。在训练阶段，我们同时更新中心和最小化特征和它们相应的类中心的距离。CNN同时在softmax loss和center loss的监督下进行训练，通过一个超参来平衡这两个监督信号。直觉上，softmax loss将不同类别的特征分开，center loss有效的将同一类别的特征拉向类的中心，使得类内特征分布变得紧凑。</p>\n"},{"title":"文档归类","date":"2017-07-17T07:40:50.000Z","description":null,"mathjax":null,"_content":"","source":"_posts/文档归类.md","raw":"---\ntitle: 文档归类\ndate: 2017-07-17 15:40:50\ncategories:\ntags:\ndescription:\nmathjax:\n---\n","slug":"文档归类","published":1,"updated":"2017-07-17T07:40:50.206Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440tr000x2sc89465hp8k","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"Triplet loss","date":"2017-06-02T12:26:35.000Z","description":null,"mathjax":true,"_content":"\n# 原理\n\nTriplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。\n\n\n\n\n\n![Triplet Loss 示意图](http://img.blog.csdn.net/20160727090101355)\n\n\n\n### Triplet loss中的margin取值分析\n\n我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。\n\n```\n当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。\n\n当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。\n\n因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。\n```\n\n\n\n## 相关\n\n区分相似图形，除了triplet loss，还有一篇CVPR：[《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》](http://blog.csdn.net/u010167269/article/details/51783446)提出的Coupled Cluster Loss.\n\n\n\n**本文参考**：\n\n[triplet loss 原理以及梯度推导](http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html)\n\n[如何在Caffe中增加layer以及Caffe中triplet loss layer的实现](http://www.voidcn.com/blog/mao_kun/article/p-6246924.html)\n\n\n\n\n\n\n\n\n\n","source":"_posts/Triplet loss.md","raw":"---\ntitle: Triplet loss\ndate: 2017-06-02 20:26:35\ncategories: Face\ntags: [深度学习，人脸识别]\ndescription:\nmathjax: true\n---\n\n# 原理\n\nTriplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。\n\n\n\n\n\n![Triplet Loss 示意图](http://img.blog.csdn.net/20160727090101355)\n\n\n\n### Triplet loss中的margin取值分析\n\n我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。\n\n```\n当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。\n\n当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。\n\n因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。\n```\n\n\n\n## 相关\n\n区分相似图形，除了triplet loss，还有一篇CVPR：[《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》](http://blog.csdn.net/u010167269/article/details/51783446)提出的Coupled Cluster Loss.\n\n\n\n**本文参考**：\n\n[triplet loss 原理以及梯度推导](http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html)\n\n[如何在Caffe中增加layer以及Caffe中triplet loss layer的实现](http://www.voidcn.com/blog/mao_kun/article/p-6246924.html)\n\n\n\n\n\n\n\n\n\n","slug":"Triplet loss","published":1,"updated":"2017-06-02T13:09:42.119Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440u300102sc87x60pm0k","content":"<h1 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h1><p>Triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。</p>\n<p><img src=\"http://img.blog.csdn.net/20160727090101355\" alt=\"Triplet Loss 示意图\"></p>\n<h3 id=\"Triplet-loss中的margin取值分析\"><a href=\"#Triplet-loss中的margin取值分析\" class=\"headerlink\" title=\"Triplet loss中的margin取值分析\"></a>Triplet loss中的margin取值分析</h3><p>我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。</div><div class=\"line\"></div><div class=\"line\">当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。</div><div class=\"line\"></div><div class=\"line\">因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。</div></pre></td></tr></table></figure>\n<h2 id=\"相关\"><a href=\"#相关\" class=\"headerlink\" title=\"相关\"></a>相关</h2><p>区分相似图形，除了triplet loss，还有一篇CVPR：<a href=\"http://blog.csdn.net/u010167269/article/details/51783446\" target=\"_blank\" rel=\"external\">《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》</a>提出的Coupled Cluster Loss.</p>\n<p><strong>本文参考</strong>：</p>\n<p><a href=\"http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html\" target=\"_blank\" rel=\"external\">triplet loss 原理以及梯度推导</a></p>\n<p><a href=\"http://www.voidcn.com/blog/mao_kun/article/p-6246924.html\" target=\"_blank\" rel=\"external\">如何在Caffe中增加layer以及Caffe中triplet loss layer的实现</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h1><p>Triplet是一个三元组，这个三元组是这样构成的：从训练数据集中随机选一个样本，该样本称为Anchor，然后再随机选取一个和Anchor (记为x_a)属于同一类的样本Positive (记为x_p)和不同类的样本Negative (记为x_n)，由此构成一个（Anchor，Positive，Negative）三元组。</p>\n<p><img src=\"http://img.blog.csdn.net/20160727090101355\" alt=\"Triplet Loss 示意图\"></p>\n<h3 id=\"Triplet-loss中的margin取值分析\"><a href=\"#Triplet-loss中的margin取值分析\" class=\"headerlink\" title=\"Triplet loss中的margin取值分析\"></a>Triplet loss中的margin取值分析</h3><p>我们的目的是为了让loss在训练迭代中下降的越小越好，即使Anchor和Positive越接近越好，Anchor和Negative越远越好，并且要让x_a与x_n之间的距离和x_a与x_p之间的距离之间有一个最小的间隔。简而言之，Triplet loss就是要使类内距离越小，类间距离越大。</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div></pre></td><td class=\"code\"><pre><div class=\"line\">当 margin 值越小时，loss 也就较容易的趋近于 0，于是 Anchor 与 Positive 都不需要拉的太近，Anchor 与 Negative 不需要拉的太远，就能使得 loss 很快的趋近于 0。这样训练得到的结果，不能够很好的区分相似的图像。</div><div class=\"line\"></div><div class=\"line\">当 Anchor 越大时，就需要使得网络参数要拼命地拉近 Anchor、Positive 之间的距离，拉远 Anchor、Negative 之间的距离。如果 margin 值设置的太大，很可能最后 loss 保持一个较大的值，难以趋近于 0 。</div><div class=\"line\"></div><div class=\"line\">因此，设置一个合理的 margin 值很关键，这是衡量相似度的重要指标。简而言之，margin 值设置的越小，loss 很容易趋近于 0 ，但很难区分相似的图像。margin 值设置的越大，loss 值较难趋近于 0，甚至导致网络不收敛，但可以较有把握的区分较为相似的图像。</div></pre></td></tr></table></figure>\n<h2 id=\"相关\"><a href=\"#相关\" class=\"headerlink\" title=\"相关\"></a>相关</h2><p>区分相似图形，除了triplet loss，还有一篇CVPR：<a href=\"http://blog.csdn.net/u010167269/article/details/51783446\" target=\"_blank\" rel=\"external\">《Deep Relative Distance Learning: Tell the Difference Between Similar Vehicles》</a>提出的Coupled Cluster Loss.</p>\n<p><strong>本文参考</strong>：</p>\n<p><a href=\"http://www.voidcn.com/blog/tangwei2014/article/p-4415770.html\" target=\"_blank\" rel=\"external\">triplet loss 原理以及梯度推导</a></p>\n<p><a href=\"http://www.voidcn.com/blog/mao_kun/article/p-6246924.html\" target=\"_blank\" rel=\"external\">如何在Caffe中增加layer以及Caffe中triplet loss layer的实现</a></p>\n"},{"title":"待办及进度","date":"2017-06-02T07:15:46.000Z","description":null,"_content":"# step1:Mirror face相关\n\n| Model              | PCA Size | Threshold | Score  |\n| ------------------ | -------- | --------- | ------ |\n| Mirror             | 192      | 0.64      | 99.42% |\n| Mirror Concat      | 192      | 0.65      | 99.42% |\n| Mirror Add/Average | 184      | 0.64      | 99.47% |\n| Mirror Max         | 144      | 0.65      | 99.43% |\n| Mirror Min         | 168      | 0.65      | 99.48% |\n| Mirror Avg+min     | 168      | 0.65      | 99.45% |\n\n# step2:单眼Patch model\n\n| eye_width | num      | eye_width | num    |\n| --------- | -------- | --------- | ------ |\n| 0~10      | 14       | 90~100    | 8,5674 |\n| 10~20     | 3329     | 100~110   | 2,9481 |\n| 20~30     | 21,3675  | 110~120   | 9051   |\n| 30~40     | 45,1416  | 120~130   | 2894   |\n| 40~50     | 49,1913  | 130~140   | 932    |\n| 50~60     | 72,8911  | 140~150   | 279    |\n| 60~70     | 137,9232 | 150~160   | 86     |\n| 70~80     | 128,7442 | 160~170   | 14     |\n| 80~90     | 30,9026  | 170~180   | 6      |\n\n## 问题\n\n1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y...],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)*0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5*eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句\n```\nif eye_width*0.5>eye_left_x:\n            eye_width=eye_left_x*2\n```\n导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张\n\n重新生成眼睛宽度估算文件，其分布如下\n\n| eye_width | num      | eye_width | num  |\n| --------- | -------- | --------- | ---- |\n| 0~10      | 1,8975   | 100~110   | 98   |\n| 10~20     | 71,3760  | 110~120   | 17   |\n| 20~30     | 126,4102 | 120~130   | 3    |\n| 30~40     | 226,4348 | 130~140   | 0    |\n| 40~50     | 59,3351  | 140~150   | 0    |\n| 50~60     | 10,6502  | 150~160   | 0    |\n| 60~70     | 2,4588   | 160~170   | 1    |\n| 70~80     | 5592     | 170~180   | 0    |\n| 80~90     | 1588     | 180~190   | 8    |\n| 90~100    | 421      | 190~200   | 21   |\n\n\n\n2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可\n3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。\n最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526\n\n<u>17个小时，迭代12万次，26.5%的准确率，loss=5.5。</u>\n\n\n\n# step3:双眼patch model\n\n生成双眼宽度估算文件，其分布如下\n\n| eye_width | num      | eye_width | num     | eye_width | num  |\n| --------- | -------- | --------- | ------- | --------- | ---- |\n| 0~10      | 30       | 100~110   | 18,8771 | 200~210   | 383  |\n| 10~20     | 5888     | 110~120   | 8,8508  | 210~220   | 196  |\n| 20~30     | 11,4595  | 120~130   | 4,6243  | 220~230   | 100  |\n| 30~40     | 35,5451  | 130~140   | 2,4982  | 230~240   | 52   |\n| 40~50     | 42,6729  | 140~150   | 1,2749  | 240~250   | 33   |\n| 50~60     | 49,1312  | 150~160   | 6714    | 250~260   | 15   |\n| 60~70     | 69,0394  | 160~170   | 3413    | 260~270   | 3    |\n| 70~80     | 104,9353 | 170~180   | 1909    | 270~280   | 2    |\n| 80~90     | 100,6618 | 180~190   | 1091    | 280~290   | 0    |\n| 90~100    | 47,7219  | 190~200   | 622     | 290~300   | 0    |\n\n筛选crop后，宽度在20~130区间的图片，共90523个类别，4932655张。\n\ntrain.txt：3982004 \n\nval.txt：950661 \n\n<u>18万次迭代之后，准确率只有66%左右。</u>\n\n\n\n# step4:crop对齐后的图片的眼睛，训练单眼模型\n\n数据集大小：5044507(90525个类)（\"/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt\"，\"/home/yf/data/msclean\"）\n\n       \"ref_points\": [\n       \t\t30.2946, 51.6963, \n            65.5318, 51.5014, \n            48.0252, 71.7366,\n            33.5493, 92.3655, \n            62.7299, 92.2041\n        ]\n\n\n         eye_width=(ref_points[2]-ref_points[0])*0.8\n            eye_height=eye_width\n            x1=ref_points[0]-0.5*eye_width=16\n            x2=ref_points[0]+0.5*eye_width=44\n            y1=ref_points[1]-0.5*eye_height=37\n            y2=ref_points[1]+0.5*eye_height=65\ntrain:4071324 张\n\nval:973183张\n\n\n\n# step5:crop对齐后的图片的眼睛，训练双眼模型\n\n数据集大小：5044507(90525个类)（\"/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt\"，\"/home/yf/data/msclean\"）\n\n\n\n       \"ref_points\": [\n       \t30.2946, 51.6963, \n        65.5318, 51.5014, \n        48.0252, 71.7366,\n        33.5493, 92.3655, \n        62.7299, 92.2041\n    ]\n         eye_width=(ref_points[2]-ref_points[0])*0.8\n        eye_height=eye_width\n        x1=ref_points[0]-0.5*eye_width=16\n        x2=ref_points[2]+0.5*eye_width=79\n        y1=ref_points[1]-0.5*eye_height=37\n        y2=ref_points[1]+0.5*eye_height=65\n<u>迭代16万次，精度为72.73%，loss=2.52</u>\n\n<u>在lfw上测试，精度最高达到77.04%</u>\n\n# step6:Center face+dropout+finetune on softmax\n\n在msclean测试集上达到93.53% \n\n| Model                             | PCA_Size | Threshold | Score  |\n| :-------------------------------- | -------- | --------- | ------ |\n| dropcenter                        | 168      | 0.64      | 99.42% |\n| dropcenter_mirror                 | 136      | 0.64      | 99.38% |\n| dropcenter +dropcenter_mirror+Min | 128      | 0.64      | 99.45% |\n| dropcenter+dropcenter_mirror+ Add | 128      | 0.64      | 99.45% |\n| center +dropcenter +Min           | 400      | 0.64      | 99.40% |\n| centermirror+dropcenter+Min       | 128      | 0.64      | 99.45% |\n| centermirror+dropcenter+Add       | 160      | 0.64      | 99.43% |\n| centermirror+dropcenter+Max       | 160      | 0.64      | 99.43% |\n| centermirror+dropcenter+Concate   | 192      | 0.65      | 99.47% |\n\n| Model                                    | PCA Size | Threshold | Score  |\n| ---------------------------------------- | -------- | --------- | ------ |\n| center_min_mirror+dropcenter+Concate     | 192      | 0.65      | 99.47% |\n| center_min_mirror+dropcenter+Min         | 128      | 0.65      | 99.42% |\n| center_min_mirror+dropcenter+Add         | 136      | 0.64      | 99.47% |\n| eye_model                                | 160      | 0.57      | 77.04% |\n| eyemodel+center+Con                      | 208      | 0.66      | 74.80% |\n| 三模型                                      |          |           |        |\n| center+center_min_mirror+dropoutcenter+Concate | 128      | 0.65      | 99.43% |\n| center+softmax+dropoutcenter+Concate     | 168      | 0.66      | 99.43% |\n| center+softmax+dropoutcenter+Add         | 496      | 0.65      | 99.42% |\n\n\n\n\n\n# step7:balance\n\n## step7.1:减小过采样的数量，防止过拟合\n\n对/home/yf/data/clean.txt中每种类别进行统计各有多少个数：\n\n| 每种类别包含图片张数 | 类别数   | 每种类别包含图片张数 | 类别数   |\n| ---------- | ----- | ---------- | ----- |\n| <10        | 1213  | 10~20      | 11617 |\n| 20~30      | 10868 | 30~40      | 9692  |\n| 40~50      | 9020  | 50~60      | 8426  |\n| 60~70      | 8443  | 70~80      | 8783  |\n| 80~90      | 8762  | 90~100     | 7317  |\n| 100~110    | 4277  | 110~120    | 1753  |\n| 120~130    | 354   |            |       |\n\n类别总数共90525。由上图可知，类别严重不均衡，之前处理类别不均衡的方法主要是欠抽样和过抽样结合，对于多数类样本丢弃一部分样本，对于少数类样本复制生成，最后的训练数据分布如下：\n\n| 每种类别包含图片张数 | 类别数   | 每种类别包含图片张数 | 类别数   |\n| ---------- | ----- | ---------- | ----- |\n| 70~80      | 3673  | 80~90      | 19068 |\n| 90~100     | 27069 | 100~110    | 40715 |\n\n由于少数类占了大多数，但是重复太多，可能导致过拟合问题，于是将每个类别的图片张数减去30，重新生成balance的训练数据，并训练模型。\n\n<u>迭代17万次后，msdata测试集上准确率达到92.28%，loss=0.27</u>\n\n<u>lfw上精度为99.18%</u>\n\n<u>mirror:99.27%</u>\n\n<u>add:99.27%</u>\n\n## step7.2:EasyEmsemble法均衡类别\n\nstep7.1的方法属于欠抽样和过抽样结合：\n\n- 对于欠抽样算法，将多数类样本删除有可能会导致分类器**丢失有关多数类的重要信息**。\n- 对于过抽样算法，虽然只是简单地将复制后的数据添加到原始数据集中，且某些样本的多个实例都是“**并列的**”，但这样也可能会导致分类器学习出现**过拟合现象**，对于同一个样本的多个复本产生多个规则条例，这就使得**规则过于具体化**；虽然在这种情况下，分类器的训练精度会很高，但在位置样本的分类性能就会非常不理想。\n\n**EasyEnsemble 核心思想是：**\n\n- 首先通过从多数类中**独立随机**抽取出若干子集\n\n- 将每个子集与少数类数据**联合**起来**训练**生成多个基分类器\n\n- 最终将这些基分类器**组合形成**一个集成学习系统\n\n  设立一个阈值50，对于类别样本数超过50的，将其分写到两个不同的文件；对于类别样本数不超过50的，利用过采样进行增添，所以最终得到两个有交集的训练集A,B，两个训练集的样本数都是\n\n  90525*50=4526250\n\n<u>训练两个model，然后提取特征，对特征进行融合。</u>\n\n\n\n| Model(acc/loss)                          | Pca Size | Threshold | Score  |\n| ---------------------------------------- | -------- | --------- | ------ |\n| model1(90.75%/0.41)                      | 176      | 0.64      | 99.05% |\n| model1(92.14%/0.26)                      | 200      | 0.62      | 99.28% |\n| model1(92.14%/0.26) Mirror               | 280      | 0.63      | 99.32% |\n| model1(92.14%/0.26) Add Mirror           | 128      | 0.65      | 99.30% |\n| model2(92.56%/0.41)                      | 128      | 0.64      | 99.27% |\n| model2(92.56%/0.41) Mirror               | 192      | 0.63      | 99.35% |\n| model2(92.56%/0.41) Add Mirror           | 192      | 0.64      | 99.32% |\n| model1 add model2                        | 152      | 0.64      | 99.33% |\n| model1 mirror add model2 mirror          | 136      | 0.65      | 99.37% |\n| model1 add model2 mirror                 | 152      | 0.64      | 99.37% |\n| model1_add_mirror add model2_add_mirror  | 152      | 0.64      | 99.38% |\n| model1_add_mirror concate model2_add_mirror | 128      | 0.65      | 99.32% |\n| model1 mirror min model2 mirror          | 168      | 0.64      | 99.37% |\n\n\n\n# step8:UMDFaces\n\n对UMDFaces数据集进行人脸对齐处理\n\nbatch1:175,534(3554类)\n\nbatch2:115,126(2590类)\n\nbatch3:77,228(2133类)\n\nframes:3,735,475(3106类)\n\n提取4个数据集的类别名称，经过处理分析后发现frames的类别属于batch1类别的子集，将3个batch与frames的数据集整合到一个数据集下，因为当静态图片和视频帧进行结合后训练的模型往往既能兼顾个体之间的差异（静态图片特征）也能学习到同一个个体的姿态变化（视频帧特征），要注意的一点就是对于frames和batch1中同一个类别的要放在一个目录下，并重新生成类别标签。\n\n数据总量:4103363(8276个类别)\n\n数据整理已经完成，接下来是在这个数据集上进行metric learning的训练。\n\ntrain:3286012\nval:817351\n\n# step9:Megaface测试\n\n| Model                           | Dataset            | Score(Megaface/LFW) |\n| ------------------------------- | ------------------ | ------------------- |\n| center-face                     | FaceScrub Set1/LFW | 67.32%/99.42%       |\n| balance-reduced                 | FaceScrub Set1/LFW | 70.99%/99.18%       |\n| easyensemble                    | FaceScrub Set1     | 73.91%/99.33%       |\n| easyensemble  concat addmirror  | FaceScrub Set1     | 74.21%/99.37%       |\n| balance-cent-soft               | FaceScrub Set1/LFW | 74.47%/99.33%       |\n| balance-cent-soft concat mirror | FaceScrub Set1     | 75.65%              |\n\n## step9.1:Megaface测试（续）\n\n| Model                                    | Dataset                 | Score(Megaface/LFW) |\n| ---------------------------------------- | ----------------------- | ------------------- |\n| Dropout_center Concat mirror             | FaceScrub Set1/LFW      | 69.34%/99.47%       |\n| normface easyensemble model1 Concart mirror | FaceScrub Set1/LFW      | 70.32%              |\n| normface easyensemble 2models Concat mirror | FaceScrub Set1          | 70.49%              |\n| balance concat mirror                    | FaceScrub(matlab_mtcnn) | 78.84%              |\n| easyensemble concat addmirror            | FaceScrub(matlab_mtcnn) | 75.92%              |\n| jitter_center_iter_190000 concat mirror  | FaceScrub(matlab_mtcnn) | 77.69%              |\n| jitter_softmax_iter_180000 concat mirror | FaceScrub(matlab_mtcnn) | 83.13%              |\n| jitter_softmax_iter_180000 only mirror   | FaceScrub(matlab_mtcnn) | 82.08%              |\n| jitter_softmax_iter_180000 add mirror    | FaceScrub(matlab_mtcnn) | 82.87%              |\n| jitter_softmax_iter_184000 concat mirror | FaceScrub(matlab_mtcnn) | 83.16%              |\n| jitter_softmax_iter_180000 concat mirror | FaceScrub(python_mtcnn) | 78.33%              |\n| jitter_softmax_iter_184000 concat mirror(matlab align) | FaceScrub(matlab_mtcnn) | 79.05%              |\n| normface_jitter_iter_124000 concat mirror(python align) | FaceScrub(python_mtcnn) | 76.50%              |\n| normface_jitter_iter124000 cancat mirror(python align) | FaceScrub(matlab_mtcnn) | 78.92%              |\n\n\n\n# step 10:MTCNN(matlab)人脸检测及对齐\n\n## step 10.1：对齐Megaface和FaceScrub\n\n主要是Megaface数据集（1028062张）,FaceScrub数据集(91712张)，其中FaceScrub数据集中通过mtcnn（/home/yf/align/align_megaface.m）检测到的有89751张，剩余的1961张需要利用数据集中提供的3个关键点进行对齐，首先需要获取未检测到的图片的路径，然后利用python 脚本(/home/yf/megaface/devkit/templatelists/analysis/analysis_json.py)解析对应的存储该图片中人脸关键点的json文件，最后在利用matlab脚本(/home/yf/align/for_not_detect/align_megaface.m)进行批量对齐。Megaface数据集中未检测到的数据集同样处理。\n\n**问题**：\n\n在进行了41万次对齐后，出现了imread的错误，然后将从目录读取路径改成了从存储图片路径的文件中(/home/yf/megaface/tests/MegaFace_align_list_image.txt)直接获取路径，并输出每次进行处理的文件名，重新进行对齐操作，然后重现了这个错误，最后比对MegaFace_align_list_image.txt的下一张图片，发现有张图片是输入为空的。\n\n## step 10.2:对齐msceleb数据\n\n重新对齐msceleb数据集用于训练。\n\n# step 11:Normface训练\n\nNormface(paper:[NormFace: L2 Hypersphere Embedding for Face Verification](https://arxiv.org/pdf/1704.06369.pdf))\n\n## step 11.1:训练EasyEnsemble模型\n\nmodel1在测试集上的准确率为92.88%，model2在测试集上的准确率为92.85%。\n\n暂时只测了单个的model1 concate mirror在Megaface(还是原始python版mtcnn对齐的)上的准确率只有70.32%。下周继续测试两个模型的效果。\n\n## step 11.2:训练Balance模型\n\n刚生成完训练的数据集，下周开始训练。\n\n# step 12:Image Jitter\n\n对图片增加随机扰动，包括缩放、角度变换、镜像操作，主要还是msceleb数据集(/home/yf/data/msclean)上进行，由于该数据集类别不均衡，所以对于样本数较少的类别可以采用这种办法增加样本容量。最终将每个类别的样本数控制在80~160之间。\n\n10240892\n\ntrain:9257534\n\nval:983342\n\n正在生成训练的数据集lmdb。\n\njitter_center_iter_190000 concat mirror  FaceScrub(matlab_mtcnn)  77.69%  \n\njitter_softmax_iter_180000 concat mirror  FaceScrub(matlab_mtcnn)  83.18%\n\njitter_softmax_iter_180000 only mirror  FaceScrub(matlab_mtcnn)  82.08%\n\njitter_softmax_iter_180000 add mirror  FaceScrub(matlab_mtcnn)  82.87%\n\njitter_softmax_iter_184000 concat mirror  FaceScrub(matlab_mtcnn)  83.16%\n\n\n\n# step 13:Gender test\n\n### 1.0\n\nVGG16在lfw上准确率90.04%，在imdb(15590测试样本)上准确率90.92%\n\nmodel training:female(69847),male(86061)\n\ntrain:124728\n\nval:15590\n\ntest:15590\n\n### 2.0\n\n网络：AlexNet 在清理后的数据集上，迭代29500次后，训练准确率为98.16%，loss=0.55\n\n在测试集上达到98.11%（15127/15418)\n\n\n\n\n\n# Todo1\n\n- [x] Create umdfaces-->lmdb\n- [x] EasyEnsemble train and test\n- [x] Use matcaffe for metric learning\n- [ ] Megaface test\n      - - [x] center face\n        - [x] balance-cent-soft\n        - [x] reduced\n        - [x] mirror or concatenate\n        - [x] EasyEnsemble\n\n\n- [x] Paper reading:One-shot face recognition by promoting underrepresented classes\n\n      ​\n\n\n# Todo2\n\n- [ ] jitter model training(softmax first)\n\n- [ ] balance model retrain on normface(include center loss)\n\n- [ ] aligned by matlab_mtcnn megaface(balance model 75.65% version)\n\n- [ ] gender classfication model training\n\n      - check the dataset (detect and crop by matlab_mtcnn)\n\n      - generate lmdb\n\n      - choose a model(ResNet?)\n\n        ​\n\n### 参考链接\n\n[happynear-face-verification](https://github.com/happynear/FaceVerification)\n\n[dlib-jitter](https://github.com/davisking/dlib/blob/cbd187fb6109d21406f6a76bb0e9aa0689b1e54a/examples/dnn_face_recognition_ex.cpp)\n\n[dlib-face-verification-blog](http://blog.dlib.net)\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_posts/待办及进度.md","raw":"---\ntitle: 待办及进度\ndate: 2017-06-02 15:15:46\ncategories:\ntags:\ndescription:\n---\n# step1:Mirror face相关\n\n| Model              | PCA Size | Threshold | Score  |\n| ------------------ | -------- | --------- | ------ |\n| Mirror             | 192      | 0.64      | 99.42% |\n| Mirror Concat      | 192      | 0.65      | 99.42% |\n| Mirror Add/Average | 184      | 0.64      | 99.47% |\n| Mirror Max         | 144      | 0.65      | 99.43% |\n| Mirror Min         | 168      | 0.65      | 99.48% |\n| Mirror Avg+min     | 168      | 0.65      | 99.45% |\n\n# step2:单眼Patch model\n\n| eye_width | num      | eye_width | num    |\n| --------- | -------- | --------- | ------ |\n| 0~10      | 14       | 90~100    | 8,5674 |\n| 10~20     | 3329     | 100~110   | 2,9481 |\n| 20~30     | 21,3675  | 110~120   | 9051   |\n| 30~40     | 45,1416  | 120~130   | 2894   |\n| 40~50     | 49,1913  | 130~140   | 932    |\n| 50~60     | 72,8911  | 140~150   | 279    |\n| 60~70     | 137,9232 | 150~160   | 86     |\n| 70~80     | 128,7442 | 160~170   | 14     |\n| 80~90     | 30,9026  | 170~180   | 6      |\n\n## 问题\n\n1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y...],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)*0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5*eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句\n```\nif eye_width*0.5>eye_left_x:\n            eye_width=eye_left_x*2\n```\n导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张\n\n重新生成眼睛宽度估算文件，其分布如下\n\n| eye_width | num      | eye_width | num  |\n| --------- | -------- | --------- | ---- |\n| 0~10      | 1,8975   | 100~110   | 98   |\n| 10~20     | 71,3760  | 110~120   | 17   |\n| 20~30     | 126,4102 | 120~130   | 3    |\n| 30~40     | 226,4348 | 130~140   | 0    |\n| 40~50     | 59,3351  | 140~150   | 0    |\n| 50~60     | 10,6502  | 150~160   | 0    |\n| 60~70     | 2,4588   | 160~170   | 1    |\n| 70~80     | 5592     | 170~180   | 0    |\n| 80~90     | 1588     | 180~190   | 8    |\n| 90~100    | 421      | 190~200   | 21   |\n\n\n\n2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可\n3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。\n最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526\n\n<u>17个小时，迭代12万次，26.5%的准确率，loss=5.5。</u>\n\n\n\n# step3:双眼patch model\n\n生成双眼宽度估算文件，其分布如下\n\n| eye_width | num      | eye_width | num     | eye_width | num  |\n| --------- | -------- | --------- | ------- | --------- | ---- |\n| 0~10      | 30       | 100~110   | 18,8771 | 200~210   | 383  |\n| 10~20     | 5888     | 110~120   | 8,8508  | 210~220   | 196  |\n| 20~30     | 11,4595  | 120~130   | 4,6243  | 220~230   | 100  |\n| 30~40     | 35,5451  | 130~140   | 2,4982  | 230~240   | 52   |\n| 40~50     | 42,6729  | 140~150   | 1,2749  | 240~250   | 33   |\n| 50~60     | 49,1312  | 150~160   | 6714    | 250~260   | 15   |\n| 60~70     | 69,0394  | 160~170   | 3413    | 260~270   | 3    |\n| 70~80     | 104,9353 | 170~180   | 1909    | 270~280   | 2    |\n| 80~90     | 100,6618 | 180~190   | 1091    | 280~290   | 0    |\n| 90~100    | 47,7219  | 190~200   | 622     | 290~300   | 0    |\n\n筛选crop后，宽度在20~130区间的图片，共90523个类别，4932655张。\n\ntrain.txt：3982004 \n\nval.txt：950661 \n\n<u>18万次迭代之后，准确率只有66%左右。</u>\n\n\n\n# step4:crop对齐后的图片的眼睛，训练单眼模型\n\n数据集大小：5044507(90525个类)（\"/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt\"，\"/home/yf/data/msclean\"）\n\n       \"ref_points\": [\n       \t\t30.2946, 51.6963, \n            65.5318, 51.5014, \n            48.0252, 71.7366,\n            33.5493, 92.3655, \n            62.7299, 92.2041\n        ]\n\n\n         eye_width=(ref_points[2]-ref_points[0])*0.8\n            eye_height=eye_width\n            x1=ref_points[0]-0.5*eye_width=16\n            x2=ref_points[0]+0.5*eye_width=44\n            y1=ref_points[1]-0.5*eye_height=37\n            y2=ref_points[1]+0.5*eye_height=65\ntrain:4071324 张\n\nval:973183张\n\n\n\n# step5:crop对齐后的图片的眼睛，训练双眼模型\n\n数据集大小：5044507(90525个类)（\"/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt\"，\"/home/yf/data/msclean\"）\n\n\n\n       \"ref_points\": [\n       \t30.2946, 51.6963, \n        65.5318, 51.5014, \n        48.0252, 71.7366,\n        33.5493, 92.3655, \n        62.7299, 92.2041\n    ]\n         eye_width=(ref_points[2]-ref_points[0])*0.8\n        eye_height=eye_width\n        x1=ref_points[0]-0.5*eye_width=16\n        x2=ref_points[2]+0.5*eye_width=79\n        y1=ref_points[1]-0.5*eye_height=37\n        y2=ref_points[1]+0.5*eye_height=65\n<u>迭代16万次，精度为72.73%，loss=2.52</u>\n\n<u>在lfw上测试，精度最高达到77.04%</u>\n\n# step6:Center face+dropout+finetune on softmax\n\n在msclean测试集上达到93.53% \n\n| Model                             | PCA_Size | Threshold | Score  |\n| :-------------------------------- | -------- | --------- | ------ |\n| dropcenter                        | 168      | 0.64      | 99.42% |\n| dropcenter_mirror                 | 136      | 0.64      | 99.38% |\n| dropcenter +dropcenter_mirror+Min | 128      | 0.64      | 99.45% |\n| dropcenter+dropcenter_mirror+ Add | 128      | 0.64      | 99.45% |\n| center +dropcenter +Min           | 400      | 0.64      | 99.40% |\n| centermirror+dropcenter+Min       | 128      | 0.64      | 99.45% |\n| centermirror+dropcenter+Add       | 160      | 0.64      | 99.43% |\n| centermirror+dropcenter+Max       | 160      | 0.64      | 99.43% |\n| centermirror+dropcenter+Concate   | 192      | 0.65      | 99.47% |\n\n| Model                                    | PCA Size | Threshold | Score  |\n| ---------------------------------------- | -------- | --------- | ------ |\n| center_min_mirror+dropcenter+Concate     | 192      | 0.65      | 99.47% |\n| center_min_mirror+dropcenter+Min         | 128      | 0.65      | 99.42% |\n| center_min_mirror+dropcenter+Add         | 136      | 0.64      | 99.47% |\n| eye_model                                | 160      | 0.57      | 77.04% |\n| eyemodel+center+Con                      | 208      | 0.66      | 74.80% |\n| 三模型                                      |          |           |        |\n| center+center_min_mirror+dropoutcenter+Concate | 128      | 0.65      | 99.43% |\n| center+softmax+dropoutcenter+Concate     | 168      | 0.66      | 99.43% |\n| center+softmax+dropoutcenter+Add         | 496      | 0.65      | 99.42% |\n\n\n\n\n\n# step7:balance\n\n## step7.1:减小过采样的数量，防止过拟合\n\n对/home/yf/data/clean.txt中每种类别进行统计各有多少个数：\n\n| 每种类别包含图片张数 | 类别数   | 每种类别包含图片张数 | 类别数   |\n| ---------- | ----- | ---------- | ----- |\n| <10        | 1213  | 10~20      | 11617 |\n| 20~30      | 10868 | 30~40      | 9692  |\n| 40~50      | 9020  | 50~60      | 8426  |\n| 60~70      | 8443  | 70~80      | 8783  |\n| 80~90      | 8762  | 90~100     | 7317  |\n| 100~110    | 4277  | 110~120    | 1753  |\n| 120~130    | 354   |            |       |\n\n类别总数共90525。由上图可知，类别严重不均衡，之前处理类别不均衡的方法主要是欠抽样和过抽样结合，对于多数类样本丢弃一部分样本，对于少数类样本复制生成，最后的训练数据分布如下：\n\n| 每种类别包含图片张数 | 类别数   | 每种类别包含图片张数 | 类别数   |\n| ---------- | ----- | ---------- | ----- |\n| 70~80      | 3673  | 80~90      | 19068 |\n| 90~100     | 27069 | 100~110    | 40715 |\n\n由于少数类占了大多数，但是重复太多，可能导致过拟合问题，于是将每个类别的图片张数减去30，重新生成balance的训练数据，并训练模型。\n\n<u>迭代17万次后，msdata测试集上准确率达到92.28%，loss=0.27</u>\n\n<u>lfw上精度为99.18%</u>\n\n<u>mirror:99.27%</u>\n\n<u>add:99.27%</u>\n\n## step7.2:EasyEmsemble法均衡类别\n\nstep7.1的方法属于欠抽样和过抽样结合：\n\n- 对于欠抽样算法，将多数类样本删除有可能会导致分类器**丢失有关多数类的重要信息**。\n- 对于过抽样算法，虽然只是简单地将复制后的数据添加到原始数据集中，且某些样本的多个实例都是“**并列的**”，但这样也可能会导致分类器学习出现**过拟合现象**，对于同一个样本的多个复本产生多个规则条例，这就使得**规则过于具体化**；虽然在这种情况下，分类器的训练精度会很高，但在位置样本的分类性能就会非常不理想。\n\n**EasyEnsemble 核心思想是：**\n\n- 首先通过从多数类中**独立随机**抽取出若干子集\n\n- 将每个子集与少数类数据**联合**起来**训练**生成多个基分类器\n\n- 最终将这些基分类器**组合形成**一个集成学习系统\n\n  设立一个阈值50，对于类别样本数超过50的，将其分写到两个不同的文件；对于类别样本数不超过50的，利用过采样进行增添，所以最终得到两个有交集的训练集A,B，两个训练集的样本数都是\n\n  90525*50=4526250\n\n<u>训练两个model，然后提取特征，对特征进行融合。</u>\n\n\n\n| Model(acc/loss)                          | Pca Size | Threshold | Score  |\n| ---------------------------------------- | -------- | --------- | ------ |\n| model1(90.75%/0.41)                      | 176      | 0.64      | 99.05% |\n| model1(92.14%/0.26)                      | 200      | 0.62      | 99.28% |\n| model1(92.14%/0.26) Mirror               | 280      | 0.63      | 99.32% |\n| model1(92.14%/0.26) Add Mirror           | 128      | 0.65      | 99.30% |\n| model2(92.56%/0.41)                      | 128      | 0.64      | 99.27% |\n| model2(92.56%/0.41) Mirror               | 192      | 0.63      | 99.35% |\n| model2(92.56%/0.41) Add Mirror           | 192      | 0.64      | 99.32% |\n| model1 add model2                        | 152      | 0.64      | 99.33% |\n| model1 mirror add model2 mirror          | 136      | 0.65      | 99.37% |\n| model1 add model2 mirror                 | 152      | 0.64      | 99.37% |\n| model1_add_mirror add model2_add_mirror  | 152      | 0.64      | 99.38% |\n| model1_add_mirror concate model2_add_mirror | 128      | 0.65      | 99.32% |\n| model1 mirror min model2 mirror          | 168      | 0.64      | 99.37% |\n\n\n\n# step8:UMDFaces\n\n对UMDFaces数据集进行人脸对齐处理\n\nbatch1:175,534(3554类)\n\nbatch2:115,126(2590类)\n\nbatch3:77,228(2133类)\n\nframes:3,735,475(3106类)\n\n提取4个数据集的类别名称，经过处理分析后发现frames的类别属于batch1类别的子集，将3个batch与frames的数据集整合到一个数据集下，因为当静态图片和视频帧进行结合后训练的模型往往既能兼顾个体之间的差异（静态图片特征）也能学习到同一个个体的姿态变化（视频帧特征），要注意的一点就是对于frames和batch1中同一个类别的要放在一个目录下，并重新生成类别标签。\n\n数据总量:4103363(8276个类别)\n\n数据整理已经完成，接下来是在这个数据集上进行metric learning的训练。\n\ntrain:3286012\nval:817351\n\n# step9:Megaface测试\n\n| Model                           | Dataset            | Score(Megaface/LFW) |\n| ------------------------------- | ------------------ | ------------------- |\n| center-face                     | FaceScrub Set1/LFW | 67.32%/99.42%       |\n| balance-reduced                 | FaceScrub Set1/LFW | 70.99%/99.18%       |\n| easyensemble                    | FaceScrub Set1     | 73.91%/99.33%       |\n| easyensemble  concat addmirror  | FaceScrub Set1     | 74.21%/99.37%       |\n| balance-cent-soft               | FaceScrub Set1/LFW | 74.47%/99.33%       |\n| balance-cent-soft concat mirror | FaceScrub Set1     | 75.65%              |\n\n## step9.1:Megaface测试（续）\n\n| Model                                    | Dataset                 | Score(Megaface/LFW) |\n| ---------------------------------------- | ----------------------- | ------------------- |\n| Dropout_center Concat mirror             | FaceScrub Set1/LFW      | 69.34%/99.47%       |\n| normface easyensemble model1 Concart mirror | FaceScrub Set1/LFW      | 70.32%              |\n| normface easyensemble 2models Concat mirror | FaceScrub Set1          | 70.49%              |\n| balance concat mirror                    | FaceScrub(matlab_mtcnn) | 78.84%              |\n| easyensemble concat addmirror            | FaceScrub(matlab_mtcnn) | 75.92%              |\n| jitter_center_iter_190000 concat mirror  | FaceScrub(matlab_mtcnn) | 77.69%              |\n| jitter_softmax_iter_180000 concat mirror | FaceScrub(matlab_mtcnn) | 83.13%              |\n| jitter_softmax_iter_180000 only mirror   | FaceScrub(matlab_mtcnn) | 82.08%              |\n| jitter_softmax_iter_180000 add mirror    | FaceScrub(matlab_mtcnn) | 82.87%              |\n| jitter_softmax_iter_184000 concat mirror | FaceScrub(matlab_mtcnn) | 83.16%              |\n| jitter_softmax_iter_180000 concat mirror | FaceScrub(python_mtcnn) | 78.33%              |\n| jitter_softmax_iter_184000 concat mirror(matlab align) | FaceScrub(matlab_mtcnn) | 79.05%              |\n| normface_jitter_iter_124000 concat mirror(python align) | FaceScrub(python_mtcnn) | 76.50%              |\n| normface_jitter_iter124000 cancat mirror(python align) | FaceScrub(matlab_mtcnn) | 78.92%              |\n\n\n\n# step 10:MTCNN(matlab)人脸检测及对齐\n\n## step 10.1：对齐Megaface和FaceScrub\n\n主要是Megaface数据集（1028062张）,FaceScrub数据集(91712张)，其中FaceScrub数据集中通过mtcnn（/home/yf/align/align_megaface.m）检测到的有89751张，剩余的1961张需要利用数据集中提供的3个关键点进行对齐，首先需要获取未检测到的图片的路径，然后利用python 脚本(/home/yf/megaface/devkit/templatelists/analysis/analysis_json.py)解析对应的存储该图片中人脸关键点的json文件，最后在利用matlab脚本(/home/yf/align/for_not_detect/align_megaface.m)进行批量对齐。Megaface数据集中未检测到的数据集同样处理。\n\n**问题**：\n\n在进行了41万次对齐后，出现了imread的错误，然后将从目录读取路径改成了从存储图片路径的文件中(/home/yf/megaface/tests/MegaFace_align_list_image.txt)直接获取路径，并输出每次进行处理的文件名，重新进行对齐操作，然后重现了这个错误，最后比对MegaFace_align_list_image.txt的下一张图片，发现有张图片是输入为空的。\n\n## step 10.2:对齐msceleb数据\n\n重新对齐msceleb数据集用于训练。\n\n# step 11:Normface训练\n\nNormface(paper:[NormFace: L2 Hypersphere Embedding for Face Verification](https://arxiv.org/pdf/1704.06369.pdf))\n\n## step 11.1:训练EasyEnsemble模型\n\nmodel1在测试集上的准确率为92.88%，model2在测试集上的准确率为92.85%。\n\n暂时只测了单个的model1 concate mirror在Megaface(还是原始python版mtcnn对齐的)上的准确率只有70.32%。下周继续测试两个模型的效果。\n\n## step 11.2:训练Balance模型\n\n刚生成完训练的数据集，下周开始训练。\n\n# step 12:Image Jitter\n\n对图片增加随机扰动，包括缩放、角度变换、镜像操作，主要还是msceleb数据集(/home/yf/data/msclean)上进行，由于该数据集类别不均衡，所以对于样本数较少的类别可以采用这种办法增加样本容量。最终将每个类别的样本数控制在80~160之间。\n\n10240892\n\ntrain:9257534\n\nval:983342\n\n正在生成训练的数据集lmdb。\n\njitter_center_iter_190000 concat mirror  FaceScrub(matlab_mtcnn)  77.69%  \n\njitter_softmax_iter_180000 concat mirror  FaceScrub(matlab_mtcnn)  83.18%\n\njitter_softmax_iter_180000 only mirror  FaceScrub(matlab_mtcnn)  82.08%\n\njitter_softmax_iter_180000 add mirror  FaceScrub(matlab_mtcnn)  82.87%\n\njitter_softmax_iter_184000 concat mirror  FaceScrub(matlab_mtcnn)  83.16%\n\n\n\n# step 13:Gender test\n\n### 1.0\n\nVGG16在lfw上准确率90.04%，在imdb(15590测试样本)上准确率90.92%\n\nmodel training:female(69847),male(86061)\n\ntrain:124728\n\nval:15590\n\ntest:15590\n\n### 2.0\n\n网络：AlexNet 在清理后的数据集上，迭代29500次后，训练准确率为98.16%，loss=0.55\n\n在测试集上达到98.11%（15127/15418)\n\n\n\n\n\n# Todo1\n\n- [x] Create umdfaces-->lmdb\n- [x] EasyEnsemble train and test\n- [x] Use matcaffe for metric learning\n- [ ] Megaface test\n      - - [x] center face\n        - [x] balance-cent-soft\n        - [x] reduced\n        - [x] mirror or concatenate\n        - [x] EasyEnsemble\n\n\n- [x] Paper reading:One-shot face recognition by promoting underrepresented classes\n\n      ​\n\n\n# Todo2\n\n- [ ] jitter model training(softmax first)\n\n- [ ] balance model retrain on normface(include center loss)\n\n- [ ] aligned by matlab_mtcnn megaface(balance model 75.65% version)\n\n- [ ] gender classfication model training\n\n      - check the dataset (detect and crop by matlab_mtcnn)\n\n      - generate lmdb\n\n      - choose a model(ResNet?)\n\n        ​\n\n### 参考链接\n\n[happynear-face-verification](https://github.com/happynear/FaceVerification)\n\n[dlib-jitter](https://github.com/davisking/dlib/blob/cbd187fb6109d21406f6a76bb0e9aa0689b1e54a/examples/dnn_face_recognition_ex.cpp)\n\n[dlib-face-verification-blog](http://blog.dlib.net)\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"待办及进度","published":1,"updated":"2018-02-03T13:18:52.579Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjda440ue00132sc8jybvaupy","content":"<h1 id=\"step1-Mirror-face相关\"><a href=\"#step1-Mirror-face相关\" class=\"headerlink\" title=\"step1:Mirror face相关\"></a>step1:Mirror face相关</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>PCA Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Mirror</td>\n<td>192</td>\n<td>0.64</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Concat</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Add/Average</td>\n<td>184</td>\n<td>0.64</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>Mirror Max</td>\n<td>144</td>\n<td>0.65</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>Mirror Min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.48%</td>\n</tr>\n<tr>\n<td>Mirror Avg+min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.45%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step2-单眼Patch-model\"><a href=\"#step2-单眼Patch-model\" class=\"headerlink\" title=\"step2:单眼Patch model\"></a>step2:单眼Patch model</h1><table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>14</td>\n<td>90~100</td>\n<td>8,5674</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>3329</td>\n<td>100~110</td>\n<td>2,9481</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>21,3675</td>\n<td>110~120</td>\n<td>9051</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>45,1416</td>\n<td>120~130</td>\n<td>2894</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>49,1913</td>\n<td>130~140</td>\n<td>932</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>72,8911</td>\n<td>140~150</td>\n<td>279</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>137,9232</td>\n<td>150~160</td>\n<td>86</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>128,7442</td>\n<td>160~170</td>\n<td>14</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>30,9026</td>\n<td>170~180</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y…],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)<em>0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5</em>eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">if eye_width*0.5&gt;eye_left_x:</div><div class=\"line\">            eye_width=eye_left_x*2</div></pre></td></tr></table></figure></p>\n<p>导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张</p>\n<p>重新生成眼睛宽度估算文件，其分布如下</p>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>1,8975</td>\n<td>100~110</td>\n<td>98</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>71,3760</td>\n<td>110~120</td>\n<td>17</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>126,4102</td>\n<td>120~130</td>\n<td>3</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>226,4348</td>\n<td>130~140</td>\n<td>0</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>59,3351</td>\n<td>140~150</td>\n<td>0</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>10,6502</td>\n<td>150~160</td>\n<td>0</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>2,4588</td>\n<td>160~170</td>\n<td>1</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>5592</td>\n<td>170~180</td>\n<td>0</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>1588</td>\n<td>180~190</td>\n<td>8</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>421</td>\n<td>190~200</td>\n<td>21</td>\n</tr>\n</tbody>\n</table>\n<p>2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可<br>3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。<br>最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526</p>\n<p><u>17个小时，迭代12万次，26.5%的准确率，loss=5.5。</u></p>\n<h1 id=\"step3-双眼patch-model\"><a href=\"#step3-双眼patch-model\" class=\"headerlink\" title=\"step3:双眼patch model\"></a>step3:双眼patch model</h1><p>生成双眼宽度估算文件，其分布如下</p>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>30</td>\n<td>100~110</td>\n<td>18,8771</td>\n<td>200~210</td>\n<td>383</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>5888</td>\n<td>110~120</td>\n<td>8,8508</td>\n<td>210~220</td>\n<td>196</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>11,4595</td>\n<td>120~130</td>\n<td>4,6243</td>\n<td>220~230</td>\n<td>100</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>35,5451</td>\n<td>130~140</td>\n<td>2,4982</td>\n<td>230~240</td>\n<td>52</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>42,6729</td>\n<td>140~150</td>\n<td>1,2749</td>\n<td>240~250</td>\n<td>33</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>49,1312</td>\n<td>150~160</td>\n<td>6714</td>\n<td>250~260</td>\n<td>15</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>69,0394</td>\n<td>160~170</td>\n<td>3413</td>\n<td>260~270</td>\n<td>3</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>104,9353</td>\n<td>170~180</td>\n<td>1909</td>\n<td>270~280</td>\n<td>2</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>100,6618</td>\n<td>180~190</td>\n<td>1091</td>\n<td>280~290</td>\n<td>0</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>47,7219</td>\n<td>190~200</td>\n<td>622</td>\n<td>290~300</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<p>筛选crop后，宽度在20~130区间的图片，共90523个类别，4932655张。</p>\n<p>train.txt：3982004 </p>\n<p>val.txt：950661 </p>\n<p><u>18万次迭代之后，准确率只有66%左右。</u></p>\n<h1 id=\"step4-crop对齐后的图片的眼睛，训练单眼模型\"><a href=\"#step4-crop对齐后的图片的眼睛，训练单眼模型\" class=\"headerlink\" title=\"step4:crop对齐后的图片的眼睛，训练单眼模型\"></a>step4:crop对齐后的图片的眼睛，训练单眼模型</h1><p>数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”）</p>\n<pre><code>&quot;ref_points&quot;: [\n        30.2946, 51.6963, \n     65.5318, 51.5014, \n     48.0252, 71.7366,\n     33.5493, 92.3655, \n     62.7299, 92.2041\n ]\n\n\n  eye_width=(ref_points[2]-ref_points[0])*0.8\n     eye_height=eye_width\n     x1=ref_points[0]-0.5*eye_width=16\n     x2=ref_points[0]+0.5*eye_width=44\n     y1=ref_points[1]-0.5*eye_height=37\n     y2=ref_points[1]+0.5*eye_height=65\n</code></pre><p>train:4071324 张</p>\n<p>val:973183张</p>\n<h1 id=\"step5-crop对齐后的图片的眼睛，训练双眼模型\"><a href=\"#step5-crop对齐后的图片的眼睛，训练双眼模型\" class=\"headerlink\" title=\"step5:crop对齐后的图片的眼睛，训练双眼模型\"></a>step5:crop对齐后的图片的眼睛，训练双眼模型</h1><p>数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”）</p>\n<pre><code>   &quot;ref_points&quot;: [\n       30.2946, 51.6963, \n    65.5318, 51.5014, \n    48.0252, 71.7366,\n    33.5493, 92.3655, \n    62.7299, 92.2041\n]\n     eye_width=(ref_points[2]-ref_points[0])*0.8\n    eye_height=eye_width\n    x1=ref_points[0]-0.5*eye_width=16\n    x2=ref_points[2]+0.5*eye_width=79\n    y1=ref_points[1]-0.5*eye_height=37\n    y2=ref_points[1]+0.5*eye_height=65\n</code></pre><p><u>迭代16万次，精度为72.73%，loss=2.52</u></p>\n<p><u>在lfw上测试，精度最高达到77.04%</u></p>\n<h1 id=\"step6-Center-face-dropout-finetune-on-softmax\"><a href=\"#step6-Center-face-dropout-finetune-on-softmax\" class=\"headerlink\" title=\"step6:Center face+dropout+finetune on softmax\"></a>step6:Center face+dropout+finetune on softmax</h1><p>在msclean测试集上达到93.53% </p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Model</th>\n<th>PCA_Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">dropcenter</td>\n<td>168</td>\n<td>0.64</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter_mirror</td>\n<td>136</td>\n<td>0.64</td>\n<td>99.38%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter +dropcenter_mirror+Min</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter+dropcenter_mirror+ Add</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">center +dropcenter +Min</td>\n<td>400</td>\n<td>0.64</td>\n<td>99.40%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Min</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Add</td>\n<td>160</td>\n<td>0.64</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Max</td>\n<td>160</td>\n<td>0.64</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Concate</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.47%</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>PCA Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>center_min_mirror+dropcenter+Concate</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>center_min_mirror+dropcenter+Min</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>center_min_mirror+dropcenter+Add</td>\n<td>136</td>\n<td>0.64</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>eye_model</td>\n<td>160</td>\n<td>0.57</td>\n<td>77.04%</td>\n</tr>\n<tr>\n<td>eyemodel+center+Con</td>\n<td>208</td>\n<td>0.66</td>\n<td>74.80%</td>\n</tr>\n<tr>\n<td>三模型</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>center+center_min_mirror+dropoutcenter+Concate</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>center+softmax+dropoutcenter+Concate</td>\n<td>168</td>\n<td>0.66</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>center+softmax+dropoutcenter+Add</td>\n<td>496</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step7-balance\"><a href=\"#step7-balance\" class=\"headerlink\" title=\"step7:balance\"></a>step7:balance</h1><h2 id=\"step7-1-减小过采样的数量，防止过拟合\"><a href=\"#step7-1-减小过采样的数量，防止过拟合\" class=\"headerlink\" title=\"step7.1:减小过采样的数量，防止过拟合\"></a>step7.1:减小过采样的数量，防止过拟合</h2><p>对/home/yf/data/clean.txt中每种类别进行统计各有多少个数：</p>\n<table>\n<thead>\n<tr>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>&lt;10</td>\n<td>1213</td>\n<td>10~20</td>\n<td>11617</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>10868</td>\n<td>30~40</td>\n<td>9692</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>9020</td>\n<td>50~60</td>\n<td>8426</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>8443</td>\n<td>70~80</td>\n<td>8783</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>8762</td>\n<td>90~100</td>\n<td>7317</td>\n</tr>\n<tr>\n<td>100~110</td>\n<td>4277</td>\n<td>110~120</td>\n<td>1753</td>\n</tr>\n<tr>\n<td>120~130</td>\n<td>354</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>类别总数共90525。由上图可知，类别严重不均衡，之前处理类别不均衡的方法主要是欠抽样和过抽样结合，对于多数类样本丢弃一部分样本，对于少数类样本复制生成，最后的训练数据分布如下：</p>\n<table>\n<thead>\n<tr>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>70~80</td>\n<td>3673</td>\n<td>80~90</td>\n<td>19068</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>27069</td>\n<td>100~110</td>\n<td>40715</td>\n</tr>\n</tbody>\n</table>\n<p>由于少数类占了大多数，但是重复太多，可能导致过拟合问题，于是将每个类别的图片张数减去30，重新生成balance的训练数据，并训练模型。</p>\n<p><u>迭代17万次后，msdata测试集上准确率达到92.28%，loss=0.27</u></p>\n<p><u>lfw上精度为99.18%</u></p>\n<p><u>mirror:99.27%</u></p>\n<p><u>add:99.27%</u></p>\n<h2 id=\"step7-2-EasyEmsemble法均衡类别\"><a href=\"#step7-2-EasyEmsemble法均衡类别\" class=\"headerlink\" title=\"step7.2:EasyEmsemble法均衡类别\"></a>step7.2:EasyEmsemble法均衡类别</h2><p>step7.1的方法属于欠抽样和过抽样结合：</p>\n<ul>\n<li>对于欠抽样算法，将多数类样本删除有可能会导致分类器<strong>丢失有关多数类的重要信息</strong>。</li>\n<li>对于过抽样算法，虽然只是简单地将复制后的数据添加到原始数据集中，且某些样本的多个实例都是“<strong>并列的</strong>”，但这样也可能会导致分类器学习出现<strong>过拟合现象</strong>，对于同一个样本的多个复本产生多个规则条例，这就使得<strong>规则过于具体化</strong>；虽然在这种情况下，分类器的训练精度会很高，但在位置样本的分类性能就会非常不理想。</li>\n</ul>\n<p><strong>EasyEnsemble 核心思想是：</strong></p>\n<ul>\n<li><p>首先通过从多数类中<strong>独立随机</strong>抽取出若干子集</p>\n</li>\n<li><p>将每个子集与少数类数据<strong>联合</strong>起来<strong>训练</strong>生成多个基分类器</p>\n</li>\n<li><p>最终将这些基分类器<strong>组合形成</strong>一个集成学习系统</p>\n<p>设立一个阈值50，对于类别样本数超过50的，将其分写到两个不同的文件；对于类别样本数不超过50的，利用过采样进行增添，所以最终得到两个有交集的训练集A,B，两个训练集的样本数都是</p>\n<p>90525*50=4526250</p>\n</li>\n</ul>\n<p><u>训练两个model，然后提取特征，对特征进行融合。</u></p>\n<table>\n<thead>\n<tr>\n<th>Model(acc/loss)</th>\n<th>Pca Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>model1(90.75%/0.41)</td>\n<td>176</td>\n<td>0.64</td>\n<td>99.05%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26)</td>\n<td>200</td>\n<td>0.62</td>\n<td>99.28%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26) Mirror</td>\n<td>280</td>\n<td>0.63</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26) Add Mirror</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.30%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41)</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.27%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41) Mirror</td>\n<td>192</td>\n<td>0.63</td>\n<td>99.35%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41) Add Mirror</td>\n<td>192</td>\n<td>0.64</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1 add model2</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.33%</td>\n</tr>\n<tr>\n<td>model1 mirror add model2 mirror</td>\n<td>136</td>\n<td>0.65</td>\n<td>99.37%</td>\n</tr>\n<tr>\n<td>model1 add model2 mirror</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.37%</td>\n</tr>\n<tr>\n<td>model1_add_mirror add model2_add_mirror</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.38%</td>\n</tr>\n<tr>\n<td>model1_add_mirror concate model2_add_mirror</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1 mirror min model2 mirror</td>\n<td>168</td>\n<td>0.64</td>\n<td>99.37%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step8-UMDFaces\"><a href=\"#step8-UMDFaces\" class=\"headerlink\" title=\"step8:UMDFaces\"></a>step8:UMDFaces</h1><p>对UMDFaces数据集进行人脸对齐处理</p>\n<p>batch1:175,534(3554类)</p>\n<p>batch2:115,126(2590类)</p>\n<p>batch3:77,228(2133类)</p>\n<p>frames:3,735,475(3106类)</p>\n<p>提取4个数据集的类别名称，经过处理分析后发现frames的类别属于batch1类别的子集，将3个batch与frames的数据集整合到一个数据集下，因为当静态图片和视频帧进行结合后训练的模型往往既能兼顾个体之间的差异（静态图片特征）也能学习到同一个个体的姿态变化（视频帧特征），要注意的一点就是对于frames和batch1中同一个类别的要放在一个目录下，并重新生成类别标签。</p>\n<p>数据总量:4103363(8276个类别)</p>\n<p>数据整理已经完成，接下来是在这个数据集上进行metric learning的训练。</p>\n<p>train:3286012<br>val:817351</p>\n<h1 id=\"step9-Megaface测试\"><a href=\"#step9-Megaface测试\" class=\"headerlink\" title=\"step9:Megaface测试\"></a>step9:Megaface测试</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Dataset</th>\n<th>Score(Megaface/LFW)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>center-face</td>\n<td>FaceScrub Set1/LFW</td>\n<td>67.32%/99.42%</td>\n</tr>\n<tr>\n<td>balance-reduced</td>\n<td>FaceScrub Set1/LFW</td>\n<td>70.99%/99.18%</td>\n</tr>\n<tr>\n<td>easyensemble</td>\n<td>FaceScrub Set1</td>\n<td>73.91%/99.33%</td>\n</tr>\n<tr>\n<td>easyensemble  concat addmirror</td>\n<td>FaceScrub Set1</td>\n<td>74.21%/99.37%</td>\n</tr>\n<tr>\n<td>balance-cent-soft</td>\n<td>FaceScrub Set1/LFW</td>\n<td>74.47%/99.33%</td>\n</tr>\n<tr>\n<td>balance-cent-soft concat mirror</td>\n<td>FaceScrub Set1</td>\n<td>75.65%</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"step9-1-Megaface测试（续）\"><a href=\"#step9-1-Megaface测试（续）\" class=\"headerlink\" title=\"step9.1:Megaface测试（续）\"></a>step9.1:Megaface测试（续）</h2><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Dataset</th>\n<th>Score(Megaface/LFW)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Dropout_center Concat mirror</td>\n<td>FaceScrub Set1/LFW</td>\n<td>69.34%/99.47%</td>\n</tr>\n<tr>\n<td>normface easyensemble model1 Concart mirror</td>\n<td>FaceScrub Set1/LFW</td>\n<td>70.32%</td>\n</tr>\n<tr>\n<td>normface easyensemble 2models Concat mirror</td>\n<td>FaceScrub Set1</td>\n<td>70.49%</td>\n</tr>\n<tr>\n<td>balance concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>78.84%</td>\n</tr>\n<tr>\n<td>easyensemble concat addmirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>75.92%</td>\n</tr>\n<tr>\n<td>jitter_center_iter_190000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>77.69%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>83.13%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 only mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>82.08%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 add mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>82.87%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_184000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>83.16%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 concat mirror</td>\n<td>FaceScrub(python_mtcnn)</td>\n<td>78.33%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_184000 concat mirror(matlab align)</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>79.05%</td>\n</tr>\n<tr>\n<td>normface_jitter_iter_124000 concat mirror(python align)</td>\n<td>FaceScrub(python_mtcnn)</td>\n<td>76.50%</td>\n</tr>\n<tr>\n<td>normface_jitter_iter124000 cancat mirror(python align)</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>78.92%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step-10-MTCNN-matlab-人脸检测及对齐\"><a href=\"#step-10-MTCNN-matlab-人脸检测及对齐\" class=\"headerlink\" title=\"step 10:MTCNN(matlab)人脸检测及对齐\"></a>step 10:MTCNN(matlab)人脸检测及对齐</h1><h2 id=\"step-10-1：对齐Megaface和FaceScrub\"><a href=\"#step-10-1：对齐Megaface和FaceScrub\" class=\"headerlink\" title=\"step 10.1：对齐Megaface和FaceScrub\"></a>step 10.1：对齐Megaface和FaceScrub</h2><p>主要是Megaface数据集（1028062张）,FaceScrub数据集(91712张)，其中FaceScrub数据集中通过mtcnn（/home/yf/align/align_megaface.m）检测到的有89751张，剩余的1961张需要利用数据集中提供的3个关键点进行对齐，首先需要获取未检测到的图片的路径，然后利用python 脚本(/home/yf/megaface/devkit/templatelists/analysis/analysis_json.py)解析对应的存储该图片中人脸关键点的json文件，最后在利用matlab脚本(/home/yf/align/for_not_detect/align_megaface.m)进行批量对齐。Megaface数据集中未检测到的数据集同样处理。</p>\n<p><strong>问题</strong>：</p>\n<p>在进行了41万次对齐后，出现了imread的错误，然后将从目录读取路径改成了从存储图片路径的文件中(/home/yf/megaface/tests/MegaFace_align_list_image.txt)直接获取路径，并输出每次进行处理的文件名，重新进行对齐操作，然后重现了这个错误，最后比对MegaFace_align_list_image.txt的下一张图片，发现有张图片是输入为空的。</p>\n<h2 id=\"step-10-2-对齐msceleb数据\"><a href=\"#step-10-2-对齐msceleb数据\" class=\"headerlink\" title=\"step 10.2:对齐msceleb数据\"></a>step 10.2:对齐msceleb数据</h2><p>重新对齐msceleb数据集用于训练。</p>\n<h1 id=\"step-11-Normface训练\"><a href=\"#step-11-Normface训练\" class=\"headerlink\" title=\"step 11:Normface训练\"></a>step 11:Normface训练</h1><p>Normface(paper:<a href=\"https://arxiv.org/pdf/1704.06369.pdf\" target=\"_blank\" rel=\"external\">NormFace: L2 Hypersphere Embedding for Face Verification</a>)</p>\n<h2 id=\"step-11-1-训练EasyEnsemble模型\"><a href=\"#step-11-1-训练EasyEnsemble模型\" class=\"headerlink\" title=\"step 11.1:训练EasyEnsemble模型\"></a>step 11.1:训练EasyEnsemble模型</h2><p>model1在测试集上的准确率为92.88%，model2在测试集上的准确率为92.85%。</p>\n<p>暂时只测了单个的model1 concate mirror在Megaface(还是原始python版mtcnn对齐的)上的准确率只有70.32%。下周继续测试两个模型的效果。</p>\n<h2 id=\"step-11-2-训练Balance模型\"><a href=\"#step-11-2-训练Balance模型\" class=\"headerlink\" title=\"step 11.2:训练Balance模型\"></a>step 11.2:训练Balance模型</h2><p>刚生成完训练的数据集，下周开始训练。</p>\n<h1 id=\"step-12-Image-Jitter\"><a href=\"#step-12-Image-Jitter\" class=\"headerlink\" title=\"step 12:Image Jitter\"></a>step 12:Image Jitter</h1><p>对图片增加随机扰动，包括缩放、角度变换、镜像操作，主要还是msceleb数据集(/home/yf/data/msclean)上进行，由于该数据集类别不均衡，所以对于样本数较少的类别可以采用这种办法增加样本容量。最终将每个类别的样本数控制在80~160之间。</p>\n<p>10240892</p>\n<p>train:9257534</p>\n<p>val:983342</p>\n<p>正在生成训练的数据集lmdb。</p>\n<p>jitter_center_iter_190000 concat mirror  FaceScrub(matlab_mtcnn)  77.69%  </p>\n<p>jitter_softmax_iter_180000 concat mirror  FaceScrub(matlab_mtcnn)  83.18%</p>\n<p>jitter_softmax_iter_180000 only mirror  FaceScrub(matlab_mtcnn)  82.08%</p>\n<p>jitter_softmax_iter_180000 add mirror  FaceScrub(matlab_mtcnn)  82.87%</p>\n<p>jitter_softmax_iter_184000 concat mirror  FaceScrub(matlab_mtcnn)  83.16%</p>\n<h1 id=\"step-13-Gender-test\"><a href=\"#step-13-Gender-test\" class=\"headerlink\" title=\"step 13:Gender test\"></a>step 13:Gender test</h1><h3 id=\"1-0\"><a href=\"#1-0\" class=\"headerlink\" title=\"1.0\"></a>1.0</h3><p>VGG16在lfw上准确率90.04%，在imdb(15590测试样本)上准确率90.92%</p>\n<p>model training:female(69847),male(86061)</p>\n<p>train:124728</p>\n<p>val:15590</p>\n<p>test:15590</p>\n<h3 id=\"2-0\"><a href=\"#2-0\" class=\"headerlink\" title=\"2.0\"></a>2.0</h3><p>网络：AlexNet 在清理后的数据集上，迭代29500次后，训练准确率为98.16%，loss=0.55</p>\n<p>在测试集上达到98.11%（15127/15418)</p>\n<h1 id=\"Todo1\"><a href=\"#Todo1\" class=\"headerlink\" title=\"Todo1\"></a>Todo1</h1><ul>\n<li>[x] Create umdfaces–&gt;lmdb</li>\n<li>[x] EasyEnsemble train and test</li>\n<li>[x] Use matcaffe for metric learning</li>\n<li>[ ] Megaface test<pre><code>- - [x] center face\n  - [x] balance-cent-soft\n  - [x] reduced\n  - [x] mirror or concatenate\n  - [x] EasyEnsemble\n</code></pre></li>\n</ul>\n<ul>\n<li><p>[x] Paper reading:One-shot face recognition by promoting underrepresented classes</p>\n<pre><code>​\n</code></pre></li>\n</ul>\n<h1 id=\"Todo2\"><a href=\"#Todo2\" class=\"headerlink\" title=\"Todo2\"></a>Todo2</h1><ul>\n<li><p>[ ] jitter model training(softmax first)</p>\n</li>\n<li><p>[ ] balance model retrain on normface(include center loss)</p>\n</li>\n<li><p>[ ] aligned by matlab_mtcnn megaface(balance model 75.65% version)</p>\n</li>\n<li><p>[ ] gender classfication model training</p>\n<pre><code>- check the dataset (detect and crop by matlab_mtcnn)\n\n- generate lmdb\n\n- choose a model(ResNet?)\n\n  ​\n</code></pre></li>\n</ul>\n<h3 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h3><p><a href=\"https://github.com/happynear/FaceVerification\" target=\"_blank\" rel=\"external\">happynear-face-verification</a></p>\n<p><a href=\"https://github.com/davisking/dlib/blob/cbd187fb6109d21406f6a76bb0e9aa0689b1e54a/examples/dnn_face_recognition_ex.cpp\" target=\"_blank\" rel=\"external\">dlib-jitter</a></p>\n<p><a href=\"http://blog.dlib.net\" target=\"_blank\" rel=\"external\">dlib-face-verification-blog</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"step1-Mirror-face相关\"><a href=\"#step1-Mirror-face相关\" class=\"headerlink\" title=\"step1:Mirror face相关\"></a>step1:Mirror face相关</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>PCA Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Mirror</td>\n<td>192</td>\n<td>0.64</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Concat</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>Mirror Add/Average</td>\n<td>184</td>\n<td>0.64</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>Mirror Max</td>\n<td>144</td>\n<td>0.65</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>Mirror Min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.48%</td>\n</tr>\n<tr>\n<td>Mirror Avg+min</td>\n<td>168</td>\n<td>0.65</td>\n<td>99.45%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step2-单眼Patch-model\"><a href=\"#step2-单眼Patch-model\" class=\"headerlink\" title=\"step2:单眼Patch model\"></a>step2:单眼Patch model</h1><table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>14</td>\n<td>90~100</td>\n<td>8,5674</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>3329</td>\n<td>100~110</td>\n<td>2,9481</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>21,3675</td>\n<td>110~120</td>\n<td>9051</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>45,1416</td>\n<td>120~130</td>\n<td>2894</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>49,1913</td>\n<td>130~140</td>\n<td>932</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>72,8911</td>\n<td>140~150</td>\n<td>279</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>137,9232</td>\n<td>150~160</td>\n<td>86</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>128,7442</td>\n<td>160~170</td>\n<td>14</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>30,9026</td>\n<td>170~180</td>\n<td>6</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"问题\"><a href=\"#问题\" class=\"headerlink\" title=\"问题\"></a>问题</h2><p>1.对于人脸关键点数据中，[eye_left_x,eye_left_y,eye_right_x,eye_right_y…],在进行眼睛宽度估算时，利用eye_width=(eye_right_x-eye_elft_x)<em>0.78,所以crop后左眼的最左边的坐标公式为eye_left_x-0.5</em>eye_width，有一种情况没考虑到就是eye_left_x小于0.5*eye_width，所以要加一个判断语句<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div></pre></td><td class=\"code\"><pre><div class=\"line\">if eye_width*0.5&gt;eye_left_x:</div><div class=\"line\">            eye_width=eye_left_x*2</div></pre></td></tr></table></figure></p>\n<p>导致眼睛宽度估算分布就不太准确，最终导致在mscelebv1_crop数据上，最终符合要求的只有731750张</p>\n<p>重新生成眼睛宽度估算文件，其分布如下</p>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>1,8975</td>\n<td>100~110</td>\n<td>98</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>71,3760</td>\n<td>110~120</td>\n<td>17</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>126,4102</td>\n<td>120~130</td>\n<td>3</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>226,4348</td>\n<td>130~140</td>\n<td>0</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>59,3351</td>\n<td>140~150</td>\n<td>0</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>10,6502</td>\n<td>150~160</td>\n<td>0</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>2,4588</td>\n<td>160~170</td>\n<td>1</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>5592</td>\n<td>170~180</td>\n<td>0</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>1588</td>\n<td>180~190</td>\n<td>8</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>421</td>\n<td>190~200</td>\n<td>21</td>\n</tr>\n</tbody>\n</table>\n<p>2.tripletloss在100次迭代时，loss=nan，原因是初始化的base_lr过大，调小即可<br>3.test的accuracy一直为0：想到了在清洗数据的时候，有的人物并没有参与，也就是最开始直接用小蔓师兄的标签不行，最后一层的输出不等于我现在数据中的样本类别数。。得自己重新生成，首先要将空目录删除，确定有多少类别，然后label从0开始重新生成。<br>最后发现我的类别应该是90481.而最后一个全连接层的num_output=90526</p>\n<p><u>17个小时，迭代12万次，26.5%的准确率，loss=5.5。</u></p>\n<h1 id=\"step3-双眼patch-model\"><a href=\"#step3-双眼patch-model\" class=\"headerlink\" title=\"step3:双眼patch model\"></a>step3:双眼patch model</h1><p>生成双眼宽度估算文件，其分布如下</p>\n<table>\n<thead>\n<tr>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n<th>eye_width</th>\n<th>num</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0~10</td>\n<td>30</td>\n<td>100~110</td>\n<td>18,8771</td>\n<td>200~210</td>\n<td>383</td>\n</tr>\n<tr>\n<td>10~20</td>\n<td>5888</td>\n<td>110~120</td>\n<td>8,8508</td>\n<td>210~220</td>\n<td>196</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>11,4595</td>\n<td>120~130</td>\n<td>4,6243</td>\n<td>220~230</td>\n<td>100</td>\n</tr>\n<tr>\n<td>30~40</td>\n<td>35,5451</td>\n<td>130~140</td>\n<td>2,4982</td>\n<td>230~240</td>\n<td>52</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>42,6729</td>\n<td>140~150</td>\n<td>1,2749</td>\n<td>240~250</td>\n<td>33</td>\n</tr>\n<tr>\n<td>50~60</td>\n<td>49,1312</td>\n<td>150~160</td>\n<td>6714</td>\n<td>250~260</td>\n<td>15</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>69,0394</td>\n<td>160~170</td>\n<td>3413</td>\n<td>260~270</td>\n<td>3</td>\n</tr>\n<tr>\n<td>70~80</td>\n<td>104,9353</td>\n<td>170~180</td>\n<td>1909</td>\n<td>270~280</td>\n<td>2</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>100,6618</td>\n<td>180~190</td>\n<td>1091</td>\n<td>280~290</td>\n<td>0</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>47,7219</td>\n<td>190~200</td>\n<td>622</td>\n<td>290~300</td>\n<td>0</td>\n</tr>\n</tbody>\n</table>\n<p>筛选crop后，宽度在20~130区间的图片，共90523个类别，4932655张。</p>\n<p>train.txt：3982004 </p>\n<p>val.txt：950661 </p>\n<p><u>18万次迭代之后，准确率只有66%左右。</u></p>\n<h1 id=\"step4-crop对齐后的图片的眼睛，训练单眼模型\"><a href=\"#step4-crop对齐后的图片的眼睛，训练单眼模型\" class=\"headerlink\" title=\"step4:crop对齐后的图片的眼睛，训练单眼模型\"></a>step4:crop对齐后的图片的眼睛，训练单眼模型</h1><p>数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”）</p>\n<pre><code>&quot;ref_points&quot;: [\n        30.2946, 51.6963, \n     65.5318, 51.5014, \n     48.0252, 71.7366,\n     33.5493, 92.3655, \n     62.7299, 92.2041\n ]\n\n\n  eye_width=(ref_points[2]-ref_points[0])*0.8\n     eye_height=eye_width\n     x1=ref_points[0]-0.5*eye_width=16\n     x2=ref_points[0]+0.5*eye_width=44\n     y1=ref_points[1]-0.5*eye_height=37\n     y2=ref_points[1]+0.5*eye_height=65\n</code></pre><p>train:4071324 张</p>\n<p>val:973183张</p>\n<h1 id=\"step5-crop对齐后的图片的眼睛，训练双眼模型\"><a href=\"#step5-crop对齐后的图片的眼睛，训练双眼模型\" class=\"headerlink\" title=\"step5:crop对齐后的图片的眼睛，训练双眼模型\"></a>step5:crop对齐后的图片的眼睛，训练双眼模型</h1><p>数据集大小：5044507(90525个类)（”/home/yf/caffe-rc5-triplet/examples/triplet-face/clean.txt”，”/home/yf/data/msclean”）</p>\n<pre><code>   &quot;ref_points&quot;: [\n       30.2946, 51.6963, \n    65.5318, 51.5014, \n    48.0252, 71.7366,\n    33.5493, 92.3655, \n    62.7299, 92.2041\n]\n     eye_width=(ref_points[2]-ref_points[0])*0.8\n    eye_height=eye_width\n    x1=ref_points[0]-0.5*eye_width=16\n    x2=ref_points[2]+0.5*eye_width=79\n    y1=ref_points[1]-0.5*eye_height=37\n    y2=ref_points[1]+0.5*eye_height=65\n</code></pre><p><u>迭代16万次，精度为72.73%，loss=2.52</u></p>\n<p><u>在lfw上测试，精度最高达到77.04%</u></p>\n<h1 id=\"step6-Center-face-dropout-finetune-on-softmax\"><a href=\"#step6-Center-face-dropout-finetune-on-softmax\" class=\"headerlink\" title=\"step6:Center face+dropout+finetune on softmax\"></a>step6:Center face+dropout+finetune on softmax</h1><p>在msclean测试集上达到93.53% </p>\n<table>\n<thead>\n<tr>\n<th style=\"text-align:left\">Model</th>\n<th>PCA_Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td style=\"text-align:left\">dropcenter</td>\n<td>168</td>\n<td>0.64</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter_mirror</td>\n<td>136</td>\n<td>0.64</td>\n<td>99.38%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter +dropcenter_mirror+Min</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">dropcenter+dropcenter_mirror+ Add</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">center +dropcenter +Min</td>\n<td>400</td>\n<td>0.64</td>\n<td>99.40%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Min</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Add</td>\n<td>160</td>\n<td>0.64</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Max</td>\n<td>160</td>\n<td>0.64</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td style=\"text-align:left\">centermirror+dropcenter+Concate</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.47%</td>\n</tr>\n</tbody>\n</table>\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>PCA Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>center_min_mirror+dropcenter+Concate</td>\n<td>192</td>\n<td>0.65</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>center_min_mirror+dropcenter+Min</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n<tr>\n<td>center_min_mirror+dropcenter+Add</td>\n<td>136</td>\n<td>0.64</td>\n<td>99.47%</td>\n</tr>\n<tr>\n<td>eye_model</td>\n<td>160</td>\n<td>0.57</td>\n<td>77.04%</td>\n</tr>\n<tr>\n<td>eyemodel+center+Con</td>\n<td>208</td>\n<td>0.66</td>\n<td>74.80%</td>\n</tr>\n<tr>\n<td>三模型</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>center+center_min_mirror+dropoutcenter+Concate</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>center+softmax+dropoutcenter+Concate</td>\n<td>168</td>\n<td>0.66</td>\n<td>99.43%</td>\n</tr>\n<tr>\n<td>center+softmax+dropoutcenter+Add</td>\n<td>496</td>\n<td>0.65</td>\n<td>99.42%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step7-balance\"><a href=\"#step7-balance\" class=\"headerlink\" title=\"step7:balance\"></a>step7:balance</h1><h2 id=\"step7-1-减小过采样的数量，防止过拟合\"><a href=\"#step7-1-减小过采样的数量，防止过拟合\" class=\"headerlink\" title=\"step7.1:减小过采样的数量，防止过拟合\"></a>step7.1:减小过采样的数量，防止过拟合</h2><p>对/home/yf/data/clean.txt中每种类别进行统计各有多少个数：</p>\n<table>\n<thead>\n<tr>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>&lt;10</td>\n<td>1213</td>\n<td>10~20</td>\n<td>11617</td>\n</tr>\n<tr>\n<td>20~30</td>\n<td>10868</td>\n<td>30~40</td>\n<td>9692</td>\n</tr>\n<tr>\n<td>40~50</td>\n<td>9020</td>\n<td>50~60</td>\n<td>8426</td>\n</tr>\n<tr>\n<td>60~70</td>\n<td>8443</td>\n<td>70~80</td>\n<td>8783</td>\n</tr>\n<tr>\n<td>80~90</td>\n<td>8762</td>\n<td>90~100</td>\n<td>7317</td>\n</tr>\n<tr>\n<td>100~110</td>\n<td>4277</td>\n<td>110~120</td>\n<td>1753</td>\n</tr>\n<tr>\n<td>120~130</td>\n<td>354</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<p>类别总数共90525。由上图可知，类别严重不均衡，之前处理类别不均衡的方法主要是欠抽样和过抽样结合，对于多数类样本丢弃一部分样本，对于少数类样本复制生成，最后的训练数据分布如下：</p>\n<table>\n<thead>\n<tr>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n<th>每种类别包含图片张数</th>\n<th>类别数</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>70~80</td>\n<td>3673</td>\n<td>80~90</td>\n<td>19068</td>\n</tr>\n<tr>\n<td>90~100</td>\n<td>27069</td>\n<td>100~110</td>\n<td>40715</td>\n</tr>\n</tbody>\n</table>\n<p>由于少数类占了大多数，但是重复太多，可能导致过拟合问题，于是将每个类别的图片张数减去30，重新生成balance的训练数据，并训练模型。</p>\n<p><u>迭代17万次后，msdata测试集上准确率达到92.28%，loss=0.27</u></p>\n<p><u>lfw上精度为99.18%</u></p>\n<p><u>mirror:99.27%</u></p>\n<p><u>add:99.27%</u></p>\n<h2 id=\"step7-2-EasyEmsemble法均衡类别\"><a href=\"#step7-2-EasyEmsemble法均衡类别\" class=\"headerlink\" title=\"step7.2:EasyEmsemble法均衡类别\"></a>step7.2:EasyEmsemble法均衡类别</h2><p>step7.1的方法属于欠抽样和过抽样结合：</p>\n<ul>\n<li>对于欠抽样算法，将多数类样本删除有可能会导致分类器<strong>丢失有关多数类的重要信息</strong>。</li>\n<li>对于过抽样算法，虽然只是简单地将复制后的数据添加到原始数据集中，且某些样本的多个实例都是“<strong>并列的</strong>”，但这样也可能会导致分类器学习出现<strong>过拟合现象</strong>，对于同一个样本的多个复本产生多个规则条例，这就使得<strong>规则过于具体化</strong>；虽然在这种情况下，分类器的训练精度会很高，但在位置样本的分类性能就会非常不理想。</li>\n</ul>\n<p><strong>EasyEnsemble 核心思想是：</strong></p>\n<ul>\n<li><p>首先通过从多数类中<strong>独立随机</strong>抽取出若干子集</p>\n</li>\n<li><p>将每个子集与少数类数据<strong>联合</strong>起来<strong>训练</strong>生成多个基分类器</p>\n</li>\n<li><p>最终将这些基分类器<strong>组合形成</strong>一个集成学习系统</p>\n<p>设立一个阈值50，对于类别样本数超过50的，将其分写到两个不同的文件；对于类别样本数不超过50的，利用过采样进行增添，所以最终得到两个有交集的训练集A,B，两个训练集的样本数都是</p>\n<p>90525*50=4526250</p>\n</li>\n</ul>\n<p><u>训练两个model，然后提取特征，对特征进行融合。</u></p>\n<table>\n<thead>\n<tr>\n<th>Model(acc/loss)</th>\n<th>Pca Size</th>\n<th>Threshold</th>\n<th>Score</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>model1(90.75%/0.41)</td>\n<td>176</td>\n<td>0.64</td>\n<td>99.05%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26)</td>\n<td>200</td>\n<td>0.62</td>\n<td>99.28%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26) Mirror</td>\n<td>280</td>\n<td>0.63</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1(92.14%/0.26) Add Mirror</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.30%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41)</td>\n<td>128</td>\n<td>0.64</td>\n<td>99.27%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41) Mirror</td>\n<td>192</td>\n<td>0.63</td>\n<td>99.35%</td>\n</tr>\n<tr>\n<td>model2(92.56%/0.41) Add Mirror</td>\n<td>192</td>\n<td>0.64</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1 add model2</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.33%</td>\n</tr>\n<tr>\n<td>model1 mirror add model2 mirror</td>\n<td>136</td>\n<td>0.65</td>\n<td>99.37%</td>\n</tr>\n<tr>\n<td>model1 add model2 mirror</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.37%</td>\n</tr>\n<tr>\n<td>model1_add_mirror add model2_add_mirror</td>\n<td>152</td>\n<td>0.64</td>\n<td>99.38%</td>\n</tr>\n<tr>\n<td>model1_add_mirror concate model2_add_mirror</td>\n<td>128</td>\n<td>0.65</td>\n<td>99.32%</td>\n</tr>\n<tr>\n<td>model1 mirror min model2 mirror</td>\n<td>168</td>\n<td>0.64</td>\n<td>99.37%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step8-UMDFaces\"><a href=\"#step8-UMDFaces\" class=\"headerlink\" title=\"step8:UMDFaces\"></a>step8:UMDFaces</h1><p>对UMDFaces数据集进行人脸对齐处理</p>\n<p>batch1:175,534(3554类)</p>\n<p>batch2:115,126(2590类)</p>\n<p>batch3:77,228(2133类)</p>\n<p>frames:3,735,475(3106类)</p>\n<p>提取4个数据集的类别名称，经过处理分析后发现frames的类别属于batch1类别的子集，将3个batch与frames的数据集整合到一个数据集下，因为当静态图片和视频帧进行结合后训练的模型往往既能兼顾个体之间的差异（静态图片特征）也能学习到同一个个体的姿态变化（视频帧特征），要注意的一点就是对于frames和batch1中同一个类别的要放在一个目录下，并重新生成类别标签。</p>\n<p>数据总量:4103363(8276个类别)</p>\n<p>数据整理已经完成，接下来是在这个数据集上进行metric learning的训练。</p>\n<p>train:3286012<br>val:817351</p>\n<h1 id=\"step9-Megaface测试\"><a href=\"#step9-Megaface测试\" class=\"headerlink\" title=\"step9:Megaface测试\"></a>step9:Megaface测试</h1><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Dataset</th>\n<th>Score(Megaface/LFW)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>center-face</td>\n<td>FaceScrub Set1/LFW</td>\n<td>67.32%/99.42%</td>\n</tr>\n<tr>\n<td>balance-reduced</td>\n<td>FaceScrub Set1/LFW</td>\n<td>70.99%/99.18%</td>\n</tr>\n<tr>\n<td>easyensemble</td>\n<td>FaceScrub Set1</td>\n<td>73.91%/99.33%</td>\n</tr>\n<tr>\n<td>easyensemble  concat addmirror</td>\n<td>FaceScrub Set1</td>\n<td>74.21%/99.37%</td>\n</tr>\n<tr>\n<td>balance-cent-soft</td>\n<td>FaceScrub Set1/LFW</td>\n<td>74.47%/99.33%</td>\n</tr>\n<tr>\n<td>balance-cent-soft concat mirror</td>\n<td>FaceScrub Set1</td>\n<td>75.65%</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"step9-1-Megaface测试（续）\"><a href=\"#step9-1-Megaface测试（续）\" class=\"headerlink\" title=\"step9.1:Megaface测试（续）\"></a>step9.1:Megaface测试（续）</h2><table>\n<thead>\n<tr>\n<th>Model</th>\n<th>Dataset</th>\n<th>Score(Megaface/LFW)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Dropout_center Concat mirror</td>\n<td>FaceScrub Set1/LFW</td>\n<td>69.34%/99.47%</td>\n</tr>\n<tr>\n<td>normface easyensemble model1 Concart mirror</td>\n<td>FaceScrub Set1/LFW</td>\n<td>70.32%</td>\n</tr>\n<tr>\n<td>normface easyensemble 2models Concat mirror</td>\n<td>FaceScrub Set1</td>\n<td>70.49%</td>\n</tr>\n<tr>\n<td>balance concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>78.84%</td>\n</tr>\n<tr>\n<td>easyensemble concat addmirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>75.92%</td>\n</tr>\n<tr>\n<td>jitter_center_iter_190000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>77.69%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>83.13%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 only mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>82.08%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 add mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>82.87%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_184000 concat mirror</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>83.16%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_180000 concat mirror</td>\n<td>FaceScrub(python_mtcnn)</td>\n<td>78.33%</td>\n</tr>\n<tr>\n<td>jitter_softmax_iter_184000 concat mirror(matlab align)</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>79.05%</td>\n</tr>\n<tr>\n<td>normface_jitter_iter_124000 concat mirror(python align)</td>\n<td>FaceScrub(python_mtcnn)</td>\n<td>76.50%</td>\n</tr>\n<tr>\n<td>normface_jitter_iter124000 cancat mirror(python align)</td>\n<td>FaceScrub(matlab_mtcnn)</td>\n<td>78.92%</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"step-10-MTCNN-matlab-人脸检测及对齐\"><a href=\"#step-10-MTCNN-matlab-人脸检测及对齐\" class=\"headerlink\" title=\"step 10:MTCNN(matlab)人脸检测及对齐\"></a>step 10:MTCNN(matlab)人脸检测及对齐</h1><h2 id=\"step-10-1：对齐Megaface和FaceScrub\"><a href=\"#step-10-1：对齐Megaface和FaceScrub\" class=\"headerlink\" title=\"step 10.1：对齐Megaface和FaceScrub\"></a>step 10.1：对齐Megaface和FaceScrub</h2><p>主要是Megaface数据集（1028062张）,FaceScrub数据集(91712张)，其中FaceScrub数据集中通过mtcnn（/home/yf/align/align_megaface.m）检测到的有89751张，剩余的1961张需要利用数据集中提供的3个关键点进行对齐，首先需要获取未检测到的图片的路径，然后利用python 脚本(/home/yf/megaface/devkit/templatelists/analysis/analysis_json.py)解析对应的存储该图片中人脸关键点的json文件，最后在利用matlab脚本(/home/yf/align/for_not_detect/align_megaface.m)进行批量对齐。Megaface数据集中未检测到的数据集同样处理。</p>\n<p><strong>问题</strong>：</p>\n<p>在进行了41万次对齐后，出现了imread的错误，然后将从目录读取路径改成了从存储图片路径的文件中(/home/yf/megaface/tests/MegaFace_align_list_image.txt)直接获取路径，并输出每次进行处理的文件名，重新进行对齐操作，然后重现了这个错误，最后比对MegaFace_align_list_image.txt的下一张图片，发现有张图片是输入为空的。</p>\n<h2 id=\"step-10-2-对齐msceleb数据\"><a href=\"#step-10-2-对齐msceleb数据\" class=\"headerlink\" title=\"step 10.2:对齐msceleb数据\"></a>step 10.2:对齐msceleb数据</h2><p>重新对齐msceleb数据集用于训练。</p>\n<h1 id=\"step-11-Normface训练\"><a href=\"#step-11-Normface训练\" class=\"headerlink\" title=\"step 11:Normface训练\"></a>step 11:Normface训练</h1><p>Normface(paper:<a href=\"https://arxiv.org/pdf/1704.06369.pdf\" target=\"_blank\" rel=\"external\">NormFace: L2 Hypersphere Embedding for Face Verification</a>)</p>\n<h2 id=\"step-11-1-训练EasyEnsemble模型\"><a href=\"#step-11-1-训练EasyEnsemble模型\" class=\"headerlink\" title=\"step 11.1:训练EasyEnsemble模型\"></a>step 11.1:训练EasyEnsemble模型</h2><p>model1在测试集上的准确率为92.88%，model2在测试集上的准确率为92.85%。</p>\n<p>暂时只测了单个的model1 concate mirror在Megaface(还是原始python版mtcnn对齐的)上的准确率只有70.32%。下周继续测试两个模型的效果。</p>\n<h2 id=\"step-11-2-训练Balance模型\"><a href=\"#step-11-2-训练Balance模型\" class=\"headerlink\" title=\"step 11.2:训练Balance模型\"></a>step 11.2:训练Balance模型</h2><p>刚生成完训练的数据集，下周开始训练。</p>\n<h1 id=\"step-12-Image-Jitter\"><a href=\"#step-12-Image-Jitter\" class=\"headerlink\" title=\"step 12:Image Jitter\"></a>step 12:Image Jitter</h1><p>对图片增加随机扰动，包括缩放、角度变换、镜像操作，主要还是msceleb数据集(/home/yf/data/msclean)上进行，由于该数据集类别不均衡，所以对于样本数较少的类别可以采用这种办法增加样本容量。最终将每个类别的样本数控制在80~160之间。</p>\n<p>10240892</p>\n<p>train:9257534</p>\n<p>val:983342</p>\n<p>正在生成训练的数据集lmdb。</p>\n<p>jitter_center_iter_190000 concat mirror  FaceScrub(matlab_mtcnn)  77.69%  </p>\n<p>jitter_softmax_iter_180000 concat mirror  FaceScrub(matlab_mtcnn)  83.18%</p>\n<p>jitter_softmax_iter_180000 only mirror  FaceScrub(matlab_mtcnn)  82.08%</p>\n<p>jitter_softmax_iter_180000 add mirror  FaceScrub(matlab_mtcnn)  82.87%</p>\n<p>jitter_softmax_iter_184000 concat mirror  FaceScrub(matlab_mtcnn)  83.16%</p>\n<h1 id=\"step-13-Gender-test\"><a href=\"#step-13-Gender-test\" class=\"headerlink\" title=\"step 13:Gender test\"></a>step 13:Gender test</h1><h3 id=\"1-0\"><a href=\"#1-0\" class=\"headerlink\" title=\"1.0\"></a>1.0</h3><p>VGG16在lfw上准确率90.04%，在imdb(15590测试样本)上准确率90.92%</p>\n<p>model training:female(69847),male(86061)</p>\n<p>train:124728</p>\n<p>val:15590</p>\n<p>test:15590</p>\n<h3 id=\"2-0\"><a href=\"#2-0\" class=\"headerlink\" title=\"2.0\"></a>2.0</h3><p>网络：AlexNet 在清理后的数据集上，迭代29500次后，训练准确率为98.16%，loss=0.55</p>\n<p>在测试集上达到98.11%（15127/15418)</p>\n<h1 id=\"Todo1\"><a href=\"#Todo1\" class=\"headerlink\" title=\"Todo1\"></a>Todo1</h1><ul>\n<li>[x] Create umdfaces–&gt;lmdb</li>\n<li>[x] EasyEnsemble train and test</li>\n<li>[x] Use matcaffe for metric learning</li>\n<li>[ ] Megaface test<pre><code>- - [x] center face\n  - [x] balance-cent-soft\n  - [x] reduced\n  - [x] mirror or concatenate\n  - [x] EasyEnsemble\n</code></pre></li>\n</ul>\n<ul>\n<li><p>[x] Paper reading:One-shot face recognition by promoting underrepresented classes</p>\n<pre><code>​\n</code></pre></li>\n</ul>\n<h1 id=\"Todo2\"><a href=\"#Todo2\" class=\"headerlink\" title=\"Todo2\"></a>Todo2</h1><ul>\n<li><p>[ ] jitter model training(softmax first)</p>\n</li>\n<li><p>[ ] balance model retrain on normface(include center loss)</p>\n</li>\n<li><p>[ ] aligned by matlab_mtcnn megaface(balance model 75.65% version)</p>\n</li>\n<li><p>[ ] gender classfication model training</p>\n<pre><code>- check the dataset (detect and crop by matlab_mtcnn)\n\n- generate lmdb\n\n- choose a model(ResNet?)\n\n  ​\n</code></pre></li>\n</ul>\n<h3 id=\"参考链接\"><a href=\"#参考链接\" class=\"headerlink\" title=\"参考链接\"></a>参考链接</h3><p><a href=\"https://github.com/happynear/FaceVerification\" target=\"_blank\" rel=\"external\">happynear-face-verification</a></p>\n<p><a href=\"https://github.com/davisking/dlib/blob/cbd187fb6109d21406f6a76bb0e9aa0689b1e54a/examples/dnn_face_recognition_ex.cpp\" target=\"_blank\" rel=\"external\">dlib-jitter</a></p>\n<p><a href=\"http://blog.dlib.net\" target=\"_blank\" rel=\"external\">dlib-face-verification-blog</a></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cjda440la00002sc8gjwoy16z","category_id":"cjda440ma00022sc8350gp7qr","_id":"cjda440ne00072sc8askvv3at"},{"post_id":"cjda440m000012sc8gjqgkrtl","category_id":"cjda440na00052sc8q9alqrfr","_id":"cjda440ns000b2sc86vjdho4i"},{"post_id":"cjda440n100042sc8swzpqe1d","category_id":"cjda440na00052sc8q9alqrfr","_id":"cjda440nw000e2sc8b5hbzp74"},{"post_id":"cjda440t4000q2sc80h4mg7jy","category_id":"cjda440na00052sc8q9alqrfr","_id":"cjda440tm000w2sc89lmjy92t"},{"post_id":"cjda440rx000j2sc8a4u0ndk1","category_id":"cjda440sw000n2sc8bvjmsgja","_id":"cjda440u1000z2sc8tt5eekm8"},{"post_id":"cjda440th000v2sc88qj4rpcx","category_id":"cjda440td000s2sc83di72d8a","_id":"cjda440ud00122sc8ccoo1gjg"},{"post_id":"cjda440sd000l2sc88g9deobd","category_id":"cjda440td000s2sc83di72d8a","_id":"cjda440uk00152sc8e58d8ep1"},{"post_id":"cjda440u300102sc87x60pm0k","category_id":"cjda440td000s2sc83di72d8a","_id":"cjda440ur00172sc81u4nmjso"}],"PostTag":[{"post_id":"cjda440la00002sc8gjwoy16z","tag_id":"cjda440my00032sc8vvkkc5de","_id":"cjda440nq000a2sc8nnk5nu07"},{"post_id":"cjda440la00002sc8gjwoy16z","tag_id":"cjda440nb00062sc8iidsnceb","_id":"cjda440ns000c2sc85pzzth8k"},{"post_id":"cjda440m000012sc8gjqgkrtl","tag_id":"cjda440nh00092sc8gx98pdsh","_id":"cjda440nz000g2sc8v3vde2aw"},{"post_id":"cjda440m000012sc8gjqgkrtl","tag_id":"cjda440nu000d2sc80k5vgupa","_id":"cjda440o0000h2sc8pwyv08g0"},{"post_id":"cjda440n100042sc8swzpqe1d","tag_id":"cjda440ny000f2sc87royys9k","_id":"cjda440o2000i2sc8jczwf92b"},{"post_id":"cjda440rx000j2sc8a4u0ndk1","tag_id":"cjda440sx000o2sc80s7hpcuc","_id":"cjda440th000u2sc8u1vr1abx"},{"post_id":"cjda440sd000l2sc88g9deobd","tag_id":"cjda440te000t2sc8gsia7ca0","_id":"cjda440u600112sc83xzwcpk6"},{"post_id":"cjda440sy000p2sc89qdz2ih2","tag_id":"cjda440tv000y2sc8dyiofppu","_id":"cjda440up00162sc8ilh1nq4c"},{"post_id":"cjda440t4000q2sc80h4mg7jy","tag_id":"cjda440uj00142sc8ex1qjtfw","_id":"cjda440v0001a2sc8l282xbnb"},{"post_id":"cjda440t4000q2sc80h4mg7jy","tag_id":"cjda440ut00182sc8kp241cbj","_id":"cjda440vb001b2sc8g8qlf9qf"},{"post_id":"cjda440th000v2sc88qj4rpcx","tag_id":"cjda440uy00192sc87q1uazfu","_id":"cjda440vj001d2sc8k40atkjc"},{"post_id":"cjda440u300102sc87x60pm0k","tag_id":"cjda440vf001c2sc87uv5ctzb","_id":"cjda440vm001e2sc8bp52s9e0"}],"Tag":[{"name":"Face","_id":"cjda440my00032sc8vvkkc5de"},{"name":"笔记","_id":"cjda440nb00062sc8iidsnceb"},{"name":"caffe","_id":"cjda440nh00092sc8gx98pdsh"},{"name":"调参","_id":"cjda440nu000d2sc80k5vgupa"},{"name":"人脸识别","_id":"cjda440ny000f2sc87royys9k"},{"name":"Hexo","_id":"cjda440sx000o2sc80s7hpcuc"},{"name":"笔记，人脸识别","_id":"cjda440te000t2sc8gsia7ca0"},{"name":"Python","_id":"cjda440tv000y2sc8dyiofppu"},{"name":"深度学习","_id":"cjda440uj00142sc8ex1qjtfw"},{"name":"神经网络","_id":"cjda440ut00182sc8kp241cbj"},{"name":"深度学习，论文笔记","_id":"cjda440uy00192sc87q1uazfu"},{"name":"深度学习，人脸识别","_id":"cjda440vf001c2sc87uv5ctzb"}]}}